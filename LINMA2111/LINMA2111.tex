\documentclass[12pt, openany]{report}
\usepackage{qtree}
\usepackage{qtree}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage[a4paper,left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage[english]{babel}
\usepackage{libertine}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{float}
\usepackage{enumitem}
\usepackage{pythonhighlight}
\usepackage[]{titletoc}
\usepackage{empheq}
\usepackage{titlesec}
\usepackage{mathpazo}
\usepackage{xfrac}
\usepackage{textcomp}
\usepackage{mathtools}
\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning}
\usepackage{caption}
\usepackage{tabularray}
\usepackage{subcaption}
\usepackage[bottom]{footmisc}
\usepackage{pdfpages}
\usepackage{tabularx}
\usepackage{amsthm}
\usepackage[skins]{tcolorbox}
\titleformat{\chapter}[display]
  {\normalfont\bfseries}{}{0pt}{\Huge}
\usepackage{hyperref}
\newcommand{\hsp}{\hspace{20pt}}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\Oo}{\mathcal{O}}
\theoremstyle{definition}
\newtheorem{thm}{Theorem}[chapter]
\newtheorem{definition}[thm]{Definition}
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{corollary}[thm]{Corollary}
\newtheorem{example}[thm]{Example}
\def\mat#1{\underline{\underline{#1}}}

\hbadness=100000
\begin{document}
\begin{titlepage}
    \begin{sffamily}
    \begin{center}
        \includegraphics[scale=0.25]{img/page_de_garde.png} \\[1cm]
        \HRule \\[0.4cm]
        { \huge \bfseries LINMA2111 - Discrete mathematics II \\[0.4cm] }
    
        \HRule \\[1.5cm]
        \textsc{\LARGE Simon Desmidt\\ Issambre L'Hermite Dumont}\\[1cm]
        \vfill
        \vspace{2cm}
        {\large Academic year 2025-2026 - Q1}
        \vspace{0.4cm}
         
        \includegraphics[width=0.15\textwidth]{img/epl.png}
        
        UCLouvain\\
    
    \end{center}
    \end{sffamily}
\end{titlepage}

\setcounter{tocdepth}{1}
\tableofcontents
\chapter{Sorting Algorithms}
Let $S$ be a set with a total order $\le$. Given an array of $n$ elements of $S$, we owuld like to get a permutation of that input array that respects the order. When analyzing an algorithm, we would like to check its correctness and its complexity (both time and space). 
\section{Bachman-Landau complexity notations}
Let $f,g:\N\to \R^+$. We write 
\begin{equation}
  	\begin{aligned}
		&f\in \mathcal{O}(g) \Longleftrightarrow \exists c>0, \exists n_0, \forall n>n_0,\ f(n)\le cg(n)\\
		&f\in \Omega(g)\Longleftrightarrow g\in \mathcal{O}(f)\\
		&f\in \Theta(g)\Longleftrightarrow f\in \mathcal{O}(g)\text{ and } f\in \Omega(g)\\
		&f\in o(g) \Longleftrightarrow \forall c>0, \exists n_0, \forall n>n_0,\ f(n)< cg(n)\\
		&f\in \omega(g)\Longleftrightarrow g\in o(f)\\
  	\end{aligned}
\end{equation}
For example, $\frac{n(n-1)}{2}\in \mathcal{O}(n^3)$ and $\frac{n(n-1)}{2}\in \Omega (n\log(n))$. 
\begin{prop}
    \begin{equation}
        \begin{aligned}
            \forall f,g, \qquad& \Theta(f)+\Theta(g) = \Theta(f+g)=\Theta(\max(f,g))\\
            &\mathcal{O}(f)+\mathcal{O}(g) = \mathcal{O}(f+g) = \mathcal{O}(\max(f,g))
        \end{aligned}
    \end{equation}
\end{prop}
The time complexity is the number of operations as a function of the input size. The space complexity is the amount of memory (used in addition to the input) as a function of the input size. \\
The worst-case (respectively best-case) time complexity is the maximum (respectively minimum) running time over all instances of size $n$. The average-case time complexity is the average time over all instances of size $n$, for some probability distribution over the instances:
\begin{equation}
	t = \mathbb{E}_{x\sim D}[T(x)]
\end{equation}
where $D$ is the distribution of the input.\\
\section{Sorting Algorithms}
\subsection{Selection Sort}
This algorithm is the naive approach in complexity $\mathcal{O}(n^2)$. It goes as follows:
\begin{algorithm}[H]
	\caption{Selection sort algorithm}\label{algo:selection-sort}
	\begin{algorithmic}[1]
	\For{$i$   from   1   to   $n-1$}
		\State Select the smallest element in the subarray from index i to n;
		\State Swap it with element at index i;
	\EndFor
	\end{algorithmic}
\end{algorithm}
The invariant of this algorithm is that the elements 1 to $i-1$ are sorted.
\subsection{Insertion Sort}
The idea of this algorithm is to shift the elements to the left until they are well placed. This also has a complexity $\mathcal{O}(n^2)$, although the best case is in $\mathcal{O}(n)$. 
\begin{algorithm}[H]
	\caption{Insertion sort algorithm}\label{algo:insertion-sort}
	\begin{algorithmic}[1]
		\For{$i$   from 2 to $n$}
			\State shift element $i$ to the left by successive swaps until it is well placed;
		\EndFor 
	\end{algorithmic}
\end{algorithm}
The invariant of this algorithm is that the elements 1 to $i-1$ are sorted.\\ 
To prove correctness of the algorithm:
\begin{enumerate}
    \item Check the basic and the induction cases;
    \item Check that it is a permutation of the original array;
    \item Check that it is sorted.
\end{enumerate}
To prove the correctness of an algorithm, we generally use the Hoare triple, i.e. a tuple for any input array $x_0$:
\begin{equation}
  \begin{aligned}
	&\{\text{Algorithm to be used};\ \text{Precondition}; \ \text{Postcondition}\}\\
	&\left\{IS;\ x=x_0;\ x\text{ is sorted and is a permutation of }x_0\right\}
\end{aligned}
\end{equation}
where IS is the insertion sort algorithm, and $x$ is the sorted array. \\
In practice, to prove the correctness of an algorithm, we define the invariant and the base case, and do an induction step show that the invariant is preserved. The proof is done in my notes at page 1.\\
For insertion sort, 
\begin{itemize}
    \item the worst-case complexity is $\mathcal{O}(n^2)$;
    \item the best-case complexity is $\mathcal{O}(n)$;
    \item the average-case complexity is $\mathcal{O}(n^2)$: for a uniform probability distribution over the instances,
    \begin{equation}
        \E(time) = \E\left[\Theta(\sum_{i=2}^nt_i)\right] = \Theta\left[\sum_{i=2}^n \E(t_i)\right] = \Theta\left[\sum_{i=2}^n \frac{i}{2}\right] = \Theta(n^2)
    \end{equation}
\end{itemize}
\subsection{Quick sort}
The quick sort algorithm is based on divide and conquer methods (see chapter \ref{chap:d&c}): it splits and solves smaller sorting problems, and recombines the outputs at the end into a sorted instance.
\begin{algorithm}[H]
	\caption{Quick Sort algorithm}
	\begin{algorithmic}[1]
		\State pivot = T[1];
		\State T\_low = [T[i] \: : \: T[i] < pivot and i >1];
		\State T\_high = [T[i] \: : \: T[i] > pivot and i >1];
		\State quicksort(T\_low);
		\State quicksort(T\_high);
		\State T = [T\_low; pivot; T\_high];
	\end{algorithmic}
\end{algorithm}
The worst-case complexity is still $\mathcal{O}(n^2)$:
\begin{equation}
    t_n = t_{n-1}+\Theta(n) = t_{n-2}+\Theta(n-1)+\Theta(n) = \Theta(n^2)
\end{equation}
where the $\Theta(n)$ comes from the partioning of the array. 
The average-case complexity is lower: $\mathcal{O}(n\log (n))$:
\begin{equation}
    t_n = 2 t_{n/2} + \Theta(n) = 2^{\log_2(n)}t_1 + \Theta(n\log_2(n)) = \Theta(n\log(n))
\end{equation}
The randomized version of the algorithm consists in shuffling before applying the classical quick sort, in order to change the pivot value. As this is a random algorithm, the worst-case complexity corresponds to the expected complexity, i.e. $\mathcal{O}(n\log(n))$. This is the "Robin Hood effect": we put all instances on an equal footing, and steal from the rich (good) instances to give to the poor ones. 
\begin{thm}
    For a non decreasing, nonnegative function $f(\cdot)$, we have the inequalities
    \begin{equation}
        f(0) + \dots + f(n-1) \le \int_0^n f(x)dx \le f(1)+\dots +f(n)
    \end{equation}
\end{thm}
\begin{thm}\textbf{Markov's inequality}
    For a random variable $T$ taking only nonnegative values, 
    \begin{equation}
        Pr[T>a \E[T]] \le \frac{1}{a} \qquad a>0
    \end{equation}
\end{thm}
\subsection{Merge Sort}
The merge sort algorithm is also based on divide-and-conquer, and its complexity is also $\Theta(n\log(n))$. 
\begin{algorithm}[H]
	\caption{Merge Sort algorithm}\label{algo:merge-sort}
	\begin{algorithmic}[1]
		\State T\_left = [T[i] such that i <= n/2]
		\State T\_right = [T[i] such that i > n/2]
		\State mergesort(T\_left)
		\State mergesort(T\_right)
		\State T = merge(T\_left, T\_right)
	\end{algorithmic}
\end{algorithm}
The recurrence equation for the complexity is 
\begin{equation}
	t_n = 2t_{\lceil n/2\rceil} + \Theta(n) \Longrightarrow t(n) = \Theta(n\log(n))
\end{equation}
The disadvatange of quick sort is that this algorithm does not have randomness, as it is complex to simulate. However, the constant in the complexity order is higher than for the quick sort, and the memory usage can be higher too. 
\section{Decision tree}
A decision tree focuses on strategies and outcomes. It captures the main operations and the leaves are the possible outcomes ($\mathcal{O}(n!)$ in sorting problems, for $n$ the size of the data). This helps us compute the complexity of a sorting algorithm: the number of steps is the depth $S$ of the tree. Then, we have 
\begin{equation}
    \Theta(cost(Insert) + cost(ExtractMax))
\end{equation}
This is equivalent to $\mathcal{O}(n\log(n))$ : 
\begin{equation}
    \begin{aligned}
        \text{LeftChild}(T(i)) = T(2i) &\qquad \text{RightChild}(T(i)) = T(2i+1)\\
        \text{Father}(T(i)) &= T(\lfloor i/2\rfloor)
    \end{aligned}
\end{equation}
and we know that $S \ge \int_1^{n+1} \log(x)dx$\footnote{Why?}. Then, 
\begin{equation}
	\begin{aligned}
		\left[x\log(x)-x\right]_1^n \le S&\le \left[x\log(x)-x\right]_1^{n+1}\\
		S &= \mathcal{O}(n\log (n))
	\end{aligned}
\end{equation}
\subsection{Shannon coding theorem}
Let $\mathcal{S}=\{1,\dots,N\}$ and $P$ a random variable over $\mathcal{S}$ such that $Pr[P=i] = p_i$. Let $f:\mathcal{S}\to \{0,1\}^*$ maps a number to its binary representation, under the constraint that no number is a prefix of another one. Then,
\begin{equation}
	\sum_{i=1}^N p_i |f(i)| \ge H(P) = -\sum_{i=1}^N p_i \log_2 (p_i)
\end{equation}
where $H(P)$ is called the entropy of the probability distribution $P$. This theorem states that the average length of the prefix of the encoding of $N$ elements must be at least $\log(N)$ characters long.  
\subsection{Yao's minimax principle}
Consider a probability distribution over instances of a given size of a given problem. There, there exists a deterministic algorithm solving the problem for thoses instances, whose average-case complexity for the given distribution is lower than the worst-case expected complexity of any random algorithm solving the same problem. \\
This means that a random algorithm cannot be better than a deterministic algorithm in every case of a problem. 
\subsection{What about the part on thermodynamics in CM2?}
\chapter{Divide-and-conquer algorithms}\label{sec:d&c}
The divide-and-conquer method consists in three steps:
\begin{enumerate}
	\item Divide: create smaller subproblems;
	\item Recurse: solve them;
	\item Combine: merge the solutions.
\end{enumerate}
In sorting problems, a divide-and-conquer algorithm is the merge sort algorithm. It consists in dividing the array in two, sorting each half recursively, and merging the two sorted halves. The merge operation is done in linear time.\\
\section{Complexity of an integer multiplication}\label{sec:int_mult}
Given two $n$-digit numbers, we want to compute their product:
\begin{equation}
  	\begin{cases}
		a = a_{n-1}a_{n-2}...a_1a_0\\
		b = b_{n-1}b_{n-2}...b_1b_0\\
  	\end{cases}
	\Longrightarrow c=a\cdot b = c_{2n-1}...c_0
\end{equation}
Let us define $B$ as the basis (e.g. 10) and decompose $a$ and $b$ in two parts:
\begin{equation}
	\begin{cases}
		a = \alpha_0+B\alpha_1\\
		b = \beta_0+B\beta_1
	\end{cases} \Longrightarrow a\cdot b = \alpha_0\beta_0 + B(\alpha_1\beta_0 + \alpha_0\beta_1)+B^2 \alpha_1\beta_1
\end{equation}
In that case, we find a recurrence relation for the computation time:
\begin{equation}
	T(n) = 4T(n/2) + \Theta(n)
\end{equation}
The factor $4$ comes from the fact that we need $4$ products, and the $\Theta(n)$ is the complexity of the sum of the products. \\

We introduce the Master theorem to solve this equation, see section \ref{sec:master}. It gives a complexity of $T(n) = \Theta(n^{\log_2(4)}) = \Theta(n^2)$.\\
To reduce the complexity, we can change the value of the coefficient before $T(n/2)$ from 4 to 3 by calculating only 3 products:
\begin{equation}
	\begin{cases}
		\gamma_0 = \alpha_0\beta_0\\
		\gamma_2 = \alpha_1\beta_1\\
		\gamma_1 = (\alpha_0+\alpha_1)(\beta_0+\beta_1)-\gamma_0-\gamma_2\\
	\end{cases}
\end{equation}
This reduces the complexity to $\Theta(n^{1.58})$. \\
Following a similar reasoning, we can instead divide $a$ and $b$ into 3 sums instead of 2, and get 5 multiplications. This gives a complexity of $\Theta(n^{\log_3(5)}) = \Theta(n^{1.46})$. Dividing in 4, 5, etc, we converge to a complexity of $\Theta(n)$ and this is the optimal complexity for the multiplication of two numbers of $n$ digits. The problem is that the constant in front of the $n$ starts to grow as we divide into more and more limbs, and so a bigger exponent can be enough for most arrays\footnote{We call galactical algorithm an algorithm that is asymptotically good but the constant grows so large that it is only useful for huge arrays (e.g. $\sim10^{80}$).}.
\subsection{Master Theorem} \label{sec:master}
Let $a\ge 1$ and $b>1$ be constants and $f(n)$ a positive function, and let $T(n)$ be defined by $T(0)>0$ and $T(n) = aT(\lfloor n/b\rfloor)+f(n)$. Then,
\begin{itemize}
	\item If $f(n) = \mathcal{O}(n^{\log_b (a)-\epsilon})$ for some $\epsilon>0$, then $T(n) = \Theta(n^{\log_b(a)})$;
	\item If $f(n) = \Theta(n^{\log_b(a)})$, then $T(n) = \Theta(n^{\log_b(a)}\log(n))$;
	\item If $f(n) = \Omega(n^{\log_b(a)+\epsilon})$ for some $\epsilon>0$ and if, for some $c>1$ and $n_0$ such that $af(\lfloor n/b\rfloor)\le cf(n)$ for all $n\ge n_0$, then $T(n)=\Theta(f(n))$;
\end{itemize}
\subsection{Proof of the master theorem}
\Tree [.{$n=b^k$} 
        [.{$\phantom{X}$} 
            [.{$\vdots$} {$\phantom{X}$} {$\phantom{X}$} ]
            [.{$\vdots$} {$\phantom{X}$} {$\phantom{X}$} ]
        ]
        [.{$\phantom{X}$} 
            [.{$\vdots$} {$\phantom{X}$} {$\phantom{X}$} ]
            [.{$\vdots$} {$\phantom{X}$} {$b^0$} ]
        ]
    ]

In that tree, for a level $i$, the size of the problem is $b^i$ and the number of subproblems is $a^{k-i}$. By summing up, 
\begin{equation}
	T(b^k) = \sum_{i=0}^k a^{k-i}f(b^i)
\end{equation}
For a function $f(n)=n^\alpha$, 
\begin{equation}
	T(n) = a^k \sum_{i=0}^k \left(\frac{b^\alpha}{a}\right)^i = a^k \frac{1-(\frac{b^\alpha}{a})^{k+1}}{1-b^\alpha/a}
\end{equation}
Under the assumption that $\alpha \neq \log_b(a)$. As $k=\log_b(n)$, we can replace above and simplify using the formula $a^{\log_b(n)} = n^{\log_b(a)}$. Depending if $\alpha$ is bigger or larger than $\log_b(a)$, the simplifications change. In the case where $a=b^\alpha$, then 
\begin{equation}
	T(n) = a^k (k+1) = a^{\log_b(n)} (\log_b(n)+1) = \Theta(n^{\log_b(a)}\log(n))
\end{equation}
\section{Discrete Fourier Transform}
From the previous section, if we take the points as the $n$ complex roots of 1, i.e. $x_0 = e^{2\pi i/n}$ and $x_j = e^{2\pi ij/n}$, then we get a Discrete Fourier Transform matrix. Defining $\omega_n = x_0$,
\begin{equation}
	\mat{V} = \begin{bmatrix}
			1 & 1 & 1 & 1 \\
			\vdots & \omega_n & \dots & \omega_n^{n-1}\\
			\vdots & \ddots & \omega_n^{(i-1)(j-1)} & \vdots \\
			1 & \omega_n ^{n-1} & \dots & \omega_n^{(n-1)^2}
		\end{bmatrix}
\end{equation}
This gives us 
\begin{equation}
	\left(\mat{V}^{-1}\right)_{ij} = \frac{1}{n} \begin{bmatrix}
		\omega_n^{-(i-1)(j-1)}
	\end{bmatrix}
\end{equation}
and so $\mat{V} = \frac{1}{n}\mat{V}^*$. We conclude on 
\begin{equation}
	DFT(\textbf{x}) = \mat{V}\textbf{x}
\end{equation}
\subsection{Fast Fourier Transform}
The Fast Fourier Transform (FFT) algorithm uses the divide-and-conquer approach from above to compute the Fourier transform of a vector $x$. The exact algorithm is the following.\\
Assume that $n=2^k$, $k\ge 1$, $\textbf{x}=(x_0,\dots, x_{n-1})$ and $(y_0,\dots,y_{n-1}) = \mat{V}_n \textbf{x}$. The explicit formula to calculate $y_i$, $i=0,\dots,n/2-1$ is 
\begin{equation}
	\begin{aligned}
		y_i &= \sum_{j=0}^{n-1} x_j \omega_n^{ji} = \sum_{j=0}^{n/2-1}x_{2j}\omega_{n}^{2ji} + \sum_{j=0}^{n/2-1} x_{2j+1}\omega_n^{(2j+1)i} \\
		&= \sum_{j=0}^{n/2-1}x_{2j}\omega_{n/2}^{ji} + \omega^i_n\sum_{j=0}^{n/2-1} x_{2j+1}\omega_{n/2}^{ji}
	\end{aligned}
\end{equation}
And in matrix form:
\begin{equation}
	\begin{pmatrix}
		y_0 \\ y_1 \\ \vdots \\ y_{n/2-1}
		\end{pmatrix} = \mat{V}_{n/2} \begin{pmatrix}
			x_0\\x_2 \\ \vdots \\ x_{n-2}
		\end{pmatrix} + \begin{pmatrix}
			\omega_n^0 && \\ & \ddots & \\ & & \omega_n^{n/2-1}
	\end{pmatrix} \mat{V}_{n/2} \begin{pmatrix}
		x_1\\x_3\\\vdots \\x_{n-1}
	\end{pmatrix}
\end{equation}
And for the next half of $y$, we have a similar expression:
\begin{equation}
	\begin{aligned}
		y_{i+n/2} &= \sum_{j=0}^{n/2-1} x_{2j}\omega_{n}^{2ji}\left(\omega_{n/2}^{n/2}\right)^j + \omega_n^{i+n/2}\sum_{j=0}^{n/2-1} x_{2j+1}\omega_n^{ji}\left(\omega_{n/2}^{n/2}\right)^j \\
		&= \sum_{j=0}^{n/2-1}x_{2j}\omega_{n/2}^{ji} + (-1)\cdot \omega^i_n\sum_{j=0}^{n/2-1} x_{2j+1}\omega_{n/2}^{ji}
	\end{aligned}
\end{equation}
And, once again, in matrix form:
\begin{equation}
	\begin{pmatrix}
		y_{n/2} \\ y_{n/2+1} \\ \vdots \\ y_{n-1}
		\end{pmatrix} = \mat{V}_{n/2} \begin{pmatrix}
			x_0\\x_2 \\ \vdots \\ x_{n-2}
		\end{pmatrix} - \begin{pmatrix}
			\omega_n^0 && \\ & \ddots & \\ & & \omega_n^{n/2-1}
	\end{pmatrix} \mat{V}_{n/2} \begin{pmatrix}
		x_1\\x_3\\\vdots \\x_{n-1}
	\end{pmatrix}
\end{equation}
Let us define three notations: 
\begin{equation}
	\begin{aligned}
		\textbf{x}^{[0]} &= (x_0,\dots, x_{n/2-1})\\
		\textbf{x}^{[1]} &= (x_{n/2},\dots, x_{n-1})\\
		\mat{T}_n = \begin{pmatrix}
			\omega_n^0 & & \\ & \ddots & \\ & & \omega_n^{n/2-1}
		\end{pmatrix}
	\end{aligned}
\end{equation}
Then,
\begin{equation}
	\mat{DFT}(\textbf{x}) = \begin{pmatrix}
		\mat{DFT}(\textbf{x}^{[0]} + \mat{T}_n \mat{DFT}(\textbf{x}^{[1]}))\\
		\mat{DFT}(\textbf{x}^{[0]} - \mat{T}_n \mat{DFT}(\textbf{x}^{[1]}))
	\end{pmatrix}
\end{equation}
The time complexity of this algorithm is $\Theta(n\log(n))$. 
\begin{itemize}
	\item [$\rightarrow$] Note: there exists other algorithm to compute the FFT, such as the non power-of-two $n$ alorithm.
\end{itemize}
\section{Matrix multiplication}
Given $A,B\in \Z^{n\times n }$, we want to compute $C = A\cdot B$. The basic algorithm is in $\Theta(n^3)$, but we want a better one. 
\subsection{Straßen algorithm}
Let us divide the matrices in blocks:
\begin{equation}
	A = \begin{pmatrix}
		A_{11} & A_{12} \\ A_{21} & A_{22}
	\end{pmatrix} \qquad \qquad B = \begin{pmatrix}
		B_{11} & B_{12}\\ B_{21} & B_{22}
	\end{pmatrix} \qquad \qquad C = \begin{pmatrix}
		C_{11} & C_{12} \\ C_{21} & C_{22}
	\end{pmatrix}
\end{equation}
The Straßen algorithm uses the same idea as in section \autoref{sec:int_mult}: we define 7 block matrices to reduce the number of products done.
\begin{equation}
	\begin{cases}
		M_1 = (A_{11} + A_{22})(B_{11} + B_{22})\\
		M_2 = (A_{21} + A_{22})B_{11}\\
		M_3 = A_{11}(B_{12} - B_{22})\\
		M_4 = A_{22}(B_{21}-B_{12})\\
		M_5 = (A_{11} + A_{12})B_{22}\\
		M_6 = (A_{21} - A_{11})(B_{11}+B_{12})\\
		M_7 = (A_{12} - A_{22})(B_{21} + B_{22})
	\end{cases}
	\Longrightarrow C = \begin{pmatrix}
		M_1 + M_4 - M_5 + M_7 & M_3 + M_5\\ M_2+M_4 & M_1-M_2+M_3+M_6
	\end{pmatrix}
\end{equation}
Which has a complexity of $\Theta(n^{\log_2 7}) \approx \Theta(n^{2.81})$. The lower bound for the complexity is $\Omega(n^2)$, as we need to make at least one operation per element of $C$, and the current best algorithm (galactic) is in $\Theta(n^{2.371339})$. 
\subsection{Matrix inversion}
As we can assume intuitively, matrix multiplication and inversion are closely linked. Therefore, the complexity to compute both should be linked too. Let us call $M(n)$ the time complexity of multiplication of matrices of size $n$, and $I(n)$ the time complexity of inversion of a matrix of size $n$. Then,
\begin{equation}
	D = \begin{pmatrix}
		I & A & 0 \\ 0 & I & B \\ 0 & 0 & I
	\end{pmatrix}\Longrightarrow D^{-1} = \begin{pmatrix}
		I & -A & AB \\ 0 & I & -B \\ 0 & 0 & I
	\end{pmatrix}
\end{equation}
This means that the complexity of inversion is an upper bound on the complexity of multiplication: $M(n)\le I(3n)\le 3^3I(n) \ \forall n\ge n_0$.\\
This kind of inequality is called reduction. Given problems $A$ and $B$, if $A$ can be transformed in a problem $B$ of size $f(n)$ in time $T_R(n)$, then 
\begin{equation}\label{eq:reduction}
	T_A(n) \le T_R(n) + T_B(f(n))
\end{equation}
For example, the problem of finding the median of an array can be transformed into the problem of sorting the array.\\
Let us assume that $A=A^T$ and $A\succ 0$.
\begin{equation}
	A = \begin{pmatrix}
		B & C \\ C^T & D
	\end{pmatrix} \Longrightarrow A^{-1} = \begin{pmatrix}
		B^{-1} + B^{-1}CS^{-1}C^TB^{-1} & -B^{-1}CS^{-1}\\
		- S^{-1}C^TB^{-1} & S^{-1}
	\end{pmatrix}
\end{equation}
where $S = D-C^TB^{-1}C$ is the Schur complement of $A$. From \autoref{eq:reduction}, $f(n) = \Theta(M(n))$ in our case, and so we have the following relation between inversion and multiplication:
\begin{equation}
	I(n) = \mathcal{O}(M(n))
\end{equation}
\begin{itemize}
	\item [$\rightarrow$] Note: the hypotheses that $A=A^T$ and $A\succ 0$ are not binding. If $A$ is invertible but does not verify those conditions, we can work with $AA^T$ that does, and we find the inverse of $A$ with $A^{-1} =(A^TA)^{-1} A^T$. 
\end{itemize}
\begin{thm}
    Matrix inversion and multiplication have the same complexity.
\end{thm}
\chapter{Dynamic programming}
\section{Two approaches}
There are two approaches for a dynamic programming algorithm: bottom-up and top-down. In the bottom-up approach, we solve the subproblems first, and then use their solutions to solve bigger problems. In the top-down approach, we start from the main problem, and recursively solve the subproblems as needed. 
\begin{itemize}
	\item The main idea of bottom-up is to use memoization, i.e. storing the solutions of subproblems to avoid recomputing them. 
	\item The main idea of top-down is recursion.
\end{itemize}
Dynamic programming is used to improve time complexity by trading time for space.
\section{Examples}
\subsection{Rod cutting }
The problem of rod cutting consists in wanting to cut a beam of length $n$, given commands of clients.Finding the optimal solution can be done by computing a solution on sub-beams and merging them. 
\begin{algorithm}[H]
\caption{Rod Cutting Algorithm}
\begin{algorithmic}[1]
\Function{CutRod}{$p, n$}
    \If{$n = 0$}
        \State \Return $0$
    \EndIf
    \State $q \gets -\infty$
    \For{$i = 1$ to $n$}
        \State $q \gets \max\{q,\, p_i + \Call{CutRod}{p, n-i}\}$
    \EndFor
    \State \Return $q$
\EndFunction
\end{algorithmic}
\end{algorithm}
The complexity of this algorithm is given by a recurrence relation:
\begin{equation}
	T(n) = 1 + \sum_{j=0}^{n-1}T(j) = 1 + T(n-1) + \sum_{j=0}^{n-2}T(j) = 2T(n-1) \Longrightarrow T(n) = \mathcal{O}(e^n)
\end{equation}
This basic algorithm has a very bad complexity, but memoization can improve it.
\begin{algorithm}[H]
	\caption{Memoized rod cutting algorithm}
	\begin{algorithmic}[1]
		\Function{CutRod\_Memoized}{$p,n$}
		\State $r[0:n] = -\infty $
		\Function{CRM\_Aux}{$p,n,r$}
		\If{$r[n]\ge 0$}
		\State \Return $r[n]$
		\EndIf 
		\If {$n=0$}
		\State \Return $0$
		\EndIf 
		\State $q=-\infty$
		\For{i=1 to n}
		\State $q=\max\{q, p_i + CRM\_Aux(p,n-i,r)\}$
		\EndFor
		\State $r[n]=q$\\
		\qquad \: \: \Return $q$
		\EndFunction\\
		\qquad \Return CRM\_Aux(p,n,r)
		\EndFunction 
	\end{algorithmic}
\end{algorithm}
Finally, another algorithm exists, using a bottom-up approach in dynamic programming. It has a time complexity $\Theta(n^2)$.
\begin{algorithm}[H]
	\caption{Bottom-up efficient rod cutting algorithm}
	\begin{algorithmic}[1]
		\Function{Bottom-up-CR}{$p,n$}
		\State $r[0:n]$
		\State $r[0] = 0$
		\For {j=1 to n}
		\State $q=-\infty$
		\For{i=1 to j}
		\State $q=\max\{q,p_i+r_{j-1}\}$
		\EndFor 
		\State $r[j] = q$
		\EndFor

		\Return $r[n]$
		\EndFunction 
	\end{algorithmic}
\end{algorithm}
\section{Principles of dynamic programming}
\begin{itemize}
    \item Principle of optimality: for any optimal structure (e.g. list, tree) answering the problem for an instance, the substructures are also optimal for the subinstances.
    \item Subproblems are overlapping: divide-and-conquer is wasteful because it solves many times the same subinstance. 
\end{itemize}
\section{Generating functions}\label{sec:generating-functions}
This is a powerful tool for solving recurrence equations. The idea is to associate a power series to a sequence $\{a_n\}_{n\in \N}$: 
\begin{equation}
    f(z) = \sum_{n=0}^\infty a_n z^n \qquad z\in \C 
\end{equation}
Let us define $m[i,j]$ the optimal cost of the product $A_i\dots A_j$. Our goal is to compute $m[1,n]$, with the initial condition $m[i,i] = 0$ for all $i$. The recurrence relation is 
\begin{equation}
	m[i,j] = \min_{i\le k < j} \{m[i,k] + m[k+1,j] + p_{i-1}p_kp_j\} \qquad \forall 1\le i\le j \le n	
\end{equation}
This gives a complexity $\Theta(n^2)$, as we need to compute the entries of a triangular matrix $n\times n$. 
\section{Generating functions}
Let $\{a_k\}_{k=0}^\infty$ be a sequence. We associate the function $\sum_{k=0}^\infty a_kx^k$ to the sequence, on which we have addition, multiplication, differentiation, etc. \\ Example: 
\begin{equation}
	a_k = 1 \ \forall k\in \N \Longrightarrow f(x) = \sum_{k=0}^\infty x^k = \frac{1}{1-x} = x \sum_{k=0}^\infty x^k + 1 = xf(x) + 1
\end{equation}
Another example is the Fibonacci sequence:
\begin{equation}
	f_{n+2} = f_{n+1} + f_n \qquad f_0 = 0\qquad f_1 = 1
\end{equation}
From the recurrence equation, we can get the generative function:
\begin{equation}
	\begin{aligned}
		f(x) &= f_0 x^0 + f_1 x^1 + \sum_{k\ge 2} f_k x^k = f_0 + f_1 x + \sum_{k\ge 0} f_{k+2}x^{k+2}\\
		&= f_0 + f_1 x + x^2\sum_{k\ge0 } f_{k+1} x^k + x^2\sum_{k\ge 0}f_k x^k\\
		&= f_0 + f_1 x + x(f(x)-f_0) + x^2 f(x)\\
		&= \frac{f_0 + (f_1-f_0)x}{1-x-x^2}
	\end{aligned}
\end{equation}
Knowing that $\sum_{k\ge0 } f_{k+1} x^k = \frac{f(x)-f_0}{x}$ 
\chapter{Randomized Algorithms}
\section{Probabilistic analysis and randomized algorithm}
In probabilistic computing, we need a distribution of the inputs to be able to sample. Based on prior knowledge or assumptions, we can determine the average-case complexity. The goal is for it to be lower than the deterministic algorithm, although it comes at the price of the accuracy of the solution.\\

Let us use as example the hiring problem. We interview $n$ candidates, one at a time, and we need to decide wether we hire the candidate or not right after the interview. Our goal is of course to hire the best candidate. \\
One approach would be to interview the $n/2$ first candidates and only observe their skills. Then, hire the first candidate that is more skilled than the average (or all) of the first half of candidates.
\subsection{Indicator Random Variable}
Given a sample space $S$ and an event $A$, we define the indicator random variable as 
\begin{equation}
	X_A = \begin{cases}
		1 \text{ if $A$ occurs }\\ 0 \text{ otherwise}
	\end{cases}
\end{equation}
where $Pr[X_A = 1] = Pr[A]$. This is useful because $\E[X_A] = Pr[A]$.
\section{Random Sampling and Applications}
\subsection{Birthday paradox}
The birthday paradox is not stricly speaking a paradox, but is called that way because it goes against the first intuition. What is the number of people that must be in a room so that the probability that two people have the same birthday is higher than 1/2, assuming that the birthdays are uniformly distributed? Let $r={1,\dots,n}$ and $Pr[b_i=r] = 1/n$. Then, for two people,
\begin{equation}
	Pr[b_i = b_j] = \sum_{r=1}^n Pr[b_i=r \text{ and }b_j = r] = \sum_{r=1}^n Pr[b_i=r]Pr[b_j=r] = \sum_{r=1}^n \frac{1}{n^2} = \frac{1}{n}
\end{equation}
And for $k$ people, denoting $B_k$ the event that $k$ people have distinct birthdays, we have 
\begin{equation}
	\begin{aligned}
		Pr[B_k] &= 1\cdot \frac{n-1}{n}\frac{n-2}{n}\dots \frac{n-k+1}{n} = 1\cdot \left(1-\frac{1}{n}\right)\cdot \left(1-\frac{2}{n}\right)\dots \left(1-\frac{k-1}{n}\right)\\
		&\le e^{-1/n}e^{-2/n}\dots e^{-(k-1)/n} = e^{-k(k-1)/2n}
	\end{aligned}
\end{equation}
Then, for $Pr[B_k]\le 1/2$, we get 
\begin{equation}
	k(k-1) \ge 2n \ln{2}\Longleftrightarrow k\ge 23
\end{equation}
We can also compute the expectation: let $X_{ij}$ be the 1 if the people $i$ and $j$ have the same birthday and 0 otherwise. Then $\E[X_{ij}] = 1/n$ and for $X = \sum_{i=1}^k\sum_{j=i+1}^k X_{ij}$, 
\begin{equation}
	\E[X] = \sum_{i=1}^k \sum_{j=i+1}^k \E[X_{ij}] = \frac{k(k-1)}{2n}
\end{equation}
This means that for at least $\sqrt{2n}+1$ people in a room, we can EXPECT at least one collision. For $n=365$, this is $k=28$ people.
\subsection{Hash table}
A hash table is a data structure in which inserting, searching and deleting is done in $\Oo(1)$ in average. The idea of a hash table is to store elements in an array of size $m$ using a hash function $h:U\to \{0,\dots,m-1\}$, where $U$ is the set of possible keys. So instead of using a direct addressing table, where $k$ is stored at $T[k]$ ($\Oo(n)$ space), we store $k$ at $T[h(k)]$ ($\Oo(m)$ space). This idea save space but introduces collision. We can deal with collisions using multiples way, this will be investigated later. The simplest hash function is the modulo function $h(k) = k \mod m$.\\
Using deterministic hash functions can lead to bad performance. If the input is well chosen, we could have a worst-case complexity of $\Oo(n)$. To overcome this, we randomize the choice of the hash function.
\begin{definition}
	A family of hash functions $\mathcal{H}$ is universal if for all $k\neq l \in U$,
	\begin{equation}
		Pr_{h\in \mathcal{H}}[h(k) = h(l)] \le \frac{1}{m}
	\end{equation}
\end{definition}
This definition means that even with well-chosen inputs, the probability of collision is garanteed to be low. An example of universal hash function family is the following:
\begin{equation}
	\mathcal{H} = \{h_{a,b}(k) = ((ak+b) \mod p) \mod m \: | \: a\in \{1,\dots,p-1\}, b\in \{0,\dots,p-1\}\}
\end{equation}
where $p$ is a prime number $p > |U|$. The randomness comes from the random choice of $a$ and $b$.\\
There exists multiples ways to manage collisions. It is possible to ignore collisions, in that case you lose data. Another way is to use buckets with linked lists to store multiple elements at the same index. We can also use probing. Linear probing consists into putting the element at the next available index after a collision, so we try $h(k)$ then $h(k)+1$ then $h(k)+2$ and so on. Quadratic probing uses $h(k)+i^2$ instead of $h(k)+i$. Double hashing uses a second hash function $h_2$ to compute the next index: $h(k)+i\cdot h_2(k)$. And finally, Cuckoo hashing that consists in using two hashing function ($h_1(k)$ and $h_2(k)$) and allowing an item $k$ to be either in the place given by $h_1(k)$ or by $h_2$.
\section{Monte Carlo algorithm to compute an integral}
Assume that we would like to compute $\int_a^b f(x)dx$, given that $f(x)\ge 0$, for $f$ bounded in $[0,c]$. We can use the following randomized algorithm:
\begin{algorithm}[H]
    \caption{Random algorithm to compute an integral}
    \begin{algorithmic}[1]
        \State $k=0$;
        \For{$n$ times}
            \State Pick $(x,y)$ in $[a,b]\times [0, c]$;
            \State If $y\le f(x)$ : $k ++$;
        \EndFor \\
        \Return $(b-a)\cdot c\cdot \frac{k}{n}$
    \end{algorithmic}
\end{algorithm}
Or a deterministic algorithm, where the upper bound on $f$ is not needed:
\begin{algorithm}[H]
    \caption{Deterministic algorithm to compute an integral}
    \begin{algorithmic}[1]
        \State $s=0; \: x= a; \: D = (b-a)/n$;
        \For{$n$ times }
            \State $x += D$; 
            \State $s += f(x)$;
        \EndFor\\
        \Return $s\cdot D$
    \end{algorithmic}
\end{algorithm}
This may be used to approximate $\pi$, since \begin{equation}
    \frac{\pi}{4} = \int_0^1 \sqrt{1-x^2}dx
\end{equation}
And we can even compute the error using the variance of $X_n = \sum_{i=1}^nX_i$:
\begin{equation}
    \alpha(y|x) = \min\left\{1, \frac{f(y)g(x|y)}{f(x)g(y|x)}\right\}
\end{equation}
We set $x_{t+1} = y$ with probability $\alpha$ and $x$ otherwise. The resulting transition probability for the Metropolis-Hastings random walk from $x$ towards any $y\neq x$ are thus 
\begin{equation}
	Pr[P(r_1,\dots,r_n)=0] \le \frac{d}{|S|}
\end{equation} 
We can prove it by induction:
\begin{equation}
	P(x_1,\dots,x_n) = \sum_{i=0}^d x_1^iP_i(x_2,\dots,x_n)
\end{equation}
for some polynomials $P_i$ of degree at most $d-i$ and $n-1$ variables. We can go all the way to polynomials of one single variable and use the fundamental theorem of algebra.\\

From this lemma, the probability to predict correctly from a set of points if a polynomial is identically zero or not is 
\begin{equation}
    P(x|z) = \frac{P(z|x)P(x)}{\sum_{x\in X} P(z|x)P(x)}
\end{equation}
with $Pr[TRUE | P\equiv 0] = 1$ and $Pr[FALSE | P\not \equiv 0] \ge 1-\left(\frac{d}{|S|}\right)^k$. Then,
\begin{equation}
	Pr[correct] \ge 1-(d/|S|)^k
\end{equation}
\subsubsection{Matrix product}
Given $A,B,C\in \R^{n\times n}$, we want to check if $AB=C$. Doing the product is costly, so we can take random vectors $x\in \R^n$ and check the products:
\begin{equation}
	A(Bx) -Cx\overset{?}{=}0
\end{equation}
This is a $n$-variate polynomial of degree 1 and we can use the same method as in the previous section.
\subsubsection{MaxCut Problem}
Given a graph $G=(V,E)$, we want to partition $V$ into two sets $S_0$ and $S_1$ such that we maximize edges crossing the cut. If we assign each vertex randomly to one of the two sets with a probability $1/2$, then:
\begin{equation}
	\E[|\text{RandomCut}(S_0,S_1)|] \geq \frac{1}{2}\text{MaxCut}(G)
\end{equation}
\subsection{Amplification of Stochastic Advantage}
A randomized algorithm may be wrong but it has a bias towards the correct answer. The idea is to run $k$ times the algorithm to take advantages of the properties of the randomness. For example, for one-sided error, with Schwartz-Zippel, we know that we have no false negative. So if we get at least one TRUE in $k$ runs, we can be sure that it is TRUE. For two-sided error, we can run $k$ times independently and take the majority answer.
\section{Las Vegas and Monte Carlo}
An algorithm is said to be Las Vegas when the output is correct, but the time taken to get to this solution is random. On the other hand, an algorithm is said to be Monte Carlo when the output is correct with some probability, or we only get a bound on the real value.
\section{Randomness generation}
Let us take functions $f,g:X\to Y$ and an element $s\in X$. We define the sequence
\begin{equation}
    \begin{aligned}
        Pr[correct] &= Pr[\text{return } P\equiv 0 \text{ and }P\equiv 0] + Pr[\text{return }P\not \equiv 0 \text{ and }P\not \equiv 0]\\
        &= Pr[\text{return }P\equiv 0|P\equiv 0] Pr[P\equiv 0] + Pr[\text{return }P\not \equiv 0| P\not \equiv 0]Pr[P\not \equiv 0]\\
        &\ge Pr[P\equiv 0] + \left(1-\frac{d}{|S|}\right) Pr[P\not \equiv0] \ge 1 - \frac{d}{|S|}
    \end{aligned}
\end{equation}
The output of this pseudo-random generator (PRG) is the sequence (usual bits) $y= (y_0,\dots,y_n)$. One famous example of pseudo-random generator is Blum-Blum-Shub (BBS): \\
select random $\kappa$-bit prime integers $p,q$ such that $p=q=3 \mod 4$, and let $N=pq$. The PRG is 
\begin{equation}
    \begin{cases}
        \E[\sum_{i}X_i] = n \left(\frac{1}{2}+\varepsilon\right) \\
        Var[X_i] = \E[X_i^2] - \E[X_i]^2 = \E[X_i] - \E[X_i]^2 = \frac{1}{4}-\varepsilon^2 \\
        \Longrightarrow Var[\sum_i X_i] = n \left(\frac{1}{4}-\varepsilon^2\right)
    \end{cases}
\end{equation}
\section{Derandomization}
Derandomization transforms a randomized algorithm into a deterministic algorithm, or at least into an algorithm using less randomness, without increasing time and memory costs too much. 
\subsection{Techniques}
\begin{itemize}
    \item Use a PRG: cheap but still needs some randomness;
    \item Try all possible random values: remove all randomness but can be a polynomial time slowdown.
\end{itemize}
\chapter{Computability}
\section{Turing machine}
A Turing machine is made of the following elements:
\begin{itemize}
	\item An infinite tape serving as unlimited memory;
	\item A head that reads and write the symbols and moves around the tape;
	\item A specific blanc symbol \textvisiblespace or $\epsilon$;
	\item The number of states must be finite.
\end{itemize}
\subsection{Formal definition}
\begin{definition}
	A Turing machine is a tuple $(Q, \Sigma, \Gamma, \delta, q_0,q_{accept},q_{reject})$ where $Q, \Sigma, \Gamma$ are finite sets and 
	\begin{itemize}
		\item $Q$ is the set of states;
		\item $\Sigma$ is the input alphabet (not containing the blank symbol);
		\item $\Gamma$ is the tape alphabet, where \textvisiblespace$\in \Gamma$ and $\Sigma \subseteq \Gamma$;
		\item $\delta:Q\times \Gamma \to Q\times \Gamma \times \{L,R\}$ is the transition function\footnote{We can add a "stay put" move $S$: $\{L,R,S\}$.};
		\item $q_0\in Q$ is the initial state;
		\item $q_{accept}\in Q$ is the accept state and $q_{reject}\in Q$ is the reject state and is different from the accept state. 
	\end{itemize}
	For $a,b\in \Gamma$, and $q,r\in Q$, $\delta(q,a)=(r,b,L)$ means that if the machine is in state $q$ and the head is over a tape square containing a symbol $a$, it replaces $a$ with $b$, the state becomes $r$, and the head moves one step to the left. 
\end{definition}
The Turing machine (TM) $M$ receives its input $w=w_1\dots w_n\in \Sigma^*$ on the leftmost $n$ squares of the tape. Furthermore, $M$ never tries to move its head to the left of the left-hand end of the tape (it stays in the same place even though $\delta$ indicates $L$). \\
As a TM computes, changes occur in the current state, in the tape content and in the head location. A setting of these 3 items is called a configuration of the TM. \\
For a state $q$ and strings $u,v$ over $\Gamma$, we note $uqv$ for the configuration where the current state is $q$, the tape content is $uv$, and the current head location is the first symbol of $v$. \\
\begin{example}
    Assume we are in the configuration $uaq_ibv$ for $u,v\in \Gamma^*$, $a,b\in \Gamma$ and $q_i\in Q$. Then, if $\delta(q_i,b) = (q_j, c, L)$, the next configuration will be $uq_jacv$.
\end{example}
A Turing machine M accepts input $w$ if a sequence of configurations $c_1,\dots,c_k$ exists where 
\begin{itemize}
    \item $c_1 = q_0 w$ is the start configuration of M on input $w$;
    \item each $c_i$ yields $c_{i+1}$;
    \item $c_k$ is an accepting configuration.
\end{itemize}
The collection of strings that $M$ accepts is the language of $M$, or the language recognized by $M$, denoted $\mathcal{L}(M)$. A language is \textbf{Turing-recognizable} if some Turing machine recognizes it.\\
A decider is a Turing machine that halts on all inputs. A decider that recognizes some language decides it. A language is \textbf{Turing-decidable} if some TM decides it. 
\subsection{Example}
\begin{figure}[H]
	\centering
	\includegraphics[width = .7\textwidth]{img/turing_example.png}
	\caption{Example of a Turing machine }
	\label{fig:turing}
\end{figure}
The language $A=\{0^{2^n}|n\ge 0\}$ is decidable. Let $M_2 = (Q, \Sigma, \Gamma, \delta, q_1, q_{accept}, q_{reject})$ such that 
\begin{itemize}
	\item $Q = \{q_1,\dots,q_5,q_{accept}, q_{reject}\}$ with $q_1$ the initial state;
	\item $\Sigma=\{0\}$ and $\Gamma = \{0,x,$\textvisiblespace$\}$;
	\item $\delta$ is the \autoref{fig:turing}.
\end{itemize}
Here is the breakdown of the steps:
\begin{itemize}
	\item $q_1$: reads the first symbol and if it is $0$, replaces it with \textvisiblespace and moves right to state $q_2$. If it is something else ($x$ or \textvisiblespace), then the input is invalid and reject.
	\item $q_2$: moves to the right whatever the bit is. If the bit is $0$, then replaces it with $x$ and moves to $q_3$. If it is \textvisiblespace, then there is no more unmarked $0$ and goes to accept.
	\item $q_3$: If the bit is $0$, then moves to the right and to state $q_4$ and does nothing. If the bit is \textvisiblespace, then goes back to the left and moves to state $q_5$. If the bit is $x$, goes to the right and no change of state. 
	\item $q_4$: If the bit is 0, then marks it $x$ and goes to the right, back to state $q_3$. If it is $x$, goes to the right with no change of state. If it is \textvisiblespace, goes to the right in a reject configuration.
	\item $q_5$: if the bit is $0$ or $x$, goes back to the left and no change of state. If the bit is \textvisiblespace, goes to the right and back to state $q_2$. 
\end{itemize}
What if we allowed the head to stay put? This is equivalent to the previous machine, as one model can simulate the other, and conversely: they are equivalent as they recognize the same languages. 
\section{Multitape Turing Machines}
\begin{definition}
	A multitape Turing machine is a TM with several tapes, each with its own head. For $k$ tapes, the transition function then becomes 
\end{definition}
\begin{equation}
	\delta : Q\times \Gamma^k\to Q\times \Gamma^k\times \{L,R,S\}^k
\end{equation}
Each multitape Turing Machine has an equivalent single-tape Turing machine. 
\section{Enumerator}
\begin{definition}
	An \textbf{enumerator} is a Turing machine that always starts with a blank input on its tape, and that has an attached printer to print strings. Every time it wants to print a string to the list, it sends it to the printer. An enumerator does not have to halt, and may print an infinite list of strings. The language it enumerates is the collection of all strings that is printed out, and it can generate the strings in any order, possibly with repetitions.
\end{definition}
\begin{thm}
    A language is Turing-recognizable iff some enumerator enumerates it. 
\end{thm}
\begin{proof}
    $\Longrightarrow$: Let $E$ be an enumerator that enumerates $A$. The Turing machine that we define is $M$ = "on input $w$, run $E$; every time that $E$ outputs a string, compare it with $w$. If it is the same, accept; else continue."\\
    $\Longleftarrow$: Let $M$ be a TM that recognizes $A$. Say $s_1,\dots$ is a list of all strings in $\Sigma^*$. We take as enumerator
    $E$ : "ignore the input. Repeat the following for $i=1,2,\dots$: run $M$ for $i$ steps on each input $s_1,\dots,s_i$. If any computation accepts, then print out the corresponding $s_j$."\\
    If $M$ accepts a particular string $s$, eventually it will appear on the list generated by $E$\footnote{In fact, it will apear on the list infinitely many times because $M$ runs from the beginning on each string for each repetition.}.
\end{proof}
\section{Non-deterministic Turing Machines}
A non-deterministic TM proceeds according to several possibilities. The transition function returns a probability distribution over $Q\times \Gamma\times\{L,R\}$. The computation of a non-deterministic TM is a tree whose branches correspond to different possibilities for the machine. If some branch of the computation leads to the ACCEPT state, the machine accepts. 
\begin{thm}
	As a non deterministic TM can be represented as a tree, every non deterministic TM has an equivalent deterministic TM that goes through every possible state of the tree. 
\end{thm}
\begin{proof}
    Let $N$ be a non-deterministic TM. We design $D$, a TM inspecting the branches of $N$'s tree by using BFS. Let $b$ be the size of the largest set of possible choices given $N$'s transition function. To every node in the tree, we assign an address as a string over $\Gamma_b =\{1,\dots,b\}$. \\
    $D$ has three tapes: the first correspond to the input tape, the second to the simulation tape, and the last to the address tape. 
    \begin{enumerate}
        \item Initially, tape 1 contains the input $w$, and tapes 2 and 3 are empty;
        \item Then, we copy tape 1 on tape 2 and initialize the string on tape 3 to be $\Gamma_b$;
        \item We use tape 2 to simulate $N$ with input $w$ on one branch of its non-deterministic computation. Before each step of $N$, we consult the next symbol o, tape 3 to determine which choice to make among those allowed by $N$'s transition function. If no more symbols remain on tape 3 or if this non-deterministic choice is invalid, abort this branch by going to the next step. Also go to next step if a REJECT configuration is encountered. If an ACCEPT configuration is encountered, accept the input.
        \item Replace the string on tape 3 with the next string in the string ordering. Simulate the next branch of $N$'s computation by going back to step 2. 
    \end{enumerate}
\end{proof}
\begin{corollary}
    A language is Turing-recognizable iff a non-deterministic TM recognizes it. 
\end{corollary}
The Church-Turing thesis is that our intuitive notion of algorithm is captured by a TM. Notice that TMs can always be designed to first check the validity of the encoding. 
\subsection{10th Hilbert's problem}
"Devise a process according to which it can be determined by a finite number of operation that a given multivariate polynomial has an integer root."\\

For an univariate polynomial, we must check if there exists $a\in \Z$ such that $P(a)=0$. We know also that the roots are in the interval $\left[-n\frac{c_{\max}}{c_1}, n\frac{c_{\max}}{c_1}\right]$. Therefore, there is a finite number of integer values to check, and we can evaluate the polynom $P(x)$ at each of those values. However, for a multivariate polynomial, it is impossible. \\
The rigorous formulation of the 10th Hilbert problem is to find a decider for $D = \cup_n D_n$ with 
\begin{equation}
	D_n = \{P\in \Z[x_1,\dots,x_n] \: |\: \exists a\in \Z^n \: : \: P(a) = 0\}
\end{equation}
\section{Universal Turing Machine}
\begin{definition}
	
	A universal Turing machine is a TM that solves the following problem:
	\begin{itemize}
		\item Input: a description of the TM (T) and a description of a finite word on an initial tape ($w$);
		\item Output: $T(w)$, i.e. the finite content of the tape when $T$ stops; and undefined if $T$ never stops.
	\end{itemize}
\end{definition}
\chapter{Decidability}
\section{Countable sets}
\begin{itemize}
	\item Two sets have the same number of elements iff they are in bijective relation;
	\item A set $S$ is countable if it either is finite or has the same size as $\N$, e.g. $\Z, \Q, \N^d,\dots$; 
\end{itemize}
\section{Undecidable language}
\begin{thm}
    \begin{equation}
        A_{TM} \coloneqq \{\langle M, w\rangle | M\text{ is a TM and $M$ accepts }w\}
    \end{equation}
    is undecidable.
\end{thm}
This notation is the way to write a universal Turing machine with inputs $\langle M, w\rangle$, $M$ being a Turing machine and $w$ a string. Here is the proof of the theorem, by contradiction:\\
Let $H$ be a decider for $A_{TM}$. Then
\begin{equation}
	H(\langle H,w\rangle) = \begin{cases}
		\text{accept} & \text{ if } M \text{ accepts }w\\
		\text{reject} & \text{ if } M \text{ does not accept }w
	\end{cases}
\end{equation}
Build a Turing machine $D$ that, on input $\langle M\rangle$, runs $H$ on $\langle M, \langle M\rangle \rangle$ and halts with $\overline{H(\langle M,w\rangle)}$, i.e. $D$ returns the opposite of $H$:
\begin{equation}
	D(\langle M\rangle) = \begin{cases}
		\text{accept} & \text{ if } M \text{ does not accept } \langle M\rangle\\
		\text{reject} & \text{ if } M \text{ accepts } \langle M\rangle
	\end{cases}
\end{equation}
But then, $D(\langle D\rangle)$ is a contradiction with itself. \textcolor{red}{Define notation $\langle M\rangle$!}
\section{Unrecognizable language}
\begin{thm}
	A language is decidable iff it is Turing-recognizable and co-Turing-recognizable.
\end{thm}
Proof: \\
\begin{itemize}
	\item $\Rightarrow$ is trivial;
	\item $\Leftarrow$
\end{itemize}
Let $M_1$ be a recognizer for $A$, and $M_1$ a recognizer for $\overline A$. We build $M$ that runs $M_1$ and $M_2$ in parallel, one step at a time, on input $w$. If $M_1$ accepts, ACCEPT, and if $M_2$ accepts, REJECT. 

Therefore, $\overline{A_{TM}} = \{\langle M, w\rangle|M\text{ is a TM and }M \text{	does not accept }w\}$ is not Turing-recognizable. 
\chapter{Complexity Theory}
\section{Definitions}
\begin{definition}
	Let $M$ be a deterministic TM that halts on all inputs. The running time, or time complexity, of $M$ is the function $f:\N\to \N$ where $f(n)$ is the maximum number of steps that $M$ uses on any input of length $n$.
\end{definition}
\begin{definition}
	Let $t:\N\to \R^+$ be the time complexit class TIME$(t(n))$, i.e. the collection of languages that are decidable by an $\mathcal{O}(t(n))$ time TM. 
\end{definition}
\subsection{Example}
Let $\{0^k1^k|k\ge 0\}$ be a string composed of $k$ 0 followed by $k$ 1. The goal is to check if the number of 0 is the same as the number of 1. \\
One method would be to start at the first element. If it is 0, cross it and go right until we find a 1. Then, go left until we find the first 0, and go again. Once we have no more 0 or 1, we check that it is also the case for the other character. If not, return FALSE. The complexity is $\mathcal{O}(k^2)$.\\
Another method is to cross every other 0 and do the same with the 1. With what is left, do it again (so one every four characters), and so on. At the end, compare if there is one character left in one of the strings. The complexity of this is $O(k\log(k))$. 
\begin{itemize}
	\item [$\to$] Note: in the case where a counter variable is available, the overall complexity is not better, as the complexity to increment it is $\mathcal{O}(\log(k))$. 
\end{itemize}
\section{Complexity of multitape}
\begin{thm}	\label{thm:multitape}
	Let $t(n)\ge n$, as we can reasonably assume that we need to go through the whole input to find the solution. Every $t(n)$ time multitape TM has an equivalent $\mathcal{O}(t^2(n))$ time single-tape TM. 
\end{thm}
\section{Complexity of non deterministic TM}
Let $N$ be a nondeterministic TM that is a decider. The running time of $N$ is the function $f:\mathcal{N}\to \mathcal{N}$ where $f(n)$ is the maximum number of steps that $N$ uses on any branch of its computation on any input of length $n$.
\begin{thm}
    Comparing two programs (determining if $P_1(x)=P_2(x)$, $\forall x$) is undecidable.
\end{thm}
This is because the number of nodes is in $\mathcal{O}(b^{f(n)})$, and the number of operations for one node is $\mathcal{O}(n+f(n))$. $b$ is the number of possibilities at each node. This gives a total complexity of
\begin{equation}
	\mathcal{O}\left((n+f(n))b^{f(n)}\right) = \mathcal{O}\left(2^{\log_2(b)(\log_b(f(n)) + f(n))}\right) = 2^{\mathcal{O}(f(n))}
\end{equation}
The result is the same for a multitape or single-tape TM, as (from \autoref{thm:multitape}):
\begin{equation}
	2^{\mathcal{O}(f(n))} \rightarrow \left(2^{\mathcal{O}(f(n))}\right)^2 = 2^{\mathcal{O}(f(n))}
\end{equation}
\section{The class P}
$P$ is the class of languages that are decidable in polynomial time on a deterministic single-tape TM. In other words, 
\begin{equation}
    P = \bigcup_k TIME(n^k)
\end{equation}
This class is the same for all reasonable languages we may use to code the algorithm, as coding on a TM involves only a polynomial slowdown with respect to higher level languages. \\
We define the complement of $P$: $coP \coloneqq \{L|\bar L \in P\}$. 
The complementary of a decision problem $A$, $\bar A$, is defined by the property that the positive instances of $\bar A$ are the negative instances of $A$ and conversely.
\begin{thm}
    $coP = P$.
\end{thm}
\begin{thm}
    There is a decidable problem not in $P$.
\end{thm}
\begin{proof}
    We enumerate a list of Python programs that solve all problems in $P$? We know that every problem in $P$ is solved by a Python machine in this list, and every Python machine in thislist runs in polynomial time. Let us call this list $(M_i)_{i\in \N}$ and let us suppose that we enumerate the possible inputs as $1,2,3,4,\dots$\\
    We create a problem $A$ with input $n$ and that outputs YES if $M_n(n) = No$, and NO if $M_n(n) = Yes$. This problem outputs the opposite of the diagonal of the table (problems $\times$ inputs), and is not in $P$ because the diagonal is not a row of this table. This implies that we found a problem $A$ not in $P$. 
\end{proof}
\textbf{EXPTIME} is the class of languages that are decidable in exponential time on a deterministic single-tape TM. In other words, 
\begin{equation}
    EXPTIME = \bigcup_k TIME(2^{n^k})
\end{equation}
\begin{itemize}
    \item [$\to$] Note: the diagonal problem constructed in the proof above is in EXPTIME. In fact, we can repear the argument for any time-complexity class, and create a hierarchy of larger and larger time-complexity classes. 
\end{itemize}
\textbf{PSPACE} is the class of languages that are decidable by an algorithm using a polynomial amount of space. 
\begin{thm}
    $P\subseteq PSPACE$.
\end{thm}
\begin{proof}
    In $N$ steps of time, one can only write $N$ new symbols in memory. The memory use is lower than $n+N$ where $n$ is the input size. Hence, if $N\in \mathcal{O}(n^d)$, then $n+N\in \mathcal{O}(n^d)$ for all $d$.
\end{proof}
\begin{thm}
    $PSPACE \subseteq EXPTIME$.
\end{thm}
\section{The class NP}
\begin{definition}
    A verifier for a language $\mathcal{L}$ is an algorithm $V$ where 
    \begin{equation}
        \mathcal{L} = \{w| V\text{ accepts }\langle w,c\rangle \text{ for some string }c\}
    \end{equation}
\end{definition}
We measure the time of a verifier only in terms of the length of $w$, so a polynomial-time verifier runs in polynomial time in the length of $w$. 
\begin{definition}
    NP is the class of languages that have a polynomial-time verifier.
\end{definition}
\begin{thm}
    A language is in NP iff it is decided by some non deterministic polynomial time TM.
\end{thm}
\textbf{NTIME}(t(n)) is the class of languages that are decided by an $\mathcal{O}(t(n))$ time non-deterministic TM. 
\begin{equation}
    NP = \bigcup_k NTIME(n^k)
\end{equation}
Unlike P and PSPACE, NP is not closed under complementation: $NP\neq coNP \coloneqq \{\text{problems whose complementary is in NP}\}$. 
\begin{thm}
    $P\subseteq NP\subseteq PSPACE\subseteq EXPTIME$.
\end{thm}
The only thing that we are sure of is that $P \subsetneq EXPTIME$. 
\subsection{Example -- SAT}
The SAT problem consists in finding boolean variables $\{x_n\}_{n\ge 0}$ such that a combination of boolean operators applied to them returns 1. For example,
\begin{equation}
	\exists? \{x_1,x_2,x_3\}\text{ such that } \left((x_1 \wedge x_2) \vee  \overline{(x_2 \vee x_3)}\right) \wedge x_3 = 1
\end{equation}
It is very hard to find a solution, but easy to check. 
\subsubsection{Conjunctive normal form}
We define the conjunctive normal form as 
\begin{equation}
	\phi = \bigwedge_i c_i \qquad c_i = \bigvee_j l_j \qquad l_j = \begin{cases}
		x_k \\ \overline{x_k}
	\end{cases}
\end{equation}
We call $c_i$ the clauses and $l_j$ the literals. \\
The 3SAT problem add the constraint that there are at most 3 $l_j$ per clause. \\

The SAT problem is NP-complete.
\section{NP-completeness}
A function $f:\Sigma^*\to \Sigma^*$ is a polynomial time computable function if some polynomial time TM exists that halts with $f(w)$ on its tape when started on any input $w$. \\
A language $A$ is polynomial time reducible to a language $B$, written $A \le_P B$ if there exists a polynomial time computable function $f$ such that for every $w$, $w\in A\Longleftrightarrow f(w)\in B$. We call $f$ the reduction of $A$ to $B$. 
\begin{thm}
    If $A\le_P B$ and $B\in P$, then $A\in P$.
\end{thm}
\begin{definition}
    A language $B$ is NP-complete if $B\in NP$ and every $A\in NP$ is polynomial-time reducible to $B$.
\end{definition}
\begin{definition}
	A language $A$ is polynomial time reducible to a language $B$, written $A\le_P B$, if there exists a polynomial time computable function $f$ such that, for every $w$, $w\in A\Leftrightarrow f(w)\in B$. We call $f$ the reduction of $A$ to $B$.
\end{definition}
\begin{equation}
	A\le_P B \text{ and }B\in \textbf{P} \Longrightarrow A \in \textbf{P}
\end{equation}
\begin{definition}
	A language $B$ is NP-complete if $B\in \textbf{NP}$ and every $A\in \textbf{NP}$ is reducible to $B$ in polynomial time. \\
	If $B$ is NP-complete and $B\in \textbf{P}$, then $\textbf{P}=\textbf{NP}$. 
\end{definition}
\begin{equation}
	\forall B \text{ NP-complete and }C\in \textbf{NP}, \: B \le_P C \Longrightarrow C\text{ is NP-complete}
\end{equation}
Je sais pas trop quoi faire des slides 21 à 26 du dernier cours. 
\end{document}