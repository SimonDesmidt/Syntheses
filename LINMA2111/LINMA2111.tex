\documentclass[12pt, openany]{report}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage[a4paper,left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage[english]{babel}
\usepackage{libertine}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{float}
\usepackage{enumitem}
\usepackage{pythonhighlight}
\usepackage[]{titletoc}
\usepackage{empheq}
\usepackage{titlesec}
\usepackage{qtree}
\usepackage{mathpazo}
\usepackage{xfrac}
\usepackage{textcomp}
\usepackage{mathtools}
\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning}
\usepackage{caption}
\usepackage{tabularray}
\usepackage{subcaption}
\usepackage[bottom]{footmisc}
\usepackage{pdfpages}
\usepackage{tabularx}
\usepackage{amsthm}
\usepackage[skins]{tcolorbox}
\titleformat{\chapter}[display]
  {\normalfont\bfseries}{}{0pt}{\Huge}
\usepackage{hyperref}
\newcommand{\hsp}{\hspace{20pt}}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\Q}{\mathbb{Q}}
\theoremstyle{definition}
\newtheorem{thm}{Theorem}[chapter]
\newtheorem{definition}[thm]{Definition}
\newtheorem{lem}[thm]{Lemma}
\def\mat#1{\underline{\underline{#1}}}

\hbadness=100000
\begin{document}
\begin{titlepage}
    \begin{sffamily}
    \begin{center}
        \includegraphics[scale=0.25]{img/page_de_garde.png} \\[1cm]
        \HRule \\[0.4cm]
        { \huge \bfseries LINMA2111 - Discrete mathematics II \\[0.4cm] }
    
        \HRule \\[1.5cm]
        \textsc{\LARGE Simon Desmidt\\ Issambre L'Hermite Dumont}\\[1cm]
        \vfill
        \vspace{2cm}
        {\large Academic year 2025-2026 - Q1}
        \vspace{0.4cm}
         
        \includegraphics[width=0.15\textwidth]{img/epl.png}
        
        UCLouvain\\
    
    \end{center}
    \end{sffamily}
\end{titlepage}

\setcounter{tocdepth}{1}
\tableofcontents
\chapter{Introduction}
\section{Sorting problems}
A sorting problem is a problem that consists of taking a sequence of $n$ objects and putting them in order. This kind of problem is made of three main elements:
\begin{itemize}
  \item Context: set $S$ with a partial order $<$;
  \item Input: $n$ elements of $S$;
  \item Output: permutation of the input elements respecting the order.
\end{itemize}
To prove the correctness of an algorithm, we generally use the Hoare triple, i.e. a tuple for any input array $x_0$:
\begin{equation}
  \begin{aligned}
	&\{\text{Algorithm to be used};\ \text{Precondition}; \ \text{Postcondition}\}\\
	&\left\{IS;\ x=x_0;\ x\text{ is sorted and is a permutation of }x_0\right\}
\end{aligned}
\end{equation}
where IS is the insertion sort algorithm, and $x$ is the sorted array. \\
In practice, to prove the correctness of an algorithm, we define the invariant and the base case, and do an induction step show that the invariant is preserved. The proof is done in my notes at page 1.\\
\section{Definition of complexity}
For some functions $f,g:\mathbb{N} \to \R^+$, 
\begin{equation}
  	\begin{aligned}
		&f\in \mathcal{O}(g) \Longleftrightarrow \exists c>0, \exists n_0, \forall n>n_0,\ f(n)\le cg(n)\\
		&f\in \Omega(g)\Longleftrightarrow g\in \mathcal{O}(f)\\
		&f\in \Theta(g)\Longleftrightarrow f\in \mathcal{O}(g)\text{ and } f\in \Omega(g)\\
		&f\in o(g) \Longleftrightarrow \forall c>0, \exists n_0, \forall n>n_0,\ f(n)< cg(n)\\
		&f\in \omega(g)\Longleftrightarrow g\in o(f)\\
  	\end{aligned}
\end{equation}
\begin{itemize}
	\item The time complexity is the number of operations as a function of the input size;
	\item The space complexity is the amount of memory used (in addition to the input) as a function of the input size. 
\end{itemize}
We define the average case complexity as an expectation of the time taken by the algorithm on each input possible, with a dependence in the size of the input.
\begin{equation}
	t = \mathbb{E}_{x\sim D}[T(x)]
\end{equation}
where $D$ is the distribution of the input.\\
\section{Complexity of sorting algorithms}
No sorting algorithm can be faster than $\Omega(n \log(n))$. This can be proven through the complexity of comparison-based algorithms. 
\section{Sorting algorithms}
\subsection{Selection Sort}
This algorithm is the naive approach in complexity $\mathcal{O}(n^2)$. It goes as follows:
\begin{algorithm}[H]
	\caption{Selection sort algorithm}\label{algo:selection-sort}
	\begin{algorithmic}[1]
	\For{$i$   from   1   to   $n-1$}
		\State Select the smallest element in the subarray from index i to n;
		\State Swap it with element at index i;
	\EndFor
	\end{algorithmic}
\end{algorithm}
The invariant of this algorithm is that the elements 1 to $i-1$ are sorted.
\subsection{Insertion Sort}
The idea of this algorithm is to shift the elements to the left until they are well placed. This also has a complexity $\mathcal{O}(n^2)$, although the best case is in $\mathcal{O}(n)$. 
\begin{algorithm}[H]
	\caption{Insertion sort algorithm}\label{algo:insertion-sort}
	\begin{algorithmic}[1]
		\For{$i$   from 2 to $n$}
			\State shift element $i$ to the left by successive swaps until it is well placed;
		\EndFor 
	\end{algorithmic}
\end{algorithm}
The invariant of this algorithm is that the elements 1 to $i-1$ are sorted.
\subsection{Quick Sort}
The quick sort algorithm is based on divide and conquer methods (see chapter \ref{sec:d&c}): it splits and solves smaller sorting problems, and recombines the outputs at the end into a sorted instance.
\begin{algorithm}[H]
	\caption{Quick Sort algorithm}
	\begin{algorithmic}[1]
		\State pivot = T[1];
		\State T\_low = [T[i] \: : \: T[i] < pivot and i >1];
		\State T\_high = [T[i] \: : \: T[i] > pivot and i >1];
		\State quicksort(T\_low);
		\State quicksort(T\_high);
		\State T = [T\_low; pivot; T\_high];
	\end{algorithmic}
\end{algorithm}
The worst-case complexity is still $\mathcal{O}(n^2)$. However, here, the average-case complexity is lower: $\mathcal{O}(n\log (n))$. \\
The randomized version of the algorithm consists in shuffling before applying the classical quick sort, in order to change the pivot value. As this is a random algorithm, the worst-case complexity corresponds to the expected complexity, i.e. $\mathcal{O}(n\log(n))$. 
\subsection{Merge Sort}
The merge sort algorithm is also based on divide-and-conquer, and its complexity is also $\mathcal{O}(n^2)$. 
\begin{algorithm}[H]
	\caption{Merge Sort algorithm}\label{algo:merge-sort}
	\begin{algorithmic}[1]
		\State T\_left = [T[i] such that i <= n/2]
		\State T\_right = [T[i] such that i > n/2]
		\State mergesort(T\_left)
		\State mergesort(T\_right)
		\State T = merge(T\_left, T\_right)
	\end{algorithmic}
\end{algorithm}
The recurrence equation for the complexity is 
\begin{equation}
	t_n = 2t_{\lceil n/2\rceil} + \Theta(n) \Longrightarrow t(n) = \Theta(n\log(n))
\end{equation}
The disadvatange of quick sort is that this algorithm does not have randomness, as it is complex to simulate. However, the constant in the complexity order is higher than for the quick sort, and the memory usage can be higher too. 
\section{Decision tree}
A decision tree focuses on strategies and outcomes. It captures the main operations and the leaves are the possible outcomes ($\mathcal{O}(n!)$ in sorting problems, for $n$ the size of the data). This helps us compute the complexity of a sorting algorithm: the number of steps is the depth $S$ of the tree. Then, we have 
\begin{equation}
	2^S \ge n! \Longrightarrow S = \mathcal{O}(log(n!))
\end{equation}
This is equivalent to $\mathcal{O}(n\log(n))$ : 
\begin{equation}
	S = \log(n!) = \sum_{i=1}^n \log(i) \le \int_1^n log(x)dx 
\end{equation}
and we know that $S \ge \int_1^{n+1} \log(x)dx$\footnote{Why?}. Then, 
\begin{equation}
	\begin{aligned}
		\left[x\log(x)-x\right]_1^n \le S&\le \left[x\log(x)-x\right]_1^{n+1}\\
		S &= \mathcal{O}(n\log (n))
	\end{aligned}
\end{equation}
\subsection{Shannon coding theorem}
Let $\mathcal{S}=\{1,\dots,N\}$ and $P$ a random variable over $\mathcal{S}$ such that $Pr[P=i] = p_i$. Let $f:\mathcal{S}\to \{0,1\}^*$ maps a number to its binary representation, under the constraint that no number is a prefix of another one. Then,
\begin{equation}
	\sum_{i=1}^N p_i |f(i)| \ge H(P) = -\sum_{i=1}^N p_i \log_2 (p_i)
\end{equation}
where $H(P)$ is called the entropy of the probability distribution $P$. This theorem states that the average length of the prefix of the encoding of $N$ elements must be at least $\log(N)$ characters long.  
\subsection{Yao's minimax principle}
Consider a probability distribution over instances of a given size of a given problem. There, there exists a deterministic algorithm solving the problem for thoses instances, whose average-case complexity for the given distribution is lower than the worst-case expected complexity of any random algorithm solving the same problem. \\
This means that a random algorithm cannot be better than a deterministic algorithm in every case of a problem. 
\subsection{What about the part on thermodynamics in CM2?}
\chapter{Divide-and-conquer algorithms}\label{sec:d&c}
The divide-and-conquer method consists in three steps:
\begin{enumerate}
	\item Divide: create smaller subproblems;
	\item Recurse: solve them;
	\item Combine: merge the solutions.
\end{enumerate}
In sorting problems, a divide-and-conquer algorithm is the merge sort algorithm. It consists in dividing the array in two, sorting each half recursively, and merging the two sorted halves. The merge operation is done in linear time.\\
\section{Complexity of an integer multiplication}\label{sec:int_mult}
Given two $n$-digit numbers, we want to compute their product:
\begin{equation}
  	\begin{cases}
		a = a_{n-1}a_{n-2}...a_1a_0\\
		b = b_{n-1}b_{n-2}...b_1b_0\\
  	\end{cases}
	\Longrightarrow c=a\cdot b = c_{2n-1}...c_0
\end{equation}
Let us define $B$ as the basis (e.g. 10) and decompose $a$ and $b$ in two parts:
\begin{equation}
	\begin{cases}
		a = \alpha_0+B\alpha_1\\
		b = \beta_0+B\beta_1
	\end{cases} \Longrightarrow a\cdot b = \alpha_0\beta_0 + B(\alpha_1\beta_0 + \alpha_0\beta_1)+B^2 \alpha_1\beta_1
\end{equation}
In that case, we find a recurrence relation for the computation time:
\begin{equation}
	T(n) = 4T(n/2) + \Theta(n)
\end{equation}
The factor $4$ comes from the fact that we need $4$ products, and the $\Theta(n)$ is the complexity of the sum of the products. \\

We introduce the Master theorem to solve this equation, see section \ref{sec:master}. It gives a complexity of $T(n) = \Theta(n^{\log_2(4)}) = \Theta(n^2)$.\\
To reduce the complexity, we can change the value of the coefficient before $T(n/2)$ from 4 to 3 by calculating only 3 products:
\begin{equation}
	\begin{cases}
		\gamma_0 = \alpha_0\beta_0\\
		\gamma_2 = \alpha_1\beta_1\\
		\gamma_1 = (\alpha_0+\alpha_1)(\beta_0+\beta_1)-\gamma_0-\gamma_2\\
	\end{cases}
\end{equation}
This reduces the complexity to $\Theta(n^{1.58})$. \\
Following a similar reasoning, we can instead divide $a$ and $b$ into 3 sums instead of 2, and get 5 multiplications. This gives a complexity of $\Theta(n^{\log_3(5)}) = \Theta(n^{1.46})$. Dividing in 4, 5, etc, we converge to a complexity of $\Theta(n)$ and this is the optimal complexity for the multiplication of two numbers of $n$ digits. The problem is that the constant in front of the $n$ starts to grow as we divide into more and more limbs, and so a bigger exponent can be enough for most arrays\footnote{We call galactical algorithm an algorithm that is asymptotically good but the constant grows so large that it is only useful for huge arrays (e.g. $\sim10^{80}$).}.
\subsection{Master Theorem} \label{sec:master}
Let $a\ge 1$ and $b>1$ be constants and $f(n)$ a positive function, and let $T(n)$ be defined by $T(0)>0$ and $T(n) = aT(\lfloor n/b\rfloor)+f(n)$. Then,
\begin{itemize}
	\item If $f(n) = \mathcal{O}(n^{\log_b (a)-\epsilon})$ for some $\epsilon>0$, then $T(n) = \Theta(n^{\log_b(a)})$;
	\item If $f(n) = \Theta(n^{\log_b(a)})$, then, $T(n) = \Theta(n^{\log_b(a)}\log(n))$;
	\item If $f(n) = \Omega(n^{\log_b(a)+\epsilon})$ for some $\epsilon>0$ and if, for some $c>1$ and $n_0$ such that $af(\lfloor n/b\rfloor)\le cf(n)$ for all $n\ge n_0$, then $T(n)=\Theta(f(n))$;
\end{itemize}
\subsection{Proof of the master theorem}
\Tree [.{$n=b^k$} 
        [.{$\phantom{X}$} 
            [.{$\vdots$} {$\phantom{X}$} {$\phantom{X}$} ]
            [.{$\vdots$} {$\phantom{X}$} {$\phantom{X}$} ]
        ]
        [.{$\phantom{X}$} 
            [.{$\vdots$} {$\phantom{X}$} {$\phantom{X}$} ]
            [.{$\vdots$} {$\phantom{X}$} {$b^0$} ]
        ]
    ]

In that tree, for a level $i$, the size of the problem is $b^i$ and the number of subproblems is $a^{k-i}$. By summing up, 
\begin{equation}
	T(b^k) = \sum_{i=0}^k a^{k-i}f(b^i)
\end{equation}
For a function $f(n)=n^\alpha$, 
\begin{equation}
	T(n) = a^k \sum_{i=0}^k \left(\frac{b^\alpha}{a}\right)^i = a^k \frac{1-(\frac{b^\alpha}{a})^{k+1}}{1-b^\alpha/a}
\end{equation}
Under the assumption that $\alpha \neq \log_b(a)$. As $k=\log_b(n)$, we can replace above and simplify using the formula $a^{\log_b(n)} = n^{\log_b(a)}$. In the case where $a=b^\alpha$, then 
\begin{equation}
	T(n) = a^k (k+1) = a^{\log_b(n)} (\log_b(n)+1) = \Theta(n^{\log_b(a)}\log(n))
\end{equation}
\section{Multiplying polynomials}
Consider $a(x) = a_0+a_1x+a_2 x^2 + \dots + a_{n-1}x^{n-1}$. It can either be represented by its coefficients, or some values $(a(x_0),\dots, a(x_{n-1}))$ for $n$ distinct values. To compute those values, we can use a matrix-vector product:
\begin{equation}
	\begin{bmatrix}
		1 & x_0 & \dots & x_0^{n-1}\\
		\vdots & \vdots & \ddots & \vdots \\
		1 & x_{n-1} & \dots & x_{n-1}^{n-1}\\
	\end{bmatrix}\begin{bmatrix}
		a_0 \\ \vdots \\ a_{n-1}
	\end{bmatrix} = \mat{V}\textbf{a}
\end{equation}
where the matrix $\mat{V}$ is called the Vandermonde matrix. 
\subsection{Convolution theorem}
\begin{center}
	\begin{tikzpicture}[node distance=4cm, auto, font=\small]
		
		% Nodes
		\node[draw, red, thick, rectangle, inner sep=8pt] (A) {\textcolor{black}{
			$\begin{aligned}
				a(x) &= \sum_i a_i x^i \\ 
				b(x) &= \sum_i b_i x^i
			\end{aligned}$}
			};
			
			\node[draw, red, thick, rectangle, inner sep=8pt, right=of A] (B) {
				\textcolor{black}{$c(x) = \sum_i c_i x^i$}
				};
				
				\node[draw, red, thick, rectangle, inner sep=8pt, below=of A] (C) {
					\textcolor{black}{$\begin{matrix}
						a(x_0) & \dots & a(x_{2n-1}) \\
						b(x_0) & \dots & b(x_{2n-1})
					\end{matrix}$}
					};
					
					\node[draw, red, thick, rectangle, inner sep=8pt, right=of C] (D) {
						\textcolor{black}{$c(x_0) \dots c(x_{2m-1})$}
						};
						
				% Arrows
				\draw[->, red, thick] (D) -- (B) node[midway, above] {\small \textcolor{black}{Interpolate $\mat{V}^{-1}$}};
				\draw[->, red, thick] (A) -- (C) node[midway, left] {\small \textcolor{black}{Evaluate $\mat{V}$}};
				\draw[->, red, thick] (C) -- (D) node[midway, below] {\small \textcolor{black}{pointwise mult}};
						
	\end{tikzpicture}
\end{center}
This method gives bad conditioning, but this is not a problem here as we consider integer matrices. 
\section{Discrete Fourier Transform}
From the previous section, if we take the points as the $n$ complex roots of 1, i.e. $x_0 = e^{2\pi i/n}$ and $x_j = e^{2\pi ij/n}$, then we get a Discrete Fourier Transform matrix. Defining $\omega_n = x_0$,
\begin{equation}
	\mat{V} = \begin{bmatrix}
			1 & 1 & 1 & 1 \\
			\vdots & \omega_n & \dots & \omega_n^{n-1}\\
			\vdots & \ddots & \omega_n^{(i-1)(j-1)} & \vdots \\
			1 & \omega_n ^{n-1} & \dots & \omega_n^{(n-1)^2}
		\end{bmatrix}
\end{equation}
This gives us 
\begin{equation}
	\left(\mat{V}^{-1}\right)_{ij} = \frac{1}{n} \begin{bmatrix}
		\omega_n^{-(i-1)(j-1)}
	\end{bmatrix}
\end{equation}
and so $\mat{V} = \frac{1}{n}\mat{V}^*$. We conclude on 
\begin{equation}
	DFT(\textbf{x}) = \mat{V}\textbf{x}
\end{equation}
\subsection{Fast Fourier Transform}
The Fast Fourier Transform (FFT) algorithm uses the divide-and-conquer approach from above to compute the Fourier transform of a vector $x$. The exact algorithm is the following.\\
Assume that $n=2^k$, $k\ge 1$, $\textbf{x}=(x_0,\dots, x_{n-1})$ and $(y_0,\dots,y_{n-1}) = \mat{V}_n \textbf{x}$. The explicit formula to calculate $y_i$, $i=0,\dots,n/2-1$ is 
\begin{equation}
	\begin{aligned}
		y_i &= \sum_{j=0}^{n-1} x_j \omega_n^{ji} = \sum_{j=0}^{n/2-1}x_{2j}\omega_{n}^{2ji} + \sum_{j=0}^{n/2-1} x_{2j+1}\omega_n^{(2j+1)i} \\
		&= \sum_{j=0}^{n/2-1}x_{2j}\omega_{n/2}^{ji} + \omega^i_n\sum_{j=0}^{n/2-1} x_{2j+1}\omega_{n/2}^{ji}
	\end{aligned}
\end{equation}
And in matrix form:
\begin{equation}
	\begin{pmatrix}
		y_0 \\ y_1 \\ \vdots \\ y_{n/2-1}
		\end{pmatrix} = \mat{V}_{n/2} \begin{pmatrix}
			x_0\\x_2 \\ \vdots \\ x_{n-2}
		\end{pmatrix} + \begin{pmatrix}
			\omega_n^0 && \\ & \ddots & \\ & & \omega_n^{n/2-1}
	\end{pmatrix} \mat{V}_{n/2} \begin{pmatrix}
		x_1\\x_3\\\vdots \\x_{n-1}
	\end{pmatrix}
\end{equation}
And for the next half of $y$, we have a similar expression:
\begin{equation}
	\begin{aligned}
		y_{i+n/2} &= \sum_{j=0}^{n/2-1} x_{2j}\omega_{n}^{2ji}\left(\omega_{n/2}^{n/2}\right)^j + \omega_n^{i+n/2}\sum_{j=0}^{n/2-1} x_{2j+1}\omega_n^{ji}\left(\omega_{n/2}^{n/2}\right)^j \\
		&= \sum_{j=0}^{n/2-1}x_{2j}\omega_{n/2}^{ji} + (-1)\cdot \omega^i_n\sum_{j=0}^{n/2-1} x_{2j+1}\omega_{n/2}^{ji}
	\end{aligned}
\end{equation}
And, once again, in matrix form:
\begin{equation}
	\begin{pmatrix}
		y_{n/2} \\ y_{n/2+1} \\ \vdots \\ y_{n-1}
		\end{pmatrix} = \mat{V}_{n/2} \begin{pmatrix}
			x_0\\x_2 \\ \vdots \\ x_{n-2}
		\end{pmatrix} - \begin{pmatrix}
			\omega_n^0 && \\ & \ddots & \\ & & \omega_n^{n/2-1}
	\end{pmatrix} \mat{V}_{n/2} \begin{pmatrix}
		x_1\\x_3\\\vdots \\x_{n-1}
	\end{pmatrix}
\end{equation}
Let us define three notations: 
\begin{equation}
	\begin{aligned}
		\textbf{x}^{[0]} &= (x_0,\dots, x_{n/2-1})\\
		\textbf{x}^{[1]} &= (x_{n/2},\dots, x_{n-1})\\
		\mat{T}_n = \begin{pmatrix}
			\omega_n^0 & & \\ & \ddots & \\ & & \omega_n^{n/2-1}
		\end{pmatrix}
	\end{aligned}
\end{equation}
Then,
\begin{equation}
	\mat{DFT}(\textbf{x}) = \begin{pmatrix}
		\mat{DFT}(\textbf{x}^{[0]} + \mat{T}_n \mat{DFT}(\textbf{x}^{[1]}))\\
		\mat{DFT}(\textbf{x}^{[0]} - \mat{T}_n \mat{DFT}(\textbf{x}^{[1]}))
	\end{pmatrix}
\end{equation}
The time complexity of this algorithm is $\Theta(n\log(n))$. 
\begin{itemize}
	\item [$\rightarrow$] Note: there exists other algorithm to compute the FFT, such as the non power-of-two $n$ alorithm.
\end{itemize}
\section{Matrix multiplication}
Given $A,B\in \Z^{n\times n }$, we want to compute $C = A\cdot B$. The basic algorithm is in $\Theta(n^3)$, but we want a better one. 
\subsection{Straßen algorithm}
Let us divide the matrices in blocks:
\begin{equation}
	A = \begin{pmatrix}
		A_{11} & A_{12} \\ A_{21} & A_{22}
	\end{pmatrix} \qquad \qquad B = \begin{pmatrix}
		B_{11} & B_{12}\\ B_{21} & B_{22}
	\end{pmatrix} \qquad \qquad C = \begin{pmatrix}
		C_{11} & C_{12} \\ C_{21} & C_{22}
	\end{pmatrix}
\end{equation}
The Straßen algorithm uses the same idea as in section \autoref{sec:int_mult}: we define 7 block matrices to reduce the number of products done.
\begin{equation}
	\begin{cases}
		M_1 = (A_{11} + A_{22})(B_{11} + B_{22})\\
		M_2 = (A_{21} + A_{22})B_{11}\\
		M_3 = A_{11}(B_{12} - B_{22})\\
		M_4 = A_{22}(B_{21}-B_{12})\\
		M_5 = (A_{11} + A_{12})B_{22}\\
		M_6 = (A_{21} - A_{11})(B_{11}+B_{12})\\
		M_7 = (A_{12} - A_{22})(B_{21} + B_{22})
	\end{cases}
	\Longrightarrow C = \begin{pmatrix}
		M_1 + M_4 - M_5 + M_7 & M_3 + M_5\\ M_2+M_4 & M_1-M_2+M_3+M_6
	\end{pmatrix}
\end{equation}
Which has a complexity of $\Theta(n^{\log_2 7}) \approx \Theta(n^{2.81})$. The lower bound for the complexity is $\Omega(n^2)$, as we need to make at least one operation per element of $C$, and the current best algorithm (galactic) is in $\Theta(n^{2.371339})$. 
\subsection{Matrix inversion}
As we can assume intuitively, matrix multiplication and inversion are closely linked. Therefore, the complexity to compute both should be linked too. Let us call $M(n)$ the time complexity of multiplication of matrices of size $n$, and $I(n)$ the time complexity of inversion of a matrix of size $n$. Then,
\begin{equation}
	D = \begin{pmatrix}
		I & A & 0 \\ 0 & I & B \\ 0 & 0 & I
	\end{pmatrix}\Longrightarrow D^{-1} = \begin{pmatrix}
		I & -A & AB \\ 0 & I & -B \\ 0 & 0 & I
	\end{pmatrix}
\end{equation}
This means that the complexity of inversion is an upper bound on the complexity of multiplication: $M(n)\le I(3n)\le 3^3I(n) \ \forall n\ge n_0$.\\
This kind of inequality is called reduction. Given problems $A$ and $B$, if $A$ can be transformed in a problem $B$ of size $f(n)$ in time $T_R(n)$, then 
\begin{equation}\label{eq:reduction}
	T_A(n) \le T_R(n) + T_B(f(n))
\end{equation}
For example, the problem of finding the median of an array can be transformed into the problem of sorting the array.\\
\subsection{Matrix inversion upper bound}
Let us assume that $A=A^T$ and $A\succ 0$.
\begin{equation}
	A = \begin{pmatrix}
		B & C \\ C^T & D
	\end{pmatrix} \Longrightarrow A^{-1} = \begin{pmatrix}
		B^{-1} + B^{-1}CS^{-1}C^TB^{-1} & -B^{-1}CS^{-1}\\
		- S^{-1}C^TB^{-1} & S^{-1}
	\end{pmatrix}
\end{equation}
where $S = D-C^TB^{-1}C$ is the Schur complement of $A$. From \autoref{eq:reduction}, $f(n) = \Theta(M(n))$ in our case, and so we have the following relation between inversion and multiplication:
\begin{equation}
	I(n) = \mathcal{O}(M(n))
\end{equation}
\begin{itemize}
	\item [$\rightarrow$] Note: the hypotheses that $A=A^T$ and $A\succ 0$ are not binding. If $A$ is invertible but does not verify those conditions, we can work with $AA^T$ that does, and we find the inverse of $A$ with $A^{-1} =(A^TA)^{-1} A^T$. 
\end{itemize}
\chapter{Dynamic Programming}
\section{Two approaches}
There are two approaches for a dynamic programming algorithm: bottom-up and top-down. In the bottom-up approach, we solve the subproblems first, and then use their solutions to solve bigger problems. In the top-down approach, we start from the main problem, and recursively solve the subproblems as needed. \\
\begin{itemize}
	\item The main idea of bottom-up is to use memoization, i.e. storing the solutions of subproblems to avoid recomputing them. 
	\item The main idea of top-down is recursion.
\end{itemize}
Dynamic programming is used to improve time complexity by trading time for space.
\section{Examples}
\subsection{Rod cutting }
The problem of rod cutting consists in wanting to cut a beam of length $n$, given commands of clients.Finding the optimal solution can be done by computing a solution on sub-beams and merging them. 
\begin{algorithm}[H]
\caption{Rod Cutting Algorithm}
\begin{algorithmic}[1]
\Function{CutRod}{$p, n$}
    \If{$n = 0$}
        \State \Return $0$
    \EndIf
    \State $q \gets -\infty$
    \For{$i = 1$ to $n$}
        \State $q \gets \max\{q,\, p_i + \Call{CutRod}{p, n-i}\}$
    \EndFor
    \State \Return $q$
\EndFunction
\end{algorithmic}
\end{algorithm}
The complexity of this algorithm is given by a recurrence relation:
\begin{equation}
	T(n) = 1 + \sum_{j=0}^{n-1}T(j) = 1 + T(n-1) + \sum_{j=0}^{n-2}T(j) = 2T(n-1) \Longrightarrow T(n) = \mathcal{O}(e^n)
\end{equation}
This basic algorithm has a very bad complexity, but memoization can improve it.
\begin{algorithm}[H]
	\caption{Memoized rod cutting algorithm}
	\begin{algorithmic}[1]
		\Function{CutRod\_Memoized}{$p,n$}
		\State $r[0:n] = -\infty $
		\Function{CRM\_Aux}{$p,n,r$}
		\If{$r[n]\ge 0$}
		\State \Return $r[n]$
		\EndIf 
		\If {$n=0$}
		\State \Return $0$
		\EndIf 
		\State $q=-\infty$
		\For{i=1 to n}
		\State $q=\max\{q, p_i + CRM\_Aux(p,n-i,r)\}$
		\EndFor
		\State $r[n]=q$\\
		\qquad \: \: \Return $q$
		\EndFunction\\
		\qquad \Return CRM\_Aux(p,n,r)
		\EndFunction 
	\end{algorithmic}
\end{algorithm}
Finally, another algorithm exists, using a bottom-up approach in dynamic programming. It has a complexity $\Theta(n^2)$.
\begin{algorithm}[H]
	\caption{Bottom-up efficient rod cutting algorithm}
	\begin{algorithmic}[1]
		\Function{Bottom-up-CR}{$p,n$}
		\State $r[0:n]$
		\State $r[0] = 0$
		\For {j=1 to n}
		\State $q=-\infty$
		\For{i=1 to j}
		\State $q=\max\{q,p_i+r_{j-1}\}$
		\EndFor 
		\State $r[j] = q$
		\EndFor

		\Return $r[n]$
		\EndFunction 
	\end{algorithmic}
\end{algorithm}
\subsection{Matrix chain multiplication}
This problems consists in doing the multiplication of matrices $A_1A_2\dots A_n$ with dimensions $p_0, p_1,\dots, p_n$. The total number of operations depends heavily on the order of the multiplication. The idea for the algorithm is to split the chain into smaller chains until we get to a product of two matrices. The splitting happens at the index $i$ that minimizes the total number of operations, knowing the cost of computing a subchain:
\begin{equation}
	Algo(p) = \min_{1\le i \le n-1} \{ Algo(p[0:i]) + Algo(p[i+1:n]) + p_0p_ip_n\}
\end{equation}
Let us define $m[i,j]$ the optimal cost of the product $A_i\dots A_j$. Our goal is to compute $m[1,n]$, with the initial condition $m[i,i] = 0$ for all $i$. The recurrence relation is 
\begin{equation}
	m[i,j] = \min_{i\le k < j} \{m[i,k] + m[k+1,j] + p_{i-1}p_kp_j\} \qquad \forall 1\le i\le j \le n	
\end{equation}
This gives a complexity $\Theta(n^2)$, as we need to compute the entries of a triangular matrix $n\times n$. 
\section{Generating functions}
Let $\{a_k\}_{k=0}^\infty$ be a sequence. We associate the function $\sum_{k=0}^\infty a_kx^k$ to the sequence, on which we have addition, multiplication, differentiation, etc. \\ Example: 
\begin{equation}
	a_k = 1 \ \forall k\in \N \Longrightarrow f(x) = \sum_{k=0}^\infty x^k = \frac{1}{1-x} = x \sum_{k=0}^\infty x^k + 1 = xf(x) + 1
\end{equation}
Another example is the Fibonacci sequence:
\begin{equation}
	f_{n+2} = f_{n+1} + f_n \qquad f_0 = 0\qquad f_1 = 1
\end{equation}
From the recurrence equation, we can get the generative function:
\begin{equation}
	\begin{aligned}
		f(x) &= f_0 x^0 + f_1 x^1 + \sum_{k\ge 2} f_k x^k = f_0 + f_1 x + \sum_{k\ge 0} f_{k+2}x^{k+2}\\
		&= f_0 + f_1 x + x^2\sum_{k\ge0 } f_{k+1} x^k + x^2\sum_{k\ge 0}f_k x^k\\
		&= f_0 + f_1 x + x(f(x)-f_0) + x^2 f(x)\\
		&= \frac{f_0 + (f_1-f_0)x}{1-x-x^2}
	\end{aligned}
\end{equation}
Knowing that $\sum_{k\ge0 } f_{k+1} x^k = \frac{f(x)-f_0}{x}$ 
\chapter{Randomized Algorithms}
\section{Probabilistic analysis and randomized algorithm}
In probabilistic computing, we need a distribution of the inputs to be able to sample. Based on prior knowledge or assumptions, we can determine the average-case complexity. The goal is for it to be lower than the deterministic algorithm, although it comes at the price of the accuracy of the solution.\\

Let us use as example the hiring problem. We interview $n$ candidates, one at a time, and we need to decide wether we hire the candidate or not right after the interview. Our goal is of course to hire the best candidate. \\
One approach would be to interview the $n/2$ first candidates and only observe their skills. Then, hire the first candidate that is more skilled than the average (or all) of the first half of candidates.
\subsection{Indicator Random Variable}
Given a sample space $S$ and an event $A$, we define the indicator random variable as 
\begin{equation}
	X_A = \begin{cases}
		1 \text{ if $A$ occurs }\\ 0 \text{ otherwise}
	\end{cases}
\end{equation}
where $Pr[X_A = 1] = Pr[A]$. This is useful because $\E[X_A] = Pr[A]$.
\section{Random Sampling and Applications}
\subsection{Birthday paradox}
The birthday paradox is not stricly speaking a paradox, but is called that way because it goes against the first intuition. What is the number of people that must be in a room so that the probability that two people have the same birthday is higher than 1/2, assuming that the birthdays are uniformly distributed? Let $r={1,\dots,n}$ and $Pr[b_i=r] = 1/n$. Then, for two people,
\begin{equation}
	Pr[b_i = b_j] = \sum_{r=1}^n Pr[b_i=r \text{ and }b_j = r] = \sum_{r=1}^n Pr[b_i=r]Pr[b_j=r] = \sum_{r=1}^n \frac{1}{n^2} = \frac{1}{n}
\end{equation}
And for $k$ people, denoting $B_k$ the event that $k$ people have distinct birthdays, we have 
\begin{equation}
	\begin{aligned}
		Pr[B_k] &= 1\cdot \frac{n-1}{n}\frac{n-2}{n}\dots \frac{n-k+1}{n} = 1\cdot \left(1-\frac{1}{n}\right)\cdot (1-\frac{2}{n})\dots \left(1-\frac{k-1}{n}\right)\\
		&\le e^{-1/n}e^{-2/n}\dots e^{-(k-1)/n} = e^{-k(k-1)/2n}
	\end{aligned}
\end{equation}
Then, for $Pr[B_k]\ge 1/2$, we get 
\begin{equation}
	k(k-1) \ge 2n \ln{2}\Longleftrightarrow k\ge 23
\end{equation}
We can also compute the expectation: let $X_{ij}$ be the 1 if the people $i$ and $j$ have the same birthday and 0 otherwise. Then $\E[X_{ij}] = 1/n$ and for $X = \sum_{i=1}^k\sum_{j=i+1}^k X_{ij}$, 
\begin{equation}
	\E[X] = \sum_{i=1}^k \sum_{j=i+1}^k X_{ij} = \frac{k(k-1)}{2n}
\end{equation}
This means that for at least $\sqrt{2n}+1$ people in a room, we can EXPECT at least one collision. For $n=365$, this is $k=28$ people.
\subsection{Hash table}
A hash table is a data structure in which inserting, searching and deleting is done in $\mathcal{O}(1)$ in average. 
\textcolor{red}{J'ai pas compris les slides 18-19/19 du CM7.}
\subsection{Randomized algorithm for computing integrals}
To approximate $\pi$ , we can put $n$ random points on a square of size 1. For each point, we define the associated random variable $X_i=1$ if $x_i^2 + y_i^2\le 1$ and 0 otherwise. If we observe $k$ points in the orthant, we have 
\begin{equation}
	\frac{k}{n}\approx \frac{\pi}{4}
\end{equation}
And we can even compute the error using the variance of $X_n = \sum_{i=1}^nX_i$:
\begin{equation}
	\V(X_n) = \frac{1}{n}\V(X_i) = \frac{1}{n}(\E[X_i^2]-\E[X_i]^2) = \frac{1}{n}(\E[X_i]-\E[X_i]^2) = \frac{1}{n}\frac{\pi}{4}\left(1-\frac{\pi}{4}\right)
\end{equation}
This is can be done for any curve, not just the quarter of the circle, and we can thus approximate any integral with this method, with an absolutr error (standard deviation) of order $\mathcal{O}(1/\sqrt{n})$. This bound is valid for any dimension of the integration.
\subsection{Randomized algorithm for decision problems}
\subsubsection{Polynomials}
An example of a decision problem is to determine wether a polynomial is identically 0 or not. In one dimension, it is easy: use $d+1$ points for a polynomial of degree $d$, and if all are $0$, then $P\equiv 0$. In more than one dimension, the number of roots of the polynomial is infinite and we need something more. We can use the Schwartz-Zippel lemma:\\

Given a finite set $S$ of an integral domain, let $P$ be a polynomial of $n$ variables $(x_1,\dots,x_n)$ and of degree at most $d\ge0$. Let us pick a random point $(r_1,\dots,r_n)\in S$ uniformly:
\begin{equation}
	Pr[P(r_1,\dots,r_n)=0] \le \frac{d}{|S|}
\end{equation} 
We can prove it by induction:
\begin{equation}
	P(x_1,\dots,x_n) = \sum_{i=0}^d x_1^iP_i(x_2,\dots,x_n)
\end{equation}
for some polynomials $P_i$ of degree at most $d-i$ and $n-1$ variables. We can go all the way to polynomials of one single variable and use the fundamental theorem of algebra.\\

From this lemma, the probability to predict correctly from a set of points if a polynomial is identically zero or not is 
\begin{equation}
	Pr[correct] = Pr[TRUE\ \& \ P\equiv 0] + Pr[FALSE\ \& \ P\not\equiv 0]
\end{equation}
with $Pr[TRUE | P\equiv 0] = 1$ and $Pr[FALSE | P\not \equiv 0] \ge 1-\left(\frac{d}{|S|}\right)^k$. Then,
\begin{equation}
	Pr[correct] \ge 1-(d/|S|)^k
\end{equation}
\subsubsection{Matrix product}
Given $A,B,C\in \R^{n\times n}$, we want to check if $AB=C$. Doing the product is costly, so we can take random vectors $x\in \R^n$ and check the products:
\begin{equation}
	A(Bx) -Cx\overset{?}{=}0
\end{equation}
This is a $n$-variate polynomial of degree 1 and we can use the same method as in the previous section.
\textcolor{red}{Dunno what to do with slides 13->17 of CM8.}
\section{Las Vegas and Monte Carlo}
An algorithm is said to be Las Vegas when the output is correct, but the time taken to get to this solution is random. On the other hand, an algorithm is said to be Monte Carlo when the output is correct with some probability, or we only get a bound on the real value. 
\section{Hash tables}
\textcolor{red}{TODO}
\section{Randomness generation}
Let us take functions $f,g:X\to Y$ and an element $s\in X$. We define the sequence
\begin{equation}
	\begin{cases}
		x_ 0 = s\\
		x_{i+1} = f(x_i)\\
		y_i = g(x_i)
	\end{cases}
\end{equation}
The output of this pseudo-random generator (PRG) is the sequence (usual bits) $y= (y_0,\dots,y_n)$. One famous example of pseudo-random generator is Blum-Blum-Shub (BBS): \\
select random $\kappa$-bit prime integers $p,q$ such that $p=q=3 \mod 4$, and let $N=pq$. The PRG is 
\begin{equation}
	\begin{cases}
		x_0 = s \in \Z_N^*\\
		f_N(x) = x^2 \mod N \\
		g(x) = x \mod 2
	\end{cases}
\end{equation}
\section{Derandomization}
Derandomization transforms a randomized algorithm into a deterministic algorithm, or at least into an algorithm using less randomness, without increasing time and memory costs too much. 
\subsection{Techniques}
\begin{itemize}
	\item Use a PRG: cheap, the randomness goes in the time complexity, and some randomness still exists;
	\item Try all possible values: trades $n$ random bits for $2^n$ time, can remove all randomness, but can have a polynomial time slowdown.
\end{itemize}
\chapter{Computability}
\section{Turing machine}
A Turing machine is made of the following elements:
\begin{itemize}
	\item An infinite tape serving as unlimited memory;
	\item A head that reads and write the symbols and moves around the tape;
	\item A specific blanc symbol \textvisiblespace or $\epsilon$;
	\item The number of states must be finite.
\end{itemize}
Initially, the tape contains only the input string and is blank elsewhere. Storing information can be done by writing on the tape with the head, and reading information can be done by moving the head over the information. The possible outputs are only \textit{accept} and \textit{reject}, obtained when entering the corresponding state $q_{accept}$ or $q_{reject}$. 
\subsection{Formal definition}
\begin{definition}
	A Turing machine is a tuple $(Q, \Sigma, \Gamma, \delta, q_0,q_{accept},q_{reject})$ where $Q, \Sigma, \Gamma$ are finite sets and 
	\begin{itemize}
		\item $Q$ is the set of states;
		\item $\Sigma$ is the input alphabet (not containing the blank symbol);
		\item $\Gamma$ is the tape alphabet, where \textvisiblespace$\in \Gamma$ and $\Sigma \subseteq \Gamma$;
		\item $\delta:Q\times \Gamma \to Q\times \Gamma \times \{L,R\}$ is the transition function\footnote{We can add a "stay put" move $S$: $\{L,R,S\}$.};
		\item $q_0\in Q$ is the initial state;
		\item $q_{accept}\in Q$ is the accept state and $q_{reject}\in Q$ is the reject state and is different from the accept state. 
	\end{itemize}
	For $a,b\in \Gamma$, and $q,r\in Q$, $\delta(q,a)=(r,b,L)$ means that if the machine is in state $q$ and the head is over a tape square containing a symbol $a$, it replaces $a$ with $b$, the state becomes $r$, and the head moves one step to the left. 
\end{definition}
\begin{itemize}
	\item [$\to$] Note: the input of length $n$ is on the $n$ leftmost squares of the tape, and the machine never tries to move its head to the left when it is on the first square.
\end{itemize}
\begin{definition}
	We call the configuration of the Turing Machine the tuple (current state, current tape content, current head location). We write $uqv$ the configuration where the current state is $q$ and the tape content is the string $uv$, and the head location is the first symbol of $v$. \\
\end{definition}
A Turing machine $M$ accepts input $w$ if a sequence of configurations $C_1,C_2,\dots,C_k$ exists where 
\begin{itemize}
	\item $C_1$ is the start configuration of $M$ on input $w$, i.e. $C_1 = q_0w$;
	\item each $C_i$ yields $C_{i+1}$;
	\item $C_k$ is an accepting configuration.
\end{itemize}
The collection of string inputs that the machine accepts is called its language, and is denoted $\mathcal{L}(M)$. We say that the language is Turing-recognizable. \\
\begin{definition}
	A decider is a Turing machine that finds a finishing state on all inputs. A decider that recognizes some language is said to decide it, and the language is Turing-decidable is some Turing machine decides it.
\end{definition}
A decision problem is decidable iff it is computable, i.e. iff there exists an algorithm that can decide this problem. \\
We say that two machines are equivalent if they recognize the same languages. 
\subsection{Example}
\begin{figure}[H]
	\centering
	\includegraphics[width = .7\textwidth]{img/turing_example.png}
	\caption{Example of a Turing machine }
	\label{fig:turing}
\end{figure}
The language $A=\{0^{2^n}|n\ge 0\}$ is decidable. Let $M_2 = (Q, \Sigma, \Gamma, \delta, q_1, q_{accept}, q_{reject})$ such that 
\begin{itemize}
	\item $Q = \{q_1,\dots,q_5,q_{accept}, q_{reject}\}$ with $q_1$ the initial state;
	\item $\Sigma=\{0\}$ and $\Gamma = \{0,x,$\textvisiblespace$\}$;
	\item $\delta$ is the \autoref{fig:turing}.
\end{itemize}
Here is the breakdown of the steps:
\begin{itemize}
	\item $q_1$: reads the first symbol and if it is $0$, replaces it with \textvisiblespace and moves right to state $q_2$. If it is something else ($x$ or \textvisiblespace), then the input is invalid and reject.
	\item $q_2$: moves to the right whatever the bit is. If the bit is $0$, then replaces it with $x$ and moves to $q_3$. If it is \textvisiblespace, then there is no more unmarked $0$ and goes to accept.
	\item $q_3$: If the bit is $0$, then moves to the right and to state $q_4$ and does nothing. If the bit is \textvisiblespace, then goes back to the left and moves to state $q_5$. If the bit is $x$, goes to the right and no change of state. 
	\item $q_4$: If the bit is 0, then marks it $x$ and goes to the right, back to state $q_3$. If it is $x$, goes to the right with no change of state. If it is \textvisiblespace, goes to the right in a reject configuration.
	\item $q_5$: if the bit is $0$ or $x$, goes back to the left and no change of state. If the bit is \textvisiblespace, goes to the right and back to state $q_2$. 
\end{itemize}
\section{Multitape Turing Machines}
\begin{definition}
	A multitape Turing machine is a TM with several tapes, each with its own head. For $k$ tapes, the transition function then becomes 
\end{definition}
\begin{equation}
	\delta : Q\times \Gamma^k\to Q\times \Gamma^k\times \{L,R,S\}^k
\end{equation}
Each multitape Turing Machine has an equivalent single-tape Turing machine. 
\section{Enumerator}
\begin{definition}
	An enumerator is a Turing machine that always starts with a blank input on its tape, and that has an attached printer to print strings. Every time it wants to print a string to the list, it sends it to the printer. An enumerator does not have to halt, and may print an infinite list of strings. The language it enumerates is the collection of all strings that is printed out, and it can generate the strings in any order, possibly with repetitions. \\
\end{definition}
\begin{thm}
	A language is Turing recognizable iff some enumerator enumerates it. 
\end{thm}
\section{Non deterministic Turing Machine}
A Turing machine is said to be non deterministic when its transition function is a probability distribution:
\begin{equation}
	\delta :Q\times \Gamma \to \mathcal{P}(Q\times \Gamma \times \{L,R\})
\end{equation}
\begin{thm}
	As a non deterministic TM can be represented as a tree, every non deterministic TM has an equivalent deterministic TM that goes through every possible state of the tree. 
\end{thm}
\subsection{10th Hilbert's problem}
"Devise a process according to which it can be determined by a finite number of operation that a given multivariate polynomial has an integer root."\\

For an univariate polynomial, we must check if there exists $a\in \Z$ such that $P(a)=0$. We know also that the roots are in the interval $\left[-n\frac{c_{\max}}{c_1}, n\frac{c_{\max}}{c_1}\right]$. Therefore, there is a finite number of integer values to check, and we can evaluate the polynom $P(x)$ at each of those values. However, for a multivariate polynomial, it is impossible. \\
The rigorous formulation of the 10th Hilbert problem is to find a decider for $D = \cup_n D_n$ with 
\begin{equation}
	D_n = \{P\in \Z[x_1,\dots,x_n] \: |\: \exists a\in \Z^n \: : \: P(a) = 0\}
\end{equation}
\section{Universal Turing Machine}
\begin{definition}
	
	A universal Turing machine is a TM that solves the following problem:
	\begin{itemize}
		\item Input: a description of the TM (T) and a description of a finite word on an initial tape ($w$);
		\item Output: $T(w)$, i.e. the finite content of the tape when $T$ stops; and undefined if $T$ never stops.
	\end{itemize}
\end{definition}
\chapter{Decidability}
\section{Countable sets}
\begin{itemize}
	\item Two sets have the same number of elements iff they are in bijective relation;
	\item A set $S$ is countable if it either is finite or has the same size as $\N$, e.g. $\Z, \Q, \N^d,\dots$; 
\end{itemize}
\section{Undecidable language}
\begin{thm}
	$A_{TM} = \{\langle M,w\rangle|M\text{ is a TM and }M\text{ accepts } w\}$ (called halting problem) is undecidable.  
\end{thm}
This notation is the way to write a universal Turing machine with inputs $\langle M, w\rangle$, $M$ being a Turing machine and $w$ a string. Here is the proof of the theorem, by contradiction:\\
Let $H$ be a decider for $A_{TM}$. Then
\begin{equation}
	H(\langle H,w\rangle) = \begin{cases}
		\text{accept} & \text{ if } M \text{ accepts }w\\
		\text{reject} & \text{ if } M \text{ does not accept }w
	\end{cases}
\end{equation}
Build a Turing machine $D$ that, on input $\langle M\rangle$, runs $H$ on $\langle M, \langle M\rangle \rangle$ and halts with $\overline{H(\langle M,w\rangle)}$, i.e. $D$ returns the opposite of $H$:
\begin{equation}
	D(\langle M\rangle) = \begin{cases}
		\text{accept} & \text{ if } M \text{ does not accept } \langle M\rangle\\
		\text{reject} & \text{ if } M \text{ accepts } \langle M\rangle
	\end{cases}
\end{equation}
But then, $D(\langle D\rangle)$ is a contradiction with itself. \textcolor{red}{Define notation $\langle M\rangle$!}
\section{Unrecognizable language}
\begin{thm}
	A language is decidable iff it is Turing-recognizable and co-Turing-recognizable.
\end{thm}
Proof: \\
\begin{itemize}
	\item $\Rightarrow$ is trivial;
	\item $\Leftarrow$
\end{itemize}
Let $M_1$ be a recognizer for $A$, and $M_1$ a recognizer for $\overline A$. We build $M$ that runs $M_1$ and $M_2$ in parallel, one step at a time, on input $w$. If $M_1$ accepts, ACCEPT, and if $M_2$ accepts, REJECT. 

Therefore, $\overline{A_{TM}} = \{\langle M, w\rangle|M\text{ is a TM and }M \text{	does not accept }w\}$ is not Turing-recognizable. 
\chapter{Complexity Theory}
\section{Definitions}
\begin{definition}
	Let $M$ be a deterministic TM that halts on all inputs. The running time, or time complexity, of $M$ is the function $f:\N\to \N$ where $f(n)$ is the maximum number of steps that $M$ uses on any input of length $n$.
\end{definition}
\begin{definition}
	Let $t:\N\to \R^+$ be the time complexit class TIME$(t(n))$, i.e. the collection of languages that are decidable by an $\mathcal{O}(t(n))$ time TM. 
\end{definition}
\subsection{Example}
Let $\{0^k1^k|k\ge 0\}$ be a string composed of $k$ 0 followed by $k$ 1. The goal is to check if the number of 0 is the same as the number of 1. \\
One method would be to start at the first element. If it is 0, cross it and go right until we find a 1. Then, go left until we find the first 0, and go again. Once we have no more 0 or 1, we check that it is also the case for the other character. If not, return FALSE. The complexity is $\mathcal{O}(k^2)$.\\
Another method is to cross every other 0 and do the same with the 1. With what is left, do it again (so one every four characters), and so on. At the end, compare if there is one character left in one of the strings. The complexity of this is $O(k\log(k))$. 
\begin{itemize}
	\item [$\to$] Note: in the case where a counter variable is available, the overall complexity is not better, as the complexity to increment it is $\mathcal{O}(\log(k))$. 
\end{itemize}
\section{Complexity of multitape}
\begin{thm}	\label{thm:multitape}
	Let $t(n)\ge n$, as we can reasonably assume that we need to go through the whole input to find the solution. Every $t(n)$ time multitape TM has an equivalent $\mathcal{O}(t^2(n))$ time single-tape TM. 
\end{thm}
\section{Complexity of non deterministic TM}
Let $N$ be a nondeterministic TM that is a decider. The running time of $N$ is the function $f:\mathcal{N}\to \mathcal{N}$ where $f(n)$ is the maximum number of steps that $N$ uses on any branch of its computation on any input of length $n$.
\begin{thm}
	Let $t(n)\ge n$. Every $t(n)$ time nondeterministic single-tape TM has an equivalent $2^{\mathcal{O}(t(n))}$ time deterministic single-tape TM. 
\end{thm}
This is because the number of nodes is in $\mathcal{O}(b^{f(n)})$, and the number of operations for one node is $\mathcal{O}(n+f(n))$. $b$ is the number of possibilities at each node. This gives a total complexity of
\begin{equation}
	\mathcal{O}\left((n+f(n))b^{f(n)}\right) = \mathcal{O}\left(2^{\log_2(b)(\log_b(f(n)) + f(n))}\right) = 2^{\mathcal{O}(f(n))}
\end{equation}
The result is the same for a multitape or single-tape TM, as (from \autoref{thm:multitape}):
\begin{equation}
	2^{\mathcal{O}(f(n))} \rightarrow \left(2^{\mathcal{O}(f(n))}\right)^2 = 2^{\mathcal{O}(f(n))}
\end{equation}
\section{The class P}
\begin{definition}
	\textbf{P} is the class of languages that are decidable in polynomial time on a deterministic single-tape TM. 
	\begin{equation}
		\textbf{P} = \cup_k TIME(n^k)
	\end{equation}
\end{definition}
\begin{definition}
	\textbf{coP} is the class of languages such that their complement is in \textbf{P}.
	\begin{equation}
		\textbf{coP} = \{L|\overline{L}\in \textbf{P}\}
	\end{equation}
\end{definition}
If $L\subset \textbf{P}(\Gamma^*)$, then $\overline{L} = \textbf{P}(\Gamma^*) \setminus L$, where $L$ is a language, and $\textbf{P}(\Gamma^*)$ is the class $\textbf{P}$ of argument $\Gamma^*$, $\Gamma$ is the alphabet and the star means the set of words of this alphabet. \\
\begin{itemize}
	\item [$\to$] Note: any problem in \textbf{coP} is in \textbf{P}, as it is only the inverse problem.
\end{itemize}
\begin{equation*}
	\Longrightarrow \textbf{P} = \textbf{coP}
\end{equation*}
\section{The class NP}
\begin{definition}
	A verifier for a language $L$ is an algorithm $V$ where 
	\begin{equation}
		L = \{w|V\text{ accepts }\langle w,c\rangle\text{ for some string }c\}
	\end{equation}
\end{definition}
\begin{definition}
	The class \textbf{NP} is the class of languages that have a polynomial time verifier. 
\end{definition}
\begin{thm}
	A language is in \textbf{NP} iff it is decided by some nondeterministic polynomial time TM. 
\end{thm}
We write $NTIME(t(n))$ the set of languages that are decided by a $\mathcal{O}(t(n))$ time nondeterministic TM, and so 
\begin{equation}
	\textbf{NP} = \cup_k NTIME(n^k)
\end{equation}
\begin{thm}
	\begin{equation}
		\textbf{P}\subset \textbf{NP}
	\end{equation}
\end{thm}
This is obvious: any problem whose solution can be found in polynomial time has a verifier in polynomial time. 
\subsection{Example -- SAT}
The SAT problem consists in finding boolean variables $\{x_n\}_{n\ge 0}$ such that a combination of boolean operators applied to them returns 1. For example,
\begin{equation}
	\exists? \{x_1,x_2,x_3\}\text{ such that } \left((x_1 \wedge x_2) \vee  \overline{(x_2 \vee x_3)}\right) \wedge x_3 = 1
\end{equation}
It is very hard to find a solution, but easy to check. 
\subsubsection{Conjunctive normal form}
We define the conjunctive normal form as 
\begin{equation}
	\phi = \bigwedge_i c_i \qquad c_i = \bigvee_j l_j \qquad l_j = \begin{cases}
		x_k \\ \overline{x_k}
	\end{cases}
\end{equation}
We call $c_i$ the clauses and $l_j$ the literals. \\
The 3SAT problem add the constraint that there are at most 3 $l_j$ per clause. \\

The SAT problem is NP-complete (see section \ref{sec:NP-complete}).
\section{The EXPTIME class}
EXPTIME is the class of languages that are decidable in exponential time on a deterministic single-tape TM. 
\begin{equation}
	\textbf{EXPTIME} = \cup_k TIME(2^{n^k})
\end{equation}
\begin{thm}
	$\textbf{P} \subset \textbf{NP} \subset \textbf{EXPTIME}$
\end{thm}
\section{Polynomial time reduction}\label{sec:NP-complete}
\begin{definition}	
	A function $f:\Sigma^*\to \Sigma^*$ is a polynomial time computable function if some polynomial time TM exists that halts with $f(w)$ on its tape when started on any input $w$. 
\end{definition}
\begin{definition}
	A language $A$ is polynomial time reducible to a language $B$, written $A\le_P B$, if there exists a polynomial time computable function $f$ such that, for every $w$, $w\in A\Leftrightarrow f(w)\in B$. We call $f$ the reduction of $A$ to $B$.
\end{definition}
\begin{equation}
	A\le_P B \text{ and }B\in \textbf{P} \Longrightarrow A \in \textbf{P}
\end{equation}
\begin{definition}
	A language $B$ is NP-complete if $B\in \textbf{NP}$ and every $A\in \textbf{NP}$ is reducible to $B$ in polynomial time. \\
	If $B$ is NP-complete and $B\in \textbf{P}$, then $\textbf{P}=\textbf{NP}$. 
\end{definition}
\begin{equation}
	\forall B \text{ NP-complete and }C\in \textbf{NP}, \: B \le_P C \Longrightarrow C\text{ is NP-complete}
\end{equation}
Je sais pas trop quoi faire des slides 21 à 26 du dernier cours. 
\end{document}