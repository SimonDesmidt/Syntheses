\documentclass[12pt, openany]{report}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage[a4paper,left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage[french]{babel}
\usepackage{libertine}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{float}
\usepackage{enumitem}
\usepackage{pythonhighlight}
\usepackage[]{titletoc}
\usepackage{empheq}
\usepackage{titlesec}
\usepackage{mathpazo}
\usepackage{xfrac}
\usepackage{textcomp}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{tabularray}
\usepackage{subcaption}
\usepackage[bottom]{footmisc}
\usepackage{pdfpages}
\usepackage{tabularx}
\usepackage{amsthm}
\usepackage[skins]{tcolorbox}
\titleformat{\chapter}[display]
  {\normalfont\bfseries}{}{0pt}{\Huge}
\usepackage{hyperref}
\newcommand{\hsp}{\hspace{20pt}}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\newcommand{\R}{\mathbb{R}}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}

% Define a new tcolorbox style with a red border and transparent interior
\tcbset{
    redbox/.style={
        enhanced,
        colframe=red,
        colback=white,
        boxrule=1pt,
        sharp corners,
        before skip=10pt,
        after skip=10pt,
        box align=center,
        width=\linewidth-2pt, % Adjust the width dynamically
    }
}
\newcommand{\boxedeq}[1]{
\begin{tcolorbox}[redbox]
    \begin{align}
        #1
    \end{align}
\end{tcolorbox}
}
\theoremstyle{definition}
\newtheorem{thm}{Theorem}[chapter]
\newtheorem{definition}[thm]{Definition}
\newtheorem{exmp}[thm]{Example} 
\newtheorem{lem}[thm]{Lemma}
\newtheorem{prop}[thm]{Property}
\newtheorem{crl}[thm]{Corollary}
\begin{document}


\begin{titlepage}
    \begin{sffamily}
    \begin{center}
        \includegraphics[scale=0.25]{img/page_de_garde.png} \\[1cm]
        \HRule \\[0.4cm]
        { \huge \bfseries LINMA2471 Optimization models and methods II \\[0.4cm] }
    
        \HRule \\[1.5cm]
        \textsc{\LARGE Simon Desmidt}\\[1cm]
        \vfill
        \vspace{2cm}
        {\large Academic year 2024-2025 - Q1}
        \vspace{0.4cm}
         
        \includegraphics[width=0.15\textwidth]{img/epl.png}
        
        UCLouvain\\
    
    \end{center}
    \end{sffamily}
\end{titlepage}

\setcounter{tocdepth}{1}
\tableofcontents
\chapter{Gradient Method}
An optimization problem is defined as 
\begin{equation}\label{eq:1}
    \min_{x\in \mathbb{R}^n} f(x)
\end{equation}
where \(f:\mathbb{R}^n\rightarrow\mathbb{R}\) is a continuously differentiable function. 
\section{Definitions}
\begin{itemize}
    \item A function \(F:\mathbb{R}^n\rightarrow \mathbb{R}^n\) is L-Lipschitz continuous when \[\lVert F(y)-F(x)\rVert \le L\lVert y-x\rVert\qquad \forall x,y\in \mathbb{R}^n\]where we use the euclidian norm. 
    \item If \(\nabla f\) is L-Lipschitz then, given \(x\in \mathbb{R}^n\), \[f(y)\le f(x)+\langle \nabla f(x), y-x\rangle +\frac{L}{2}\lVert y-x\rVert^2 = m_x(y)\qquad \forall y\in \mathbb{R}^n\]and \(f\) is said to be a L-smooth function.
    \item A function \(f:\mathbb{R}^n \rightarrow \mathbb{R}\) is convex when, given \(x,y\in \mathbb{R}^n\) and \(\lambda \in [0,1]\), we have \[f(\lambda x+(1-\lambda)y)\le \lambda f(x)+(1-\lambda)f(y)\]
    \item Let \(f:\mathbb{R}^n\rightarrow\mathbb{R}\) be convex. If \(f\) is differentiable, then \[f(y) \ge f(x) + \nabla f(x)^T (y-x) \qquad \forall x,y\in \mathbb{R}^n\]
    \item If it is twice differentiable, and its hessian is positive semi-definite on its domain, then it is convex. 
    \item A differentiable function $f:\R^n\rightarrow \R$ is $\mu$-strongly convex ($\mu\ge 0$) if $\forall x,y$, $\forall \lambda \in ]0,1[$, \[f(\lambda x+(1-\lambda)y) \le \lambda f(x)+(1-\lambda)f(y)-\frac{\mu}{2}\lambda(1-\lambda)\lVert x-y\rVert^2_2\] 
    \item A differentiable function \(f:\mathbb{R}^n\rightarrow \mathbb{R}\) is \(\mu\)-strongly convex \((\mu>0)\) iff, given \(x\in \mathbb{R}^n\), \[f(y) \ge f(x) + \nabla f(x)^T(y-x) + \frac{\mu}{2}\lVert y-x\rVert^2 \qquad \forall y\in \mathbb{R}^n\]
    \item PL inequality for a \(\mu\)-strongly convex function\footnote{\(x^*\) is the minimizer of \(f\)}: \[f(x)-f(x^*)\le \frac{1}{2\mu} \lVert \nabla f(x)\rVert^2 \qquad \forall x\in \mathbb{R}^n\]
\end{itemize}
\section{Complexity}
The demonstration of the final results here obtained is in the notes, but not explained here. 
\subsection{Hypotheses}
\begin{itemize}
    \item \(f\) is convex and differentiable;
    \item \(\nabla f\) is L-Lipschitz;
    \item we start from a \(x_0\in \mathbb{R}^n\) that is not a minimizer of \(f\);
\end{itemize}
\subsection{Results}
We use the sequence \(\{x_k\}_{k\ge 0}\), given a \(x_0\in \mathbb{R}^n\), such that \[x_{k+1} = x_k - \frac{1}{L}\nabla f(x_k)\]
\begin{center}
    \renewcommand{\arraystretch}{1.5}
    \begin{tabular}{c|c|c}\label{tab:complexity}
    Problem class & Goal & Complexity bound \\ \hline\hline
    Non-convex \(f\) & \(\lVert \nabla f(x_k)\rVert \le \varepsilon\) & \(\mathcal{O}(\varepsilon^{-2})\)\\ \hline
    Convex \(f\) & \(f(x_k)-f(x^*)\le \varepsilon\) & \(\mathcal{O}(\varepsilon^{-1})\)\\ \hline
    \(\mu\)-strongly-convex \(f\) & \(f(x_k)-f(x^*)\le \varepsilon\) & \(\mathcal{O}(\log (\varepsilon^{-1}))\)
\end{tabular}
\end{center}
\section{GM with Armijo Line Search}\label{sec:armijo}
The Armijo Line Search consists of changing the constant in the GM in order to be more efficient and be able to make bigger steps in some directions where it is possible. 
\begin{equation}
    x_{k+1} = x_k - \alpha \nabla f(x_k)\qquad \alpha >0
\end{equation}
\begin{algorithm}
    \caption{Gradient Method with Armijo Line Search}\label{algo:Armijo}
    \begin{algorithmic}[1]
    \State \textbf{Step 0:} Given $x_0 \in \mathbb{R}^n$ and $\alpha_0 > 0$, set $k \coloneqq 0$.
    \State \textbf{Step 1:} Set $\ell \coloneqq 0$.
    \State \textbf{Step 1.1:} Compute $x_k^+ = x_k - (0.5)^\ell \alpha_k \nabla f(x_k)$.
    \State \textbf{Step 1.2 (Armijo Line Search):} If
    \[
    f(x_k) - f(x_k^+) \geq \frac{(0.5)^\ell \alpha_k}{2} \|\nabla f(x_k)\|^2 \tag{1}
    \]
    set $\ell_k \coloneqq \ell$ and go to Step 2. Otherwise, set $\ell \coloneqq \ell + 1$ and go back to Step 1.1.
    \State \textbf{Step 2:} Define $x_{k+1} = x_k^+$, $\alpha_{k+1} = (0.5)^{\ell_k - 1} \alpha_k$, set $k \coloneqq k + 1$ and go back to Step 1.
    \end{algorithmic}
\end{algorithm}
\section{Problems with convex constraints}
Consider the problem
\begin{equation}\label{eq:convex_problem}
    \min_{x\in \mathbb{R}^n} f(x) \text{ such that } x\in \Omega
\end{equation}
where \(f\) is L-smooth, and \(\Omega \subseteq \mathbb{R}^n\) is nonempty, closed and convex.
Given an approximation \(x_k\in \Omega\) for a solution of \ref{eq:convex_problem}, a possible generalization of the Gradient Method is to define 
\begin{equation}
    x_{k+1} = P_{\Omega} \left(x_k-\frac{1}{L}\nabla f(x_k)\right)
\end{equation}
where \(P_{\Omega}\) is the projection of \(z\) onto \(\Omega\), and we call this method the Projected Gradient Method.\\
If \(\Omega = [a,b]^n\), then the projection of an element \(z\) onto \(\Omega\) is such that its element \(i\) is given by:
\begin{equation}
    \left[P_\Omega(z)\right]_i = \begin{cases}
        z_i \text{ if } a\le z_i\le b\\
        a \text{ if } z_i<a\\
        b \text{ if }z_i>b\\
    \end{cases} \qquad \forall i=1,\dots,n
\end{equation}
If \(x^*\) is a solution of \eqref{eq:convex_problem}, then \[\langle \nabla f(x^*),z-x^*\rangle \ge 0\qquad \forall z \in \Omega\]
\begin{itemize}
    \item [\(\rightarrow\)] Note: if \(\Omega = \mathbb{R}^n\), then this lemma is true, in particular for \(z=x^*-\nabla f(x^*)\). Then it is straightforward that we must have \(\nabla f(x^*)=0\). 
\end{itemize}
\section{Reduced gradient method}
For a L-smooth function for the problem \eqref{eq:convex_problem}, we define 
\begin{equation}
    G_L(x_k) = L(x_k-x_{k+1})
\end{equation}
where \(x_{k+1}\) is given by the general formula\footnote{This definition of \(x_{k+1}\) is true for any type of gradient method, the first case seen being with \(\Omega = \mathbb{R}^n\).}
\begin{equation}
    x_{k+1} = \arg \min_{y\in \Omega} f(x_k) + \langle \nabla f(x_k),y-x_k\rangle +\frac{L}{2}\lvert y-x_k\rVert_2^2
\end{equation}
From this, we can show as we did in the previous sections that there is a lower bound for the method:
\begin{equation}
    f(x_k)-f(x_{k+1}) \ge \frac{1}{2L}\lVert G_L(x_k)\rVert_2^2
\end{equation}
This is the same result we found for the unconstrained gradient method, but with a different gradient definition. This is thus a generalization of the first cases. Furtheremore, by the same process we used before, we can show that the complexity of this Reduced Gradient Method is the same as in the table \ref{tab:complexity}.
\section{Proximal Gradient Method}
We will here consider problems of the form 
\begin{equation}
    \min_{x\in \mathbb{R}^n} F(x) \equiv f(x) + \phi(x)
\end{equation}
where \(f(\cdot)\) is L-smooth and \(\phi :\mathbb{R}^n\rightarrow \mathbb{R}\cup \{+\infty\}\) is convex, possibly nonsmooth. \\
In this case, the formula for \(x_{k+1}\) is 
\begin{equation}
    x_{k+1} = \arg\min_{y\in \mathbb{R}^n} f(x_k) + \langle \nabla f(x_k),y-x_k\rangle + \frac{L}{2}\lVert y-x_k\rVert_2^2 + \phi(y)
\end{equation}
which can be re-expressed as 
\begin{equation}
    x_{k+1} = \arg\min_{y\in \mathbb{R}^n} \frac{1}{2}\lVert y-(x_k-\frac{1}{L}\nabla f(x_k))\rVert^2+\frac{1}{L}\phi(y)
\end{equation}
Given a convex function \(h\), we define the proximal operator \(prox_h:\mathbb{R}^n\rightarrow\mathbb{R}^n\) by
\begin{equation}
    prox_h(z) = \arg \min_{y\in \mathbb{R}^n}\frac{1}{2}\lVert y-z\rVert^2+h(y)
\end{equation}
Then, we can write
\begin{equation}
    x_{k+1} = prox_{\frac{1}{L}\phi}\left(x_k-\frac{1}{L}\nabla f(x_k)\right)
\end{equation}
\begin{itemize}
    \item [\(\rightarrow\)] Note: if the \(\phi\) function is the indicator function, i.e. \(\phi=i_\Omega = \begin{cases}
        0 \text{ if } x\in \Omega\\ \infty \text{ otherwise}
    \end{cases}\), then \(prox_{\frac{1}{L}i_\Omega}(z) = P_\Omega(z)\).
\end{itemize}
\section{Accelerated Proximal Gradient Method}
This method's goal is to take into account the history of the method, so that the convergence is faster. This method still makes the hypothesis that the function \(f\) is convex.
\begin{algorithm}[H]
    \caption{Accelerated Proximal Gradient Method}\label{algo:Acc_prox}
    \begin{algorithmic}[1]
    \State \textbf{Step 0:} Given $x_0 \in \mathbb{R}^n$, set \(y_1=x_0\), \(t_1=1\) and $k=1$.
    \State \textbf{Step 1:} Compute 
    \begin{equation}
        x_k = prox_{\frac{1}{L}\phi}\left(y_k-\frac{1}{L}\nabla f(y_k)\right)
    \end{equation}
    \State \textbf{Step 2:} Define 
    \begin{align}
        t_{k+1} &= \frac{1+\sqrt{1+4t_k^2}}{2}\\
        y_{k+1} &= x_k + \left(\frac{t_k-1}{t_{k+1}}\right)(x_k-x_{k-1})      
    \end{align}
    \State \textbf{Step 3:} Set \(k=k+1\) and go back to Step 1.
    \end{algorithmic}
\end{algorithm}
This method takes at most \(\mathcal{O}(\varepsilon^{-1/2})\) iterations to generate \(x_k\) such that \(f(x_k)-f(x^*)\le \varepsilon\).
\section{Convexly constrained optimization problem}
Consider the problem 
\begin{equation}
    \min f(x) \text{   such that   }x\in \Omega
\end{equation}
where \(f:\R^n\rightarrow \R\) is a convex function possibly nonsmooth, and \(\Omega\) is convex, closed and nonempty. 
\begin{definition}
    A subgradient of the convex, non differentiable function \(f\) at \(x\) is a function \(g:\R^n\rightarrow \R^n :x\rightarrow g(x)\) such that 
    \begin{equation}
        f(y) \ge f(x)+\langle g(x),y-x\rangle \qquad \forall y\in \R^n
    \end{equation}
    The set of all subgradients of \(f\) at point \(x\) is called subdifferential of \(f\) at \(x\) and is denoted by \(\partial f(x)\). 
\end{definition}
A generalization of PGM to non smooth functions is 
\begin{equation}
    x_{k+1} = P_\Omega (x_k-\alpha g(x_k)) \qquad g(x_k)\in \partial f(x_k),\alpha_k>0,\forall k\ge 0
\end{equation}
\begin{itemize}
    \item If we take \(\alpha_k=\alpha \), \(\forall k\ge 0\), then we need at most \(\mathcal{O}(\varepsilon^{-2})\) iterations. 
    \item If we assume that \(\lVert g(x_k)\rVert \le M\) for all \(k\ge 0\), then we can take \(\alpha_k = \frac{\varepsilon}{\lVert g(x_k)\rVert^2}\), and the convergence is in \(\mathcal{O}(\varepsilon^{-2})\) too. However, this is a good example of a dynamic step (changes with \(g(x_k)\)). 
\end{itemize}
\section{Summary}
\begin{center}
    \renewcommand{\arraystretch}{1.5}
    \begin{tabular}{c|c|c}\label{tab:summary_GM}
    Method & Goal & Complexity\\ \hline\hline
    PGM & \(F(x_k)-F(x^*)\le \varepsilon\) & \(\mathcal{O}(\varepsilon^{-1})\)\\ \hline
    Accelerated PGM & \(F(x_k)-F(x^*)\le \varepsilon\) & \(\mathcal{O}(\varepsilon^{-1/2})\)\\ \hline
    PSG & \(F(x_k)-F(x^*)\le \varepsilon\) & \(\mathcal{O}(\varepsilon^{-2})\)\\
    \end{tabular}
\end{center}
\chapter{Coordinate Descent Method}
The goal here is to solve the problem
\begin{equation}
    \min_{x\in \R^n}f(x) 
\end{equation}
where \(f:\R^n\rightarrow \R\) is L-smooth and bounded from below by \(f_{low}\).\\

The cost of computing the gradient at each step can require a lot of operations: e.g. the gradient of a quadratic function is calculated in \(\mathcal{O}(n^2)\). In this section, we consider the setting in which \(n\) is huge to such an extent that \(\mathcal{O}(n^p)\) operations to get \(\nabla f(x)\) is not acceptable. 
\section{Randomized Coordinate Descent Method}
This algorithm randomly chooses a single component of the gradient to compute the next iterate, for a L-smooth function.
\begin{algorithm}
    \caption{Randomized Coordinate Descent Method}\label{algo:random_CDM}
    \begin{algorithmic}[1]
    \State \textbf{Step 0:} Given $x_0 \in \mathbb{R}^n$ and $L > 0$, set $k \coloneqq 0$.
    \State \textbf{Step 1:} Choose \(i_k\in \{1,\dots,n\}\) randomly with uniform probability. 
    \State \textbf{Step 2:} Compute $x_{k+1} = x_k - \frac{1}{L}\left(\nabla f(x_k)\right)_{i_k}e_{i_k}$.
    \State \textbf{Step 3:} Set \(k\coloneqq k+1\), and go back to step 1.
    \end{algorithmic}
\end{algorithm}
This algorithm converges in \(\mathcal{O}(n\varepsilon^{-2})\). 
\section{Stochastic Gradient Method}
Consider a dataset \(\{(a^{(i)},b^{(i)})\}_{i=1}^N\subset \R^p\times \R\). Let \(m_X:\R^p\rightarrow \R\) be defined by a parameter \(x\in \R^n\). In ML, we want to find \(x^*\) that solves the optimization problem
\begin{equation}
    \min_{x\in \R^n}\frac{1}{N}\sum_{i=1}^N \underbrace{\left(m_x\left(a^{(i)}\right)-b^{(i)}\right)^2}_{=f_i(x)}
\end{equation}
The cost to compute \(\nabla f(x)\) is thus \(\mathcal{O}(Nn^p)\). We will use the SGD method when N is big.
\begin{algorithm}
    \caption{Stochastic Gradient Descent Method}\label{algo:SGD}
    \begin{algorithmic}[1]
        \State \textbf{Step 0:} Given \(x_0\in \R^n\), \(\alpha_0>0\), set \(k\coloneqq 0\).
        \State \textbf{Step 1:} Choose \(i_k\in \{1,\dots,N\}\) randomly with uniform probability.
        \State \textbf{Step 2:} Compute \(x_{k+1}=x_k-\alpha_k \theta \nabla f_{i_k}(x_k)\).
    \end{algorithmic}
\end{algorithm}
\begin{equation}
    x_{k+1} = x_k-\alpha_k \nabla f_{i_k}(x_k)
\end{equation}
Suppose that $f(\cdot)$ is L-smooth and bounded from below by $f_{low}$, and that $\lVert \nabla f_i(x)\rVert \le G$ $\forall i\in \{1,\dots,n\}$ and $\forall x \in \R^n$. Let us take $\alpha_k = \alpha = \frac{\varepsilon^2}{LG^2}$, the ideal case if we want $\alpha_k$ to be constant. The SGD converges in $\mathcal{O}(\varepsilon^{-4})$, which is very bad. The advantages of this method resides in the easy calculations at each step. 
\subsection{Momentum trick}
The idea is to take into account the previous iterations:
\begin{equation}
    x_{k+1} = x_k - \alpha \left(\sum_{i=0}^k \beta^{k-i}\nabla f(x_i)\right)
\end{equation}
where $\beta \in (0,1)$ is a discount factor. To get this, we can define (using $m_0=0$):
\begin{align}
    m_{k+1}&=\beta m_k + (1-\beta)\nabla f(x_k)\nonumber\\
    x_{k+1}&= x_k - \gamma m_{k+1}
\end{align}
and $\alpha = \gamma(1-\beta)$.\\
This trick can be used with SGD to improve its efficiency. Pushing this to its extremity, we get the AdaGrad method.
\section{AdaGrad}
At the beginning of the $k$th iteration, we choose $i_k\in \{1,\dots,N\}$ randomly with uniform probability and then set 
\begin{align}
    [v_{k+1}]_j &= [v_k]_j + [\nabla f_{i_k}(x_k)]_j^2 \qquad j = 1,\dots, n\nonumber \\
    [x_{k+1}]_j &= [x_k]_j - \frac{\eta}{\delta + \sqrt{[v_{k+1}]_j}}[\nabla f_{i_k}(x_k)]_j\qquad j=1,\dots, n
\end{align}
with $v_0=0$ and $\eta, \delta >0$.
We can now mix the Momentum trick with AdaGrad.
\section{RMSprop}
At the beginning of the $k$th iteration, we choose $i_k\in \{1,\dots,N\}$ randomly with uniform probability and then set 
\begin{align}
    [v_{k+1}]_j &= \beta[v_k]_j + (1-\beta)[\nabla f_{i_k}(x_k)]_j^2 \qquad j = 1,\dots, n\nonumber \\
    [x_{k+1}]_j &= [x_k]_j - \frac{\eta}{\delta + \sqrt{[v_{k+1}]_j}}[\nabla f_{i_k}(x_k)]_j\qquad j=1,\dots, n
\end{align}
with $v_0=0$ and $\eta, \delta >0$.
\section{Adam}
Even more extreme is the Adam method: RMSprop + Momentum trick.
At the beginning of the $k$th iteration, we choose $i_k\in \{1,\dots,N\}$ randomly with uniform probability and then set 
\begin{align}
    m_{k+1} &= \beta_1m_k + (1-\beta_1)\nabla f_{i_k}(x_k)\nonumber \\
    [v_{k+1}]_j &= \beta_2[v_k]_j + (1-\beta_2)[\nabla f_{i_k}(x_k)]_j^2 \qquad j = 1,\dots, n\nonumber \\
    \hat m_{k+1} &= m_{k+1}/\left(1-\beta_1^{k+1}\right)\\
    \hat v_{k+1} &= v_{k+1}/\left(1-\beta_2^{k+1}\right)\nonumber \\
    [x_{k+1}]_j &= [x_k]_j - \frac{\eta}{\delta + \sqrt{[v_{k+1}]_j}}[\hat m_{k+1}]_j\qquad j=1,\dots, n\nonumber 
\end{align}
with $m_0=0, \: v_0=0$, $\beta_1,\beta_2\in (0,1)$ and $\eta, \delta >0$.
\section{Revisiting Armijo Line Search - Cf \ref{sec:armijo}}
\begin{lem}
    Let $f: \R^n\rightarrow \R$ be differentiable at $x\in \R^n$. If $\nabla f(x)^Td<0$ and $\eta\in (0,1)$, then there exists $\delta >0$ such that 
    \begin{equation}
        (x+\alpha d)\le f(x)+\eta \alpha \nabla f(x)^Td\qquad \forall \alpha \in [0,\delta)
    \end{equation}
\end{lem}
We start from the following algorithm:
\begin{algorithm}
    \caption{General Descent Method with Armijo Line Search}\label{algo:armijo2}
    \begin{algorithmic}[1]
        \State \textbf{Step 0:} Given $x_0\in \R^n$, $\alpha_0>0$ and $\eta \in (0,1)$, set $k\coloneqq 0$.
        \State \textbf{Step 1:} If $\nabla f(x_k)=0$, stop.
        \State \textbf{Step 2:} Compute $d_k \in \R^n$ such that $\langle \nabla f(x_k),d_k\rangle <0$.
        \State \textbf{Step 2.1:} Find the smallest integer $i_k\in \{0,\dots,n\}$ such that $\alpha_k = 2^{-i_k}$ satisfies
        \begin{equation}
            f(x_k+\alpha_kd_k)\le f(x_k) + \eta \alpha_k\langle \nabla f(x_k),d_k\rangle
        \end{equation}
        \State \textbf{Step 3:} Define $x_{k+1}=x_k + \alpha_kd_k$, set $k\coloneqq k+1$ and go to \textbf{Step 1}.
    \end{algorithmic}
\end{algorithm}
\begin{lem}
    Let $\{x_k\}_{k\ge 0}$ and $\{\alpha_k\}_{k\ge 0}$ be sequences generared by algorithm \ref{algo:armijo2}. If $f$ is L-smooth, and if there exist $c_1,c_2>0$ such that 
    \begin{equation}\label{eq:A2}
        \begin{cases}
            \langle \nabla f(x_k),d_k\rangle \le -c_1\lVert \nabla f(x_k)\rVert^2\\
            \lVert d_k\rVert \le c_2 \lVert \nabla f(x_k)\rVert
        \end{cases} \qquad \forall k\ge 0
    \end{equation}
    then
    \begin{equation}
        \alpha_k\ge \min\left\{1, \frac{(1-\eta)c_1}{Lc_2^2}\right\} \equiv \alpha_{\min}
    \end{equation}
\end{lem}
\underline{Properties:}
\begin{itemize}
    \item $f(x_k)-f(x_{k+1})\ge \eta \alpha_{\min} c_1\lVert \nabla f(x_k)\rVert^2$
    \item The complexity of \ref{algo:armijo2} is described by table \ref{tab:complexity}
\end{itemize}
\subsection{Choosing the search direction}
If we choose $d_k=-B_k \nabla f(x_k)$ with $c_1I \preceq B_k\preceq c_2$, $\forall k\ge 0$, then the sequence $\{d_k\}_{k\ge 0}$ of search directions verifies equations \eqref{eq:A2}.\\
Here are some examples for $B_k$:
\begin{itemize}
    \item $B_k=I$ : $d_k =-\nabla f(x_k)\Longrightarrow$ Gradient Direction;
    \item $B_k = (\nabla^2f(x_k))^{-1}$ : Newton Direction;
    \item $B_k \approx (\nabla^2f(x_k))^{-1}$ : Quasi-Newton Direction;
\end{itemize}
\chapter{Second order methods}
\section{Newton Method}
Consider the unconstrained optimization 
\begin{equation}
    \min_{x\in \R^n} f(x)
\end{equation}
To fing the basic Newton step, we do a Second order Taylor expansion of \(f\) around \(x_k\): and minimize that quantity. This gives 
\begin{equation}
    0 = \nabla f(x_k) + \nabla^2f(x_k)h \Longleftrightarrow \nabla^2f(x_k)h = -\nabla f(x_k)
\end{equation}
Assuming the Hessian to be invertible, 
\begin{equation}
    h = -\nabla^2f(x_k)^{-1}\nabla f(x_k)
\end{equation}
We call $h$ the Newton step $n(x) = -\nabla^2f(x)^{-1}\nabla f(x)$. 
\begin{itemize}
    \item [\(\rightarrow\)] Note: In practice, we never compute $\nabla^2f(x_k)^{-1}$ as it is not needed by itself.
\end{itemize}
\begin{algorithm}
    \caption{Newton Method}\label{algo:newton}
    \begin{algorithmic}[1]
        \State \textbf{Step 0:} Given $x_0\in \R^n$, $f$, $\nabla f$, $\nabla^2f$ invertible, set $k\coloneqq 0$.
        \State \textbf{Step 1:} If $\nabla f(x_k)=0$, stop.
        \State \textbf{Step 2:} Define $x_{k+1}=x_k - \nabla^2f(x_k)^{-1}\nabla f(x_k)$, set $k\coloneqq k+1$ and go to \textbf{Step 1}.
    \end{algorithmic}
\end{algorithm}
The problem with this method is that it does not find a minimizer, it only computes the solution of $\nabla m_{x_k}(h)=0$. It is not always well defined, with not convex functions, and the computational cost of one iteration is high. 
\begin{thm}
    Let $f\in \mathcal{C}^2$. If $\nabla^2f$ is M-Lipschitz and $x^*$ is a minimum of $f$ such that $\nabla^2f(x^*)\succeq \mu I$, with $\mu>0$, then for any $x$ such that $\lVert x-x^*\rVert\le \frac{\mu}{2M}$, we have
    \begin{equation}
        \lVert x^+-x^*\rVert \le \frac{M}{\mu}\lVert x-x^*\rVert^2
    \end{equation}
    where $x^+ = x - \nabla^2f(x)^{-1}\nabla f(x)$ is well-defined.
\end{thm}
Newton's method is invariant with respect to linear changes of variables, while gradient/first-order methods are not. However, it does not always converge. 
\section{Self-concordance}
\begin{definition}
    Given an open domain $X\subseteq \R^n$, a function $f:X\rightarrow \R$ is called self-concordant iff 
    \begin{itemize}
        \item $f\in \mathcal{C}^3$ is convex;
        \item $f$ is closed, i.e. its epigraph is a closed set;
        \item $\nabla^3f(x)[h,h,h]\le 2\nabla^2 f(x)[h,h]^{3/2}$, $\forall x\in X$, $\forall h\in \R^n$.
    \end{itemize}
    with 
    \begin{itemize}
        \item $\nabla f(x)[h] = \nabla f(x) \cdot h$
        \item $\nabla^2f(x)[h,h] = h^T\nabla^2 f(x)h$
        \item $\nabla^3f(x)[h,h,h] = \sum_i\sum_j\sum_k \frac{\partial^3 f}{\partial x_i \partial x_j \partial x_k}h_ih_jh_k$
    \end{itemize}
\end{definition}
For univariate functions, the conditions are simpler:
\begin{itemize}
    \item $f\in \mathcal{C}^3$ is convex;
    \item $|f'''(x)|\le 2f''(x)^{3/2}$, $\forall x \in X$
\end{itemize}
\begin{prop}
    The self-concordance property is conserved by sum and by linear changes of variables, and is nondegenerate:\\
    Let $X$ be an open set containing no line. Then,
    \begin{itemize}
        \item Any self concordant function defined over $X$ satisfies $\nabla^2f(x)\succ 0$;
        \item $f(x)\rightarrow \infty$ as $x\rightarrow \partial X$, where $\partial X$ is the boundary of $X$.
    \end{itemize}
\end{prop}
\section{Local norms}
Optimality measure $\lVert \nabla f(x)\rVert$ is not suitable, as it is not affine-invariant. This is why we define local norms:
\begin{definition}
    Given a self-concordant function $f$, the local norm at $x$ is 
    \begin{equation}
        \lVert z\rVert_x = (z^T\nabla^2f(x)z)^{1/2}
    \end{equation}
    The corresponding dual norm is given by
    \begin{equation}
        \lVert z\rVert_x^* = (z^T\nabla^2f(x)^{-1}z)^{1/2}
    \end{equation}
\end{definition}
\section{Optimality measure}
Using the dual local norm defined in the last section, we define the optimality measure:
\begin{equation}
    \delta(x)\coloneqq \lVert \nabla f(x)\rVert_x^* = \lVert n(x)\rVert_x
\end{equation}
Because of convexity, $x$ is optimal iff $\nabla f(x) = 0\Longleftrightarrow \delta(x)=0$.
\section{Improving Newton}
Given an open convex domain $X$ and a self-concordant function $f:X\rightarrow \R$, we want $\min_{x\in X} f(x)$.\\
Let $x\in X$ such that $\delta(x)<1$:
\begin{itemize}
    \item A global minimum $x^*$ of $f$ exists;
    \item $f(x)\le f(x^*)-\delta(x) - \log(1-\delta(x)) \Longrightarrow f(x)-f(x^*) =\mathcal{O}(\delta(x)^2)$;
    \item $\lVert x-x^*\rVert_x \le \frac{\delta}{1-\delta} = \mathcal{O}(\delta(x))$;
    \item Newton step $x^+=x+n(x)$ is feasible, i.e. $x^+\in X$, meaning the method is well-defined;
    \item $\delta(x^+)\le \left(\frac{\delta}{1-\delta}\right)^2$ meaning quadratic convergence. 
\end{itemize}
If the method starts close to the minimizer, converges in less than 10 iterations.\\
If $x\in X$ and $\delta(x)\ge 1$, the good behaviour of the method is no longer guaranteed. To fix this, we introduce a damping of the steps:\\

For any $x\in X$ and thus any $\delta(x)$,
\begin{itemize}
    \item The damped Newton step $x^+=x+\left(\frac{1}{1+\delta(x)}\right)n(x)$ is feasible;
    \item The decrease is guaranteed: $f(x)-f(x^+) \ge \delta(x)-\log(1+\delta(x))\ge 0$.
\end{itemize}
\section{Globally convergent Newton's method}
Suppose $\delta(x_0)>\frac{1}{\sqrt{2}}$. While $\delta(x_i)>\frac{1}{\sqrt{2}}$, 
\begin{equation}
    f(x_i)-f(x_{i+1}) > \frac{1}{\sqrt{2}}-\log(1+\frac{1}{\sqrt{2}}) > \frac{1}{6}
\end{equation}
Hence, after at most $k \le \lceil 6(f(x_0)-f(x^*))\rceil$, we must have $\delta(x_k)\le \frac{1}{\sqrt{2}}$. Applying pure Newton steps once this stage is reached, we obtain a $\epsilon$-solution after 
\begin{equation}
    \mathcal{O}(f(x_0)-f(x^*)) + \mathcal{O}(\log \log \frac{1}{\epsilon}) \text{   iterations}
\end{equation}
\section{Interior-point methods}
Let $f:\R^n\rightarrow \R$ be a convex function and $C\subseteq \R^n$ be a closed convex set. We want to optimize 
\begin{equation}
    \inf_{x\in \R^n} c^Tx \text{ such that }x\in C 
\end{equation}
Interior-point methods consist in approximating the constrained problem by a family of unconstrained problems, using a barrier function $F$ to replace $x\in C$. 
\subsection{Central path}
Let $\mu>0$ be a scalar parameter and consider 
\begin{equation}\label{eq:central_path}
    \inf_{x\in \R^n} \frac{c^Tx}{\mu} + F(x) = f_\mu(x)
\end{equation}
The solution $x_\mu^*$ tends to $x^*$ the solution of the initial problem, as $\mu$ tends to 0. \\

To compute $x_\mu^*$, we can use Newton steps, solving 
\begin{equation}
    \frac{c}{\mu}+\nabla F(x) = 0
\end{equation}
\subsubsection{Accuracy}
Assume $f$ is a $\nu$-self-concordant barrier. We have the property 
\begin{equation}
    c^Tx_\mu -c^T x^* \le \nu \mu \qquad \forall \mu>0
\end{equation}
\begin{thm}
    Assume $x$ is such that $\delta_\mu(x)\le \tau <1$. Then,
    \begin{equation}
        c^Tx-c^Tx^* <\frac{\nu \mu}{1-\tau}
    \end{equation}
    We can thus choose $\mu_{final}$ as the solution to 
    \begin{equation}
        \frac{\nu\mu}{1-\tau} = \epsilon
    \end{equation}
    guaranteeing a final $\epsilon$ accuracy on the linear objective.
\end{thm}
\section{Short-step algorithm}
\begin{equation}\label{eq:short_step}
    \min c^Tx \text{ such that }x\in C
\end{equation}
Let $F$ be a $\nu$-self-concordant barrier such that $dom(F)=int(C)$
\begin{algorithm}
    \begin{algorithmic}[1]
        \caption{Short-step algorithm}\label{algo:short_step}
        \State Given a starting point $x_0\in int(C)$ and a target accuracy $\epsilon$;
        \State Pick suitable parameters $0<\tau<1$ and $0<\theta<1$;
        \State Find a value $\mu_0$ such that $\delta_{\mu_0}(x_0)\le \tau$ and compute $\mu_f=\epsilon \frac{1-\tau}{\nu}$;
        \While{$\mu_k>\mu_f$}\\
        $\qquad \mu_{k+1}=(1-\theta)\mu_k$;\\
        $\qquad x_{k+1} = x_k + n_{\mu_{k+1}}(x_k)$;\\
        $\qquad k=k+1$;
        \EndWhile 
    \end{algorithmic}
\end{algorithm}
\begin{prop}
    The total number of iterations until the algorithm stops is 
    \begin{equation}
        N = \left\lceil \log_{1-\theta} \left(\frac{\mu_f}{\mu_0}\right)\right\rceil
    \end{equation}
\end{prop}
The complexity of the short-step method is $\mathcal{O}(\sqrt{\nu}\log \frac{1}{\epsilon})$ iterations. 
\subsection{Initialization}
\begin{itemize}
    \item To pick a reasonable $\bar x$, we can take an element of $int(C)$, not necessarily close to the central path;
    \item For $\mu_0$, any value $>0$ works, but $\mu_0$ too small will lead to longer Initialization. A good choice is the min of $\delta_{\mu_0}(\bar x)$;
    \item Compute $x_0$ solving $\min \frac{c^Tx}{\mu_0}+F(x)$ using damped Newton steps and starting from $\bar x$; stop the procedure when $\delta_{\mu_0}(x_0)\le \tau$.
\end{itemize}
\subsection{Convex nonlinear objective}
If the objective function of \eqref{eq:short_step} is nonlinear, we can rewrite the problem:
\begin{equation}
    \min_{x,t}t \text{ such that }\begin{cases}
        f(x)\le t\\
        x\in C
    \end{cases}
\end{equation}
By the properties of convexity and self-concordant barriers, this is equivalent.
\subsection{Affine change of variables}
If the problem is of the form 
\begin{equation}
    \min c^Tx\text{ such that }Ax-b\in C
\end{equation}
and if we have a self-concordant barrier $F$ on $C$, then $x\mapsto F(Ax-b)$ is a self-concordant barrier for $\{x:Ax-b\in C\}$.
\subsection{Linear equalities}
If the problem is of the form \eqref{eq:short_step}, but with the additionnal constraint that $Ax=b$, the set $C\cap \{x:Ax=b\}$ has no interior. We must modify the short-step algorithm: the step $n_{Ax=b}$ is the solution of 
\begin{equation}
    \begin{pmatrix}
        \nabla^2F(x) & A^T\\
        A & 0\\
    \end{pmatrix}\begin{pmatrix}
        n\\ \lambda 
    \end{pmatrix} = \begin{pmatrix}
        -\nabla F(x)\\ 0\\
    \end{pmatrix}
\end{equation}
with $\lambda$ the Lagrangian multipliers.
\subsection{Nonconvex problems}
Nonconvex problems can most of the time be reexpressed as convex problems but, due to their shape, it is not helpful to solve them efficiently.\\
\section{Long-step method}
The long-step method is more efficient in terms of number of iterations than the short-step. The algorithm is the following:
\begin{algorithm}
\begin{algorithmic}[1]
    \caption{Long-step method}\label{algo:long_step}
    \State Given $x_0,\mu_0, 0<\tau<1, 0<\theta<1$ such that $\delta_{\mu_0}(x_0)\le \tau$;
    \While {$\mu_k>\mu_f$}\\
    $\qquad \mu_{k+1}=(1-\theta)\mu_k$;\\
    $\qquad$ Compute $x_{k+1}$ such that $\delta_{\mu_{k+1}}(x_{k+1})\le \tau$;
    \EndWhile
    $\qquad k=k+1$;
\end{algorithmic}
\end{algorithm}
\begin{thm}
Consider a problem equipped with a $\nu$-self-concordant barrier. For any $0<\tau<1$ and $0<\theta<1$, the long-step method stops after $\mathcal{O}(\nu \log(\frac{1}{\epsilon}))$. This is theoretically worse than the short-step, but it rarely needs more than 20-50 iterations. 
\end{thm}
\chapter{Conic optimization}
\section{Reminder}
In linear optimization, there exists a dual for any problem:
\begin{center}
    \begin{tabular}{c|c}
        Primal & Dual\\ \hline 
        $\min_x c^Tx$ & $\max_y b^Ty$\\
        $Ax=b$ and $x\ge 0$ & $A^Ty \le c$ \\ \hline 
        $x\in \R^n$ & $y\in \R^m$\\
    \end{tabular}    
\end{center}
The main goal of conic optimization is to generalize linear optimization, while trying to keep the nice properties of duality and efficient algorithms. \\
\section{Inequalities}
For vectors of real numbers $a,b\in \R^n$, we define inequalities componentwise:
\begin{equation}
    \begin{aligned}
        a\ge 0 &\Leftrightarrow a_i\ge 0 \Leftrightarrow a_i\in \R_+ \qquad \forall i\in \{1,\ldots,n\}    \\
        a\ge b & \Leftrightarrow a_i- b_i\ge 0 \Leftrightarrow a_i-b_i\in \R_+ \qquad \forall i\in \{1,\ldots,n\}
    \end{aligned}
\end{equation}
Let $K\subseteq \R^n$ be an arbitrary set. Define $a\succeq_K0\Leftrightarrow a\in K$ for any vector $a\in \R^n$.\\ This allows to define a generalized primal linear optimization problem:
\begin{equation}
    \min_{x\in \R^n} c^Tx\text{ such that }Ax=b \text{ and }x\succeq_K0
\end{equation}
\begin{itemize}
    \item [$\rightarrow$] Note: We find a linear optimization problem if $K=\R^n_+$.
\end{itemize}
The generalized dual linear optimization problem is 
\begin{equation}
    \max_{y\in \R^m}b^Ty \text{ such that } A^Ty\preceq_K c
\end{equation}
\subsection{Requirements for an order}
An order $\succeq$ or $\preceq$ must verify two properties:
\begin{enumerate}
    \item $a\succeq_K0\Rightarrow \lambda a\succeq_K0$ for any $\lambda\ge 0$. This means that $K$ is a cone.
    \item $a\succeq_K0$ and $b\succeq_K0$ implies $a+b\succeq_K0$. This means that $K$ is closed under addition. \\
    
    This simply means that the linear combination of two elements of the set stays in the set.
    \item We require $x\succeq_K0$ and $x\preceq_K0\Longrightarrow x=0$.
    \item Define the strict inequality by $a\succ_K0\Leftrightarrow a\in int(K)$. We require that $int(K) \neq \emptyset$, i.e. the cone is solid.
    \item We want the limits to preserve the order:
    \begin{equation}
        \lim_{i\rightarrow \infty} x_i = \bar x \text{ with }x_i\succeq_K0 \:\forall i \Rightarrow \bar x \succeq_K0
    \end{equation}
\end{enumerate}
\begin{itemize}
    \item [$\rightarrow$] Note: Any set satisfying those two first properties must be convex. Moreover, for a cone, those properties and convexity are equivalent. 
\end{itemize}
A cone is proper if it is solid, pointed and closed. 
\begin{itemize}
    \item [$\rightarrow$] Note: If $K_1,K_2$ are proper cones, then $K_1\times K_2$ also is. 
\end{itemize}
\section{Conic hull}
\begin{definition}
    Given a convex set $X\subseteq \R^n$ in dimension $n$, its conic hull is the following set in $n+1$ dimensions:
    \begin{equation}
        \text{conic hull }X = cl\left\{(x,t)\in \R^n\times \R \text{ such that } t>0 \text{ and }\frac{x}{t}\in X\right\}
    \end{equation}
\end{definition}
\begin{thm}
    The conic hull is always a close cone, and a set is convex iff its conic hull is convex.
\end{thm}
Hence the convex problem \eqref{eq:short_step} is equivalent to 
\begin{equation}
    \min_{x,t} \begin{pmatrix}
        c & 0
    \end{pmatrix}\begin{pmatrix}
        x\\t \\
    \end{pmatrix} \text{ such that }
    \begin{cases}
        \begin{pmatrix} x \\t\\ \end{pmatrix}
        \in \text{ conic hull}\\
        t = 1    
    \end{cases}
\end{equation}
\subsection{Examples of cones}
\begin{itemize}
    \item The only convex cones in $\R$ are $\emptyset, \R,\R_+,\R_-$. 
    \item In $\R^2$, a convex cone is either $\emptyset, \R^2$, a line through the origin, or a region between two half-lines from the origin, with an angle of less than $\pi$. 
    \item Lorentz-cone: $\mathbb{L}^n = \left\{(x_0,\dots,x_n)\in \R^{n+1}|\sqrt{x_1^2+\dots+x_n^2}\le x_0\right\}$. 
    \item Semi-definite cone: $K=\mathbb{S}_+^n$, i.e. the set of symmetric positive semidefinite matrices. 
\end{itemize}
\section{Dual cone}
Given a convex cone $K$, the dual cone $K^*$ is defined as
\begin{equation}
    K^* = \{z\in \R^n\text{ such that } x^Tz \ge 0 \: \forall x\in K\}
\end{equation}
$K^*$ is always a convex cone, even if $K$ is not, it is always closed and, if $K$ is closed, then $(K^*)^* = K$. \\
$K^*$ is pointed if $K$ is solid and solid if $K$ is pointed. Therefore, if $K$ is proper, then $K^*$ is too. \\
We can now define the dual problem. 
\subsection{Primal-Dual pair of conic problems}
\begin{equation}
    \begin{aligned}
        \min c^Tx \text{ such that } Ax=b\text{ and }x\succeq_K0\\
        \max b^Ty \text{ such that } A^Ty\preceq_K c\\
    \end{aligned}
\end{equation}
Some cones are self-dual:
\begin{itemize}
    \item $(\R_+^n)^* = \R_+^n$;
    \item $(\mathbb{L}^n) = \mathbb{L}^n$;
    \item $(\mathbb{S}_+^n)^* = \mathbb{S}_+^n$;
\end{itemize}
and the cartesian product works with duality: $(K_1\times K_2)^* = K_1^* \times K_2^*$.
\begin{thm}
    If $x$ is feasible for the primal problem and $y$ is feasible for the dual problem, then inequality $b^Ty \le c^Tx$ holds. This is weak duality. 
\end{thm}
\begin{thm}
    If both problems admit feasible solutions, the primal optimal value $p^*$ and dual optimal value $d^*$ are finite and satisfy $d^*\le p^*$. If both problems admit feasible solutions $x$ and $y$ such that $c^Tx=b^Ty$, then both $x$ and $y$ are optimal.
\end{thm}
\begin{thm}
    The dual of an unbounded problem is infeasible.
\end{thm}
\begin{itemize}
    \item [$\rightarrow$] Note: Strong duality does not hold in general for conic optimization.
\end{itemize}
\section{Strong duality}
\begin{definition}
    A feasible solution to a conic problem is strictly feasible iff it belongs to the interior of the cone:
    \begin{itemize}
        \item $x$ is strictly feasible for the primal iff we have both $Ax=b$ and $x\succeq_K0$;
        \item $y$ is strictly feasible for the dual iff $A^Ty\prec_{K^*}c$.
    \end{itemize}
\end{definition}
\begin{thm}
    If both the primal and the dual problems admit a strictly feasible solution, they are both solvable and their optimal solutions $x^*$ and $y^*$ satisfy $c^Tx^*=b^Ty^*$.
\end{thm}
\section{Cones and barriers}
\subsection{Nonnegative/linear cone}
\begin{equation}
    \R_+=\{x\in\R \text{ such that }x\ge 0\}
\end{equation}
The associated self-concordant barrier with parameter $\nu=1$ is 
\begin{equation}
    int \: \R_+ \rightarrow \R:x\rightarrow -\log x
\end{equation}
In $\R^n$, this becomes $K=\R_+^n$ with $\nu=n$. 
\subsection{Second order cone}
\begin{equation}
    \mathbb{L}^n = \{(x_0,\dots,x_n)\in \R^{n+1}|\lVert x_1^2+\dots+x_n^2\rVert\le x_0\}
\end{equation}
The corresponding self-concordant barrier with parameter $\nu=2$ is 
\begin{equation}
    int \: \mathbb{L}^n \rightarrow \R :\: x \rightarrow -\log(x_0^2-x_1^2 - \dots-x_n^2)
\end{equation}
\subsection{Rotated second-order cone}
\begin{equation}
    \mathbb{L}_R^n = \{(x_1,\dots,x_n,y,z)\in \R^n\times \R^2_+ \text{ such that }2yz \ge \lVert (x_1,\dots,x_n)\rVert^2\}
\end{equation}
The corresponding self-concordant barrier with parameter $\nu=2$ is 
\begin{equation}
    int \: \mathbb{L}_R^n \rightarrow \R :\: x \rightarrow -\log(2yz-x_1^2 - \dots-x_n^2)
\end{equation}
The rotated second-order cone is equivalent to a quadratic cone:
\begin{equation}
    (x_1,\dots,x_n,y,z)\in \mathbb{L}_R^n \Longleftrightarrow \left(\frac{y+z}{\sqrt{2}}, x_1,\dots,x_n,\frac{y-z}{\sqrt{2}}\right)\in \mathbb{L}^{n+1}
\end{equation}
\subsection{Exponential cone}
\begin{equation}
    \mathbb{E} = cl\{(x,y,z)\in \R^3\text{ such that }z\ge ye^{x/y} \text{ and } y>0\}
\end{equation}
The associated self-concordant barrier with parameter $\nu=3$ is
\begin{equation}
    int \: \mathbb{E} :(x,y,z) \rightarrow -\log (z-ye^{x/y})-\log y-\log z
\end{equation}
\subsection{Power cone}
Given a parameter $0<\alpha<1$, the power cone is 
\begin{equation}
    \mathbb{P}_\alpha = \{(x,y,z)\in \R^3 \text{ such that }x^\alpha y^{1-\alpha}\ge |z| \text{ and }x\ge0,y\ge0\}
\end{equation}
The associated self-concordant barrier with parameter $\nu=4$ is
\begin{equation}
    int \: \mathbb{P}_\alpha :(x,y,z) \rightarrow -\log (x^{2\alpha} y^{2-2\alpha}-z^2)-\log x -\log y 
\end{equation}
\begin{itemize}
    \item [$\rightarrow$] Note: the power cone with $\alpha=1/2$ is the rotated second-order cone.
\end{itemize}
\subsection{Handling p-norms}
The inequality $\lVert u\rVert_p \le t$ is not the same as $\lVert u\rVert_p^p \le t^p$. It is rather equivalent to 
\begin{equation}
    \lVert u\rVert_p \le t \Longleftrightarrow |u_i| \le t_i^{1/p}t^{1-1/p} \qquad \sum_i t_i = t
\end{equation}
\section{Duals of cones}
\begin{itemize}
    \item Linear cone: $(\R_+)^* = \R_+$;
    \item Quadratic cone: $(\mathbb{L}^n)^* = \mathbb{L}^n$;
    \item Power cone: $(\mathbb{P}_\alpha)^* = \{(u,v,w)\in \R_+\times \R_+\times \R \: |w| \le \left(\frac{u}{\alpha}\right)^\alpha \left(\frac{v}{1-\alpha}\right)^{1-\alpha}\}$;
    \item Exponential cone: $(\mathbb{E})^* = cl\{(u,v,w)\in \R_- \times \R\times \R_+\: : w\ge -ue^{v/u-1}\le 0\}$.
\end{itemize}
\section{Computing a dual problem}
\begin{equation}
    \min_z z^TQz \text{ such that }Gz\le f 
\end{equation}
with $Q\in \mathbb{S}_+^n$ and $G\in \R^{m\times n}$.
\subsection{Write a conic formulation}
\begin{enumerate}
    \item Make the objective linear using the epigraph technique:
    \begin{equation}
        \min_{z\in \R^n,t\in \R} t \text{ such that }Gz\le f\text{ and } z^TQz\le t
    \end{equation}
    \item Represent the constraints with cones: $K_1 =\R^m_+$ and $K_2 = \mathbb{L}_R$;
    \item Define the dual variable: $y=\begin{pmatrix}
        z\\t
    \end{pmatrix} \in \R^n\times \R$;
    \item Use $Q=LL^T$ for a change of variables. 
\end{enumerate}
\subsection{Derive the dual problem}
\begin{enumerate}
    \item Write the dual with the usual formula;
    \item Simplify the dual.
\end{enumerate}
\section{Duality application}
Duality is often the only way to 
\begin{itemize}
    \item Establish a lower bound for a min problem or an upper bound in a max problem;
    \item Certify optimality of a given problem;
    \item Derive properties of optimal solutions without computing them.
\end{itemize}
\subsection{Optimal value functions}
Consider a conic problem with fixed cone $K$ and matrix $A$. We define the primal and dual values 
\begin{equation}
    \begin{aligned}
        p^*(b,c) &= \min c^Tx \text{ such that } Ax=b \text{ and }x\succeq_K0\\
        d^*(b,c) &= \max b^Ty \text{ such that }A^Ty\preceq_{K^*}0
    \end{aligned}
\end{equation}
Function $d^*$ is convex in $b$ when $c$ is fixed, and function $p^*$ is concave in $c$ when $b$ is fixed. By strong duality, $p^*(b,c)=d^*(b,c)$, hence both functions are convex in $b$ and concave in $c$. \\

Let us define 
\begin{equation}
    f_c(b) \coloneqq p^*(b,c)
\end{equation}
and $y_c^*(b)$ the optimal solution of the dual for any $b$. As $d^*$ is linear in $b$, and since strong duality implies $f_c(b) = d^*(b,c)$, 
\begin{equation}
    f_c(b+\Delta b) \ge f_c(b)+y_c^*(b)^T\Delta b
\end{equation}
meaning that $y_c^*(b)$ is a subgradient of $f_c(b)$. Therefore, dual variables tell how the objective value changes when $b$ is modified, and when the dual has a unique solution, $y_c(b)$ is the gradient of $f_c(b)$. The same reasoning holds when $b$ is fixed:$x_b^*(c)$ is a sup-gradient of $g_b(c)\coloneqq p^*(b,c)$ and is its gradient if it is unique. \\
The sensitivity to coefficients of $A$ can also be computed (not here).
\subsection{Robust optimization}
Consider a single constraint $\alpha^Ty\le b$ where $\alpha$ is not known precisely and belongs to an uncertainty set $\mathcal{A} \coloneqq \{\alpha | C\alpha \succeq_K d\}$. We want to find solutions such that the constraint holds for any $\alpha\in \mathcal{A}$. \\
It is equivalent to requiring that 
\begin{equation}
    \alpha^Ty\le b\:\forall \alpha\in \mathcal{A}\Longleftrightarrow \left[\max_{\alpha\in \mathcal{A}} \alpha^Ty \right] \le b
\end{equation}
Which can be converted into the easier (conic dual) problem 
\begin{equation}
    \left[\min_{x\succeq_{K^*}0}d^Tx \text{ such that }C^Tx = y\right] \le b
\end{equation}
With this formulation, we only need to find ONE $x$ verifying the property, because if there exists one, then the min verifies it too. \\
In summary, a robust constraint is equivalent to the finite constraint 
\begin{equation}
    \exists x\text{ such that }d^Tx \le b\text{ and }C^Tx = y,x\succeq_{K^*}0
\end{equation}
\section{Semidefinite optimization}
The semidefinite cone is the cone of positive semidefinite matrices.
\begin{itemize}
    \item $S$ is positive semidefinite iff 
    \begin{itemize}
        \item all eigenvalues are nonnegative;
        \item the quadratic form $x^TSx$ is always nonnegative;
        \item there exists a factorization $S=BB^T$.
    \end{itemize}
    \item The inner product is defined by 
    \begin{equation}
        \langle S,T\rangle = tr(S^TT)
    \end{equation}
    \item The induced norm is the Frobenius norm.
    \item With that inner product, the cone is self-dual. 
\end{itemize}
Working with matrices, the problem is
\begin{equation}
    \min C \bullet X \text{ such that }A_i \bullet X = b_i \text{ and } X\succeq 0
\end{equation}
and its dual becomes 
\begin{equation}
    \max b^Ty \text{ such that }\sum_{i=1}^m A_i y_i \preceq C
\end{equation}
\begin{itemize}
    \item The primal variable is a matrix, but the dual is a vector;
    \item $A_i\in \mathcal{S}^n$, $1\le i \le m$ are $m$ symmetric.
\end{itemize}
\subsection{Applications}
\begin{itemize}
    \item An ellipsoid of center $c$ is defined as 
    \begin{equation}
        \mathcal{E} = \{x\text{ such that }(x-c)^TE(x-c) \le 1\}
    \end{equation}
    with $E$ positive semidefinite.
    \item Semidefinite constraints can exert control over the eigenvalues of the variable matrix $S$: $\lambda_{\min}(S) \ge a \Longleftrightarrow S-aI \succeq 0$.
\end{itemize}

\end{document}