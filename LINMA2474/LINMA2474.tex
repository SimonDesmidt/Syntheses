\documentclass[12pt, openany]{report}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage[a4paper,left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage[english]{babel}
\usepackage{libertine}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{float}
\usepackage{enumitem}
\usepackage{pythonhighlight}
\usepackage[]{titletoc}
\usepackage{empheq}
\usepackage{titlesec}
\usepackage{mathpazo}
\usepackage{xfrac}
\usepackage{textcomp}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{tabularray}
\usepackage{subcaption}
\usepackage{cancel}
\usepackage[bottom]{footmisc}
\usepackage{pdfpages}
\usepackage{tabularx}
\usepackage{amsthm}
\usepackage[skins]{tcolorbox}
\titleformat{\chapter}[display]
  {\normalfont\bfseries}{}{0pt}{\Huge}
\usepackage{hyperref}
\newcommand{\hsp}{\hspace{20pt}}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\N}{\mathbb{N}}
\renewcommand{\O}{\mathcal{O}}
\theoremstyle{definition}
\newtheorem{thm}{Theorem}[chapter]
\newtheorem{definition}[thm]{Definition}
\newtheorem{lem}[thm]{Lemma}

\hbadness=100000
\begin{document}
\begin{titlepage}
	\begin{sffamily}
	\begin{center}
		\includegraphics[scale=0.2]{img/page_de_garde.png} \\[1cm]
		\HRule \\[0.4cm]
		{ \huge \bfseries LINMA2474 - High-Dimensional Data Analysis and Optimization \\[0.4cm] }
	
		\HRule \\[1.5cm]
		\textsc{\LARGE Issambre L'Hermite Dumont \\ \LARGE Simon Desmidt}\\[3cm]
		{This summary may not be up-to-date, the newer version is available at this address: \hyperlink{https://github.com/SimonDesmidt/Syntheses}{https://github.com/SimonDesmidt/Syntheses}}
		\vfill
		\vspace{2cm}
		{\large Academic year 2025-2026 - Q2}
		\vspace{0.4cm}
		 
		\includegraphics[width=0.15\textwidth]{img/epl.png}
		
		UCLouvain\\
	
	\end{center}
	\end{sffamily}
\end{titlepage}

\setcounter{tocdepth}{1}
\tableofcontents
\chapter{Introduction to optimization on manifolds}
\section{Introduction}
Classical optimization methods like the gradient descent solve problems of the form 
\begin{equation}\label{eq:opt}
	\min_{x\in \mathcal{M}} f(x)
\end{equation}
for a set $M$. The methods rely on two key properties:
\begin{itemize}
	\item Linearity: $x_k$ and $\nabla f(x_k)$ belong to some vector space, in which they can be combined with linear operations;
	\item Inner product: $\nabla f(x_k)$ is the unique element of $\R^D$ such that 
	\begin{equation}
		\forall v\in \R^D, \: Df(x)[v] = \langle v, \nabla f(x)\rangle 
	\end{equation}
	where $Df(x)[v] = \lim_{t\to 0} \frac{f(x+tv) - f(x)}{t}$ is the directional derivative of $f$ at $x$ in the direction $v$.
\end{itemize}
There are two ways to see \ref{eq:opt}: as a constrained optimization problem, or as an unconstrained optimization problem assuming that nothing else exists outside the set $\mathcal{M}$. Optimization on manifolds extends the classical unconstrained optimization algorithms to problems whose search space is a manifold. \\
We define a manifold as a set that can be locally approximated linearly (and therefore smooth).
\section{Examples}
\subsection{The sphere}
The sphere is a manifold: let $\mathcal{S}^{d-1} \eqqcolon \{x\in \R^d :\: \|x\|_2^2 = 1\} = \{x\in \R^d : \: x^T x = 1\}$. This set is thus defined by the constraint $h(x)\eqqcolon x^Tx-1 = 0$. We call this function $h:\R^d \to \R$ a defining function. \\
Let us use a Taylor approximation to derive the local linearization of $\mathcal{S}^{d-1}$ around any point $x\in \mathcal{S}^{d-1}$. Let $v\in \R^d$ be an arbitrary vector.
\begin{equation}
	h(x+tv) = h(x) + tDh(x)[v] + \mathcal{O}(t^2)
\end{equation}
Therefore, at first order, $x+tv\in \mathcal{S}^{d-1}$ iff 
\begin{equation}
	Dh(x)[v] = 2x^T v = 0 \Longleftrightarrow v^Tx = 0
\end{equation}
This means that around any point $x\in \mathcal{S}^{d-1}$, the sphere can be locally approximated by the set $\{v\in \R^d : \: x^Tv = 0\}$, called the tangent space. \\
While the standard gradient descent defines the relation
\begin{equation}
	x_{k+1} = x_k - \eta \nabla f(x_k)
\end{equation}
we rather define the retraction operator for optimization on manifolds:
\begin{equation}
	x_{k+1} = \mathcal{R}_{x_k}(-\eta \text{grad}f(x_k))
\end{equation}
which will be explained later.
\begin{itemize}
	\item A cube is not a Riemannian manifold because of the edges: they are not smooth;
	\item The set of matrices of rank $r$ is a manifold, but the set of matrices of rank $\le r$ is not, because going from rank $i$ to $i+1$ is not smooth. 
\end{itemize}
\section{Applications}
\subsection{The Netflix problem}
Let us consider that we know some ratings of users for movies, and we want to predict their rating for films based on their previous experiences. As people have hardly seen any movies in the catalogue, the rating matrix is very sparse. \\
Let us define $M\in \R^{m\times n}$ as the rating matrix, and $\Omega \subseteq \{1,\dots,m\}\times \{1,\dots,n\}$ as the set of indices of the known ratings. The problem is to find $X\in \R^{m\times n}$ that solves
\begin{equation}
	\min_{X\in \R^{m\times n}} \sum_{(i,j)\in \Omega} (M_{ij}-X_{ij})^2
\end{equation}
We want to express $X$ as the product of two low-rank matrices $U\in \R^{m\times r}$ and $V\in \R^{n\times r}$, such that $X = UV^T$. Those matrices contain how much the users enjoy some features (long movies vs series, action vs romance, etc.) and how much those features are present in the movies. \\
The problem then becomes
\begin{equation}
	\min_{X\in \R^{m\times n}_k} \sum_{(i,j)\in \Omega} (M_{ij}-X_{ij})^2
\end{equation}
where the set $\R^{m\times n}_k$ is the set of matrices of size $m\times n$ and of rank $k$. This set is a manifold, as will be proved later.
\subsection{Dictionary learning}
Let $x_1,\dots,x_m$ be a collection of datapoints. The goal is to learn $k$ atoms $b_1,\dots,b_k$ ($k\ll m$) such that each datapoint $x_i$ can be represented by a small number of properly chosen atoms: we want $X\approx BC$ for $B\in \R^{d\times k}$ and $C\in \R^{k\times m}$. \\
The problem writes
\begin{equation}
	\min_{B,C} \|X-BC\|^2 + \lambda \|C\|_0 \qquad \text{   s.t.   } \qquad \|b_i\|=1 \qquad \forall i=1,\dots,k
\end{equation}
where the $\|\cdot \|_0$ norm is the number of non-zero entries in the matrix. The constraint is added to reduce the number of solutions and to be able to use a manifold. It defines the oblique manifold:
\begin{equation}
	\mathcal{OB}(d,k) \eqqcolon \{X\in \R^{d\times k} : \: \|X_{:,i}\|^2_2 = 1, \: \forall i\}
\end{equation}
\subsection{PCA}
Let $x_1,\dots,x_n$ be a centered dataset in $\R^d$. We aim to find a collection of $k$ orthogonal unit-norm vectors $u_1,\dots,u_k$ such that the subspace spanned by these vectors captures the most of the variance of the initial dataset. It can be expressed as an optimization problem:
\begin{equation}
	\max_{U\in \R^{d\times k}} tr(U^TXX^TU) \qquad \text{   s.t.   } \qquad U^TU = I_k
\end{equation}
This helps define the Stiefel manifold:
\begin{equation}
	\mathcal{St}(d,k) \eqqcolon \{X\in \R^{d\times k} : \: X^TX = I_k\}	
\end{equation}
\begin{itemize}
	\item [$\to$] Note: the cost function is invariant by rotation: $f(UQ) = f(U)$. 
\end{itemize}
\end{document}