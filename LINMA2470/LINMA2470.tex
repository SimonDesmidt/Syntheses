\documentclass[12pt, openany]{report}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage[a4paper,left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage[english]{babel}
\usepackage{libertine}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{float}
\usepackage{enumitem}
\usepackage{pythonhighlight}
\usepackage[]{titletoc}
\usepackage{empheq}
\usepackage{titlesec}
\usepackage{mathpazo}
\usepackage{xfrac}
\usepackage{textcomp}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{tabularray}
\usepackage{subcaption}
\usepackage[bottom]{footmisc}
\usepackage{pdfpages}
\usepackage{tabularx}
\usepackage{amsthm}
\usepackage[skins]{tcolorbox}
\titleformat{\chapter}[display]
  {\normalfont\bfseries}{}{0pt}{\Huge}
\usepackage{hyperref}
\newcommand{\hsp}{\hspace{20pt}}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\E}{\mathbb{E}}
\theoremstyle{definition}
\newtheorem{thm}{Theorem}[chapter]
\newtheorem{definition}[thm]{Definition}
\newtheorem{lem}[thm]{Lemma}

\hbadness=100000
\begin{document}
\begin{titlepage}
    \begin{sffamily}
    \begin{center}
        \includegraphics[scale=0.5]{img/page_de_garde.png} \\[1cm]
        \HRule \\[0.4cm]
        { \huge \bfseries LINMA2470 Stochastic Modelling \\[0.4cm] }
    
        \HRule \\[1.5cm]
        \textsc{\LARGE Simon Desmidt}\\[1cm]
        \vfill
        \vspace{2cm}
        {\large Academic year 2024-2025 - Q2}
        \vspace{0.4cm}
         
        \includegraphics[width=0.15\textwidth]{img/epl.png}
        
        UCLouvain\\
    
    \end{center}
    \end{sffamily}
\end{titlepage}

\setcounter{tocdepth}{1}
\tableofcontents
\chapter{Reminders}
\section{General properties of probability}
\begin{itemize}
  \item $P[A\cup B]=P[A]+P[B]-P[1\cap B]$;
  \item $P[A|B] = \frac{P[A\cap B]}{P[B]} = \frac{P[AB]}{P[B]}$;
  \item $A$ and $B$ are independent iff $P[AB]=P[A]P[B]\Longrightarrow P[A|B]=P[A]$;
  \item $P[X\le x]=F_X(x)$ is the distribution function, i.e. a monotone increasing function of $x$ going from 0 to 1 when $x$ goes from $-\infty$ to $+\infty$.
  \item Its derivative is the density function $f_X(x)$ such that $f_X(x)\delta \approx P[x\le X\le x+\delta]$ for an infinitesimal $\delta$.
  \item A random variable $X$ is said to be memoryless if $\forall t,x>0$, $P[X>t+x|X>t]=P[X>x]$.
  \item Markov inequality (for a nonnegative random variable): $P[Y\ge y] \le \frac{\E[Y]}{y}$;
  \item Chebyshev inequality: $P[|Z-\E[Z]| \ge \varepsilon] \le \frac{\sigma_Z^2}{\varepsilon^2}$;
\end{itemize}
\section{Expectation and variance}
\begin{itemize}
  \item For a discrete random variable, $\E[X]= \sum_{n=-\infty}^\infty nP[X=n]$;
  \item For a continuous random variable, $\E[X]=\int_{-\infty}^\infty xf_X(x)dx$;
  \item $\E[X] = \int_0^\infty (1-F_X(x))dx$.
  \item $Var[X] = \sigma_X^2 = \E[(X-\E[X])^2] = \E[X^2] - \E[X]^2$;
\end{itemize}
\section{Law of large numbers}
Let $X_1,\dots,X_n$ be a series of independent and uniformly distributed (IID) random variables with expectation $\bar X$ and finite variance $\sigma_X^2$. Let $S_n = X_1+\dots +X_n$. Then, 
\begin{itemize}
  \item Weak version:
\end{itemize}
\begin{equation}
  \lim_{n\to \infty} P\left[|\frac{S_n}{n}-\bar X|\ge \varepsilon\right] = 0
\end{equation}
\begin{itemize}
  \item Strong version:
\end{itemize}
\begin{equation}
  \lim_{n\to \infty} P\left[\sup_{m\ge n}\left(\frac{S_m}{m}-\bar X\right)>\varepsilon\right] = 0\Longleftrightarrow \lim_{n\to \infty}\frac{S_n}{n}=X\qquad \text{with probability 1}
\end{equation}
\section{Central limit theorem}
\begin{equation}
  \lim_{n\to \infty} P\left[\frac{S_n-n\bar X}{\sqrt{n}\sigma}\le y\right] = \int_{-\infty}^y \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}}dx
\end{equation}
\section{Exponential distribution}
\begin{itemize}
  \item $f_X(x)=\lambda e^{-\lambda x}$, for $x\ge 0$;
  \item $F_X(x)=1-e^{-\lambda x}$, for $x\ge 0$;
  \item $\E[X]=1/\lambda$.
  \item [$\rightarrow$] Note: the exponential distribution is memoryless.
\end{itemize}
\chapter{Poisson Processes}
A Poisson process $N(t)$ counts the number of arrivals with exponentially distributed inter-arrival times. 
\begin{equation}
  S_n = \sum_{i=1}^n X_i \qquad \qquad X_i \sim \exp(\lambda)
\end{equation}
$\forall n,t$, we have the relation $\{S_n\le t\}=\{N(t)\ge n\}$, where $S_n$ is a random variable telling at which time the $n$-th occurence appears.\\
\begin{itemize}
  \item [$\rightarrow$] Note: a Poisson process is memoryless: $P[Z_1>x]=e^{-\lambda x}$, with $Z_1$ be the duration of the time interval from $t$ until the first arrival after $t$.
\end{itemize}
For a Poisson process of rate $\lambda$, and any given $t>0$, the length of the interval from $t$ until the first arrival after $t$ is an exponentially distributed random variable. This random variable is idenpendent of both $N(t)$ and of the $N(t)$ arrival epochs before time $t$. It is also independent of $N(\tau)$, $\forall \tau \le t$.\\

Let us consider the process after $Z_1$, $Z_m$, the time until the $m$-th arrival after time $t$. It is independent of $N(t)$ and of the entier previous history of the process.\\
Let us denote $\tilde N(t,t') = N(t')-N(t)$. 
\begin{itemize}
  \item Stationary increments property: It has the same distribution as $N(t'-t)$, $\forall t'\ge t$ (stationary increments property);
  \item Independent increments property: For any sequence of times $0<t_1<\dots<t_k$, the set $\{N(t_1), \tilde N(t_1,t_2), \dots,\tilde N (t_{k-1}, t_k)\}$ is a set of independent random variables.
\end{itemize}
From the memoryless property, here is another definition of a Poisson process: \\
\begin{itemize}
  \item A Poisson process is a counting process that has the stationay and independent increment properties and such that 
\end{itemize}
\begin{equation}
  \begin{aligned}
    P[\tilde N(t, t+\delta)=0] &= 1-\lambda \delta +o(\delta)\\  P[\tilde N(t, t+\delta)=1] &= \lambda \delta +o(\delta)\\
    P[\tilde N(t, t+\delta)\ge2] &= o(\delta)
  \end{aligned}
\end{equation}
\section{\texorpdfstring{Distribution of $N(t)$}{Distribution of }}
$S_n$ is the sum $n$ IID random variables and $f_{S_n}$ is the convolution of $n$ times $f_X$:
\begin{equation}
  f_{S_n}(t) = \frac{\lambda^n t^{n}e^{-\lambda t}}{(n-1)!}
\end{equation}
From this, 
\begin{equation}\label{eq:poisson_distrib}
  P[N(t)=n-1] = \frac{(\lambda t)^{n}e^{-\lambda t}}{(n)!}
\end{equation}
and finally, 
\begin{equation}
  \E[N(t)] = \lambda t\qquad Var[N(t)] = \lambda t
\end{equation}
From equation \eqref{eq:poisson_distrib}, the Poisson process verifies the following probability conditions:
\begin{itemize}
  \item $P[\tilde N(t,t+\delta)=0]=1-\lambda \delta +o(\delta)$;
  \item $P[\tilde N(t,t+\delta)=1]=\lambda \delta +o(\delta)$;
  \item $P[\tilde N(t,t+\delta)\ge2]=o(\delta)$;
\end{itemize}
where we use a first-order approximation of the exponential term, with $o(\delta)$ its residual. As $o(\delta)$ is negligible, we can approximate the Poisson process as a Bernoulli process. 
\subsection{Combining Poisson processes}
Let $N_1(t)$ and $N_2(t)$ be tow independent Poisson processes. Let the process $N(t)=N_1(t)+N_2(t)$. We can show using the three properties above that $N(t)$ is a Poisson process with rate $\lambda_1+\lambda_2$.
\subsection{Subdividing a Poisson process}
Let $N(t)$ be a Poisson process with rate $\lambda$. We split the arrivals in 2 subprocesses $N_1(t)$ and $N_2(t)$. Each arrival of $N(t)$ is sent to $N_1(t)$ with probability $p$ and to $N_2(t)$ with probability $(1-p)$, each split being independent from all others. \\
Then, the resulting processes $N_1(t)$ and $N_2(t)$ are two independent Poisson processes with respective rate $p\lambda$ and $(1-p)\lambda$.
\subsection{Conditional arrival distribution}
The density probability function when we have $n$ Poisson processes, under the condition that $N(t)=n$, is
\begin{equation}
  f(s_1,\dots,s_n|N(t)=n)=\frac{n!}{t^n}
\end{equation}
From the previous results, we can compute that 
\begin{equation}
  P[S_1>\tau|N(t)=n]=\left(\frac{t-\tau}{t}\right)^n
\end{equation}
and the expectation is 
\begin{equation}
  E[S_1|N(t)=n] = \frac{t}{n+1}
\end{equation}
And from this, we derive that 
\begin{equation}
  P[X_i>\tau|N(t)=n] = \left(\frac{t-\tau}{t}\right)^n
\end{equation}
with expectation 
\begin{equation}
  E[X_i] = \frac{t}{n+1}
\end{equation}
And thus the density function is 
\begin{equation}
  f_{S_i}(x|N(t)=n) = \frac{x^{i-1}(t-x)^{n-i}n!}{t^n (n-i)!(i-1)!}
\end{equation}
\section{Non-homogenous Poisson processes}
A non-homogenous Poisson rocess $N(t)$ is a counting process with increments that are independent but not stationary, with
\begin{itemize}
  \item $P[\tilde N(t,t+\delta)=0]=1-\lambda (t)\delta + o(\delta)$;
  \item $P[\tilde N(t,t+\delta)=1]=\lambda (t)\delta + o(\delta)$;
  \item $P[\tilde N(t,t+\delta)\ge2]=o(\delta)$;
\end{itemize}
where $\tilde N(t,t+\delta)=N(t+\delta)-N(t)$. The time-varying arrival rate $\lambda(t)$ is assumed to be continuous and stricly positive.
\section{Bernoulli process approximation}
We can approximate the non-homogenous Poisson process with a Bernoulli process where the time is partitioned into increments of lengths inversely proportional to $\lambda (t)$ (i.e. using a nonlinear time scale).
\begin{itemize}
  \item $P[\tilde N(t,t+\epsilon/\lambda(t))=0]=1-\epsilon + o(\epsilon)$;
  \item $P[\tilde N(t,t+\epsilon/\lambda(t))=1]=\epsilon + o(\epsilon)$;
  \item $P[\tilde N(t,t+\epsilon/\lambda(t))\ge2]=o(\epsilon)$;
\end{itemize}
Letting $\epsilon$ tend to zero, we obtain
\begin{equation}
  P[N(t)=n]=\frac{(m(t))^ne^{-m(t)}}{n!}\qquad P[\tilde N(t,t')=n]=\frac{(m(t,t'))^n e^{-m(t,t')}}{n!}
\end{equation}
with 
\begin{equation}
  m(t)=\int_0^t \lambda(\tau)d\tau \qquad m(t,t')=\int_t^{t'} \lambda(\tau)d\tau 
\end{equation}
\section{Classification of queueing systems}
\begin{itemize}
  \item We note $A/B/k$ where $A$ is the type of distribution for the arrival process, $B$ for the service time and $k$ the number of servers.
\end{itemize}
We suppose that the arrivals wait in a single queue.
Commonly used letters are 
\begin{itemize}
  \item M: exponential distribution (for A) or Poisson process (for B);
  \item D: deterministic time intervals;
  \item E: Erlang distribution;
  \item G: general distribution.
\end{itemize}
\chapter{Renewal Processes}
A renewal process is a counting process with IID interarrival intervals. We note $X_i$ the interval between arrivals, $\bar X=\E[X]$ is supposed to be finite with probability $P[X_i]>0=1$\footnote{A probability of 1 means that the opposite can happen, but is so rare that the probability is 0.}, $\sigma$ can be finite, and we denote $S_n=\sum_{i=1} X_i$ the time of the $n$-th arrival.\\
\section{Strong law of large numbers}
Let $\{N(t);t\ge 0\}$ be a renewal process, then 
\begin{equation}
  \lim_{t\to \infty} N(t) = \infty \qquad \qquad \lim_{t\to \infty} \E[N(t)] = \infty 
\end{equation}
This implies that 
\begin{equation}
  \lim_{t\to \infty} \frac{N(t)}{t} = \frac{1}{\bar X} \text{ with probability 1}
\end{equation}
\section{Central limit theorem}
If the interarrival intervals of the renewal process $N(t)$ have a finite standard deviation, then from the CLT for IID random variables, we have 
\begin{equation}
  \lim_{t\to \infty} P\left[\frac{S_n-n\bar X}{\sqrt{n}\sigma}\le \alpha\right] = \Phi(\alpha)
\end{equation}
\textcolor{red}{What is $\Phi(\alpha)$?}\\
and 
\begin{equation}
  \lim_{t\to \infty} P\left[\frac{N(t)-t/\bar X}{\sigma \bar X^{-3/2}\sqrt{t}}<\alpha \right] = \Phi(\alpha)
\end{equation}
\begin{itemize}
  \item [$\to$] Note: The reliability of the observed mean of successive results that are supposed to be IID depends a lot on the rule used to decide when we stop repeating the experiment.
\end{itemize}
\section{Stopping time}
Let $N$ be the rv corresponding to the total number of experiments observed. Let $I_n$ be a series of rv being the indicator fucnction of $\{N\ge n\}$:
\begin{equation}
  I_n = \begin{cases}
    1 & \text{if the $n$-th experiment is observed}\\
    0 & \text{otherwise}
  \end{cases}
\end{equation}
$N$ is a stopping time if $I_n$ depends only on $X_1,\dots,X_{n-1}$. This means that stopping at 3pm, for example, is not a stopping time, because it can depend on $X_n$, depending if the $n$-th arrival is before or after 3pm.\\
\subsection{Wald's inequality}
Let $N$ be a stopping time for $\{X_n;n\ge 1\}$. Then, $\E[S_N]=\E[N]\bar X$.
\section{Blackwell's renewal theorem}
\subsection{Arithmetic distribution}
If interarrival intervals can only have a length that is a multiple of some real number $d$, the interarrival distribution will be called an arithmetic distribution, and $d$ the span of the distribution. 
\subsection{Blackwell's inequality}
If the interarrival distribution of a renewal process $N(t)$ is not arithmetic, then 
\begin{equation}
  \lim_{t\to\infty } \left(m(t+\delta)-m(t)\right) = \frac{\delta}{\bar X} \qquad \forall \delta 
\end{equation}
If the interarrival distribution is arithmetic with span $d$, then 
\begin{equation}
  \lim_{t\to\infty } \left(m(t+nd)-m(t)\right) = \frac{nd}{\bar X} \qquad \forall n\ge 1
\end{equation}
\subsection{Relationship with a Poisson process}
The sum of many renewal processes tends to a Poisson process: for a non-arithmetic renewal process with $P[X_i=0]=0$, we have 
\begin{equation}
  \begin{aligned}
    &\lim_{t\to\infty} P[N(t+\delta)-N(t)=0]=1-\delta/\bar X + o(\delta)\\
    &\lim_{t\to\infty} P[N(t+\delta)-N(t)=1]=\delta/\bar X + o(\delta)\\
	&\lim_{t\to\infty} P[N(t+\delta)-N(t)\ge 2]=o(\delta)
  \end{aligned}
\end{equation}
The increments are asymptotically stationary, but not independent. 
|section{Renewal reward process}
Along to the renewal process $N(t)$, we can add a reward function $R(t)$. It models the rate at which the process is accumulating a reward or cost. It can however only depend on the current renewal but not the previous ones. \\
Let $Y(y)$ be the residual life at time $t$ for the current renewal:
\begin{equation}
	R(t)=Y(t)=S_{N(t)+1}-t
\end{equation}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{img/reward.png}
\end{figure}
The time average residual life is $\frac{1}{t}\int_0^t Y(\tau)d\tau$.\\
From the definition of $Y(t)$, we can calculate that 
\begin{equation}
	\lim_{t\to \infty} \frac{1}{t}\int_0^t Y(\tau)d\tau = \frac{\E[X^2]}{2\E[X]} = \frac{1}{2}\E[X] + \frac{Var(X)}{\E[X]} > \frac{1}{2}\E[X]\text{ with probability 1}
\end{equation}
\subsection{Time average age}
Let $Z(t)$ be the age of the current renewal at time $t$: $R(t)=Z(t)=t-S_{N(t)}$. The time average age is $\lim_{t\to \infty} \frac{1}{t}\int_0^t Z(\tau)d\tau= \frac{\E[X^2]}{2\E[X]}$.\\
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{img/age.png}
\end{figure}
\subsection{Time average duration}
Let $X(t)$ be the duration of the renewal containing time $t$: $R(t)=X(t)=S_{N(t)+1}-S_{N(t)}$. The time average duration is $\lim_{t\to \infty} \frac{1}{t}\int_0^t X(\tau)d\tau = \frac{\E[X^2]}{\E[X]}$.\\
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{img/duration.png}
\end{figure}
|subsection{General renewal reward functions}
Let $R(t)$ be a reward function for a renewal process with expected inter-renewal times $\bar X<\infty$, then with probability 1, 
\begin{equation}
	\lim_{t\to \infty} \frac{1}{t}\int_0^t R(\tau)d\tau = \frac{\E[R_n]}{\E[X]}
\end{equation}
where $R_n$ is defined as 
\begin{equation}
	R_n = \int_{S_n}^{S_{n+1}} R(\tau)d\tau
\end{equation}
\subsection{Distribution of residual life}
We are interested in the fraction of time that $Y(t)\le y$: 
\begin{equation}
	R(t)= I\{Y(t)\le y\} \qquad \qquad R_n = \min\{y, X_n\}
\end{equation}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{img/residual.png}
\end{figure}
And we can calculate that 
\begin{equation}
	\begin{aligned}
		\E[R_n]= \int_0^y P[X>x]dx \\
		F_Y(y) = \frac{1}{\E[X]}\int_0^y P[X>x]dx
	\end{aligned}
\end{equation}
\subsection{Key theorem}
Let $N(t)$ be a non-arithmetic renewal process, let $R(z,x)\ge 0$ be such that $r(z)=\int_{x=z}^\infty R(z,x)dF_X(x)$ is directly Rieman integrable. Then, 
\begin{equation}
	\lim_{t\to \infty} \E[R(t)] = \frac{\E[R_n]}{\bar X}
\end{equation}
\section{Little's Law}
Let a queueing system be such that 
\begin{itemize}
	\item $A(t)$ is the number of arrivals between $0$ and $t$;
	\item $D(t)$ is the number of departures between $0$ and $t$;
	\item $L(t)=A(t)-D(t)$ is the number of customers in the system at time $t$;
	\item $w_i$ the time the $i^{th}$ customer spends in the system;
	\item $N(t)$ is the renewal process counting the number of busy periods of the system (each time a customer arrives when the system is empty).
\end{itemize}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{img/queue.png}
\end{figure}
Let us use $L(t)$ as a reward function for the renewal process $N(t)$. This implies 
\begin{equation}
	\begin{aligned}
		\sum_{n=1}^{N(t)} R_n \le \int_0^t L(\tau)d\tau &\le \sum_{i=1}^{A(t)} w_i \le \sum_{n=1}^{N(t)+1} R_n\\
		\lim_{t\to \infty} \frac{1}{t}\int_0^t L(\tau)d\tau &= \frac{\E[R_n]}{\E[X]}	
	\end{aligned}
\end{equation}
Putting all this together, we can show that $\bar L=\lim_{t\to \infty}\frac{1}{t}\int_0^tL(\tau)d\tau = \bar  W\lambda$.
\subsection{M/G/1 queue}
Let $R(t)$ be the remaining time for the customer being served. Let $U(t)$ be the time an arrival at time $t$ would have to wait before being served. Let $L_q(t)$ be the number of customers in queue at time $t$, independent of the $Z_i$. We define 
\begin{equation}
	U(t)=\sum_{i=1}^{L_q(t)}Z_i + R(t) \Longrightarrow \E[U(t)]=\E[L_q(t)]\E[Z]+\E[R(t)]
\end{equation}
We can show that 
\begin{equation}
	\int_0^{S_N(t)} R(\tau)d\tau \le \int_0^{S_N(t)+1} R(\tau)d\tau
\end{equation}
And from Little's Law,
\begin{equation}
	\lim_{t\to \infty} \E[L_q(t)] = \lambda \bar W_q \Longrightarrow \lim_{t\to \infty} \E[U(t)] = \lambda \bar W_q \E[Z] + \lambda \frac{\E[Z^2]}{2} 
\end{equation}
Poisson arrival process implies that arrivals occur with identical probability at any moment, this implies independence with $U(t)$. Hence $\E[W_q(t)]=\E[U(t)]$. Hence $\bar W_q = \lambda \bar W_q\E[Z] + \lambda \frac{\E[Z^2]}{2}$. And we can isolate $\bar W_q$:
\begin{equation}
	\bar W_q = \frac{\lambda (\E[Z]^2+\sigma^2)}{2(1-\lambda \E[Z])}
\end{equation}
And we remember $\bar W = \bar W_q + \E[Z]$.\\
\end{document}