\documentclass[12pt, openany]{report}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage[a4paper,left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage[english]{babel}
\usepackage{libertine}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{float}
\usepackage{enumitem}
\usepackage{pythonhighlight}
\usepackage[]{titletoc}
\usepackage{empheq}
\usepackage{titlesec}
\usepackage{mathpazo}
\usepackage{xfrac}
\usepackage{textcomp}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{tabularray}
\usepackage{subcaption}
\usepackage{cancel}
\usepackage[bottom]{footmisc}
\usepackage{pdfpages}
\usepackage{tabularx}
\usepackage{amsthm}
\usepackage[skins]{tcolorbox}
\titleformat{\chapter}[display]
  {\normalfont\bfseries}{}{0pt}{\Huge}
\usepackage{hyperref}
\newcommand{\hsp}{\hspace{20pt}}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathcal{C}}
\renewcommand{\O}{\mathcal{O}}
\theoremstyle{definition}
\newtheorem{thm}{Theorem}[chapter]
\newtheorem{definition}[thm]{Definition}
\newtheorem{lem}[thm]{Lemma}

\hbadness=100000
\begin{document}
\begin{titlepage}
    \begin{sffamily}
    \begin{center}
        \includegraphics[scale=0.3]{img/page_de_garde.png} \\[1cm]
        \HRule \\[0.4cm]
        { \huge \bfseries LINMA2460 Nonlinear Programming \\[0.4cm] }
    
        \HRule \\[1.5cm]
        \textsc{\LARGE Simon Desmidt\\ Issambre L'Hermite Dumont}\\[1cm]
        \vfill
        \vspace{2cm}
        {\large Academic year 2024-2025 - Q2}
        \vspace{0.4cm}
         
        \includegraphics[width=0.15\textwidth]{img/epl.png}
        
        UCLouvain\\
    
    \end{center}
    \end{sffamily}
\end{titlepage}

\setcounter{tocdepth}{1}
\tableofcontents
\chapter{Definitions, notations and random properties}
\begin{itemize}
	\item The Taylor expansion of order $p$ of the function $f$ around $x_k$ and evaluated at $y$ is: 
	\begin{equation}
		T_p(y;x_k) = f(x_k) + \sum_{i=1}^{p} \frac{1}{i!} D^i f(x_k) (y-x_k)^i
	\end{equation}
	\item We can thus define the gradient w.r.t. $y$ of the Taylor expansion of order $p$ of $f$ around $x_k$ and evaluated at $x_{k+1}$:
	\begin{equation}
		\nabla_y T_p(x_{k+1};x_k) = \left. \nabla_y T_p(y;x_k) \right|_{y=x_{k+1}}
	\end{equation}
	\item An oracle is a "black box" that gives information about the derivatives based on $x$. The general form of an oracle is:
	\begin{equation}
		\text{p-order oracle:} \quad x \mapsto \{D^if(x)\}_{i=0}^p
	\end{equation}
	And so we have the following simple oracles examples:
	\begin{equation}
		\begin{aligned}
			\text{Zero}^{th} \text{-order oracle:} \quad &x \mapsto \{f(x)\} \\
			\text{First-order oracle:} \quad &x \mapsto \{ f(x), \nabla f(x) \} \\
			\text{Second-order oracle:} \quad &x \mapsto \{ f(x), \nabla f(x), \nabla^2 f(x) \}
		\end{aligned}
	\end{equation}
    \item $\C^p_L(\R^n)$: Class of functions p-times continuously differentiable with L-Lipschitz continuous p-order derivative, i.e. $\| D^pf(x) - D^pf(y) \| \leq L \|x-y \|$, $\forall x,y\in \R^n$. And so we have the following simple classes of problems:
    \begin{itemize}
		\item $\C^1_L(\R^n)$: Class of continuously differentiable functions with L-Lipschitz gradient;
    \item $\C^2_L(\R^n)$: Class of continuously differentiable functions with L-Lipschitz hessian.
	\end{itemize}
	\item  pth-order method (generalization of GM):
	\begin{equation}
		x_{k+1} = \arg\min_{y\in \R^n} \Omega_{x_k,y,p}(y) \equiv T_{x_k,p}(y) + \frac{M}{(p+1)!}\|y-x_k\|^{p+1}
	\end{equation}
\end{itemize}
\section{Properties}
\begin{itemize}
	\item For a function $f\in \C^1(\Omega)$ and $\Omega$ is bounded, the following holds: $\lVert \nabla f(x)\rVert \le L$ for all $x\in \Omega$ for some $L\ge 0$.
	\item By the mean value theorem, for a continuously differentiable function $f$, $\forall x,y\in \Omega,\: \exists \ z\in \Omega:$ $f(y)-f(x) = \langle \nabla f(z),y-x\rangle$.
	\item For a matrix $A$ and a scalar $b$, $\lVert A\rVert \le b\Longrightarrow |\lambda (A)|\le b \Longrightarrow |A| \preceq bI_n$, where the absolute value of the matrix is taken component wise. 
\end{itemize}
\section{Complexity table}
\begin{center}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		Method & Lipschitz & $\nabla f$ & $\nabla^2 f$ & \dots & $\nabla^p f$ \\
		\hline
		Zero order & & $O(n\varepsilon^{-2})$ & & & \\
		\hline
		First order & $p=1$ & $O(\varepsilon^{-2})$ & & & \\
		\hline
		Second order & $p=2$ & \textcolor{red}{\cancel{X}} & $O(\varepsilon^{-3/2})$ & & \\
		\hline
		\vdots & & \textcolor{red}{\cancel{X}} & \textcolor{red}{\cancel{X}} & $\ddots$ & \\
		\hline
		p order & & \textcolor{red}{\cancel{X}} & \textcolor{red}{\cancel{X}} & \textcolor{red}{\cancel{X}} & $O(\varepsilon^{-\frac{p+1}{p}})$ \\
		\hline
	\end{tabular}
\end{center}
\chapter{TODO}\label{chap:right_oracle}
We can generalise the property of a L-Lipschitz function to $f\in \C_L^p(\R^n)$. For $p=1$, we had 
\begin{equation}
	f(y)\le f(x_k) + \langle \nabla f(x_k),y-x_k\rangle + \frac{L}{2}\lVert y-x_k\rVert^2 \qquad \forall y\in \R^n 
\end{equation}
For a general value of $p$, it becomes 
\begin{equation}
	f(y) \le T_p(y;x_k) + \frac{L}{(p+1)!}\lVert y-x_k\rVert^{p+1} \qquad \forall y\in \R^n 
\end{equation}
Using this, \textcolor{blue}{we need a $p$-th order oracle} for the method to work. \\
To solve $\min_{x\in \R^n} f(x)$, we can use the iteration 
\begin{equation}\label{eq:argmin}
	x_{k+1} = \arg\min_{y\in \R^n} T_p(y;x_k)+\frac{M}{(p+1)!}\lVert y-x_k\rVert^{p+1}
\end{equation}
where the constant $M$ is an approximation of the Lipschitz constant $L$. \textcolor{blue}{Assuming $f\in \C^p_L(\R^n)$}, we have 
\begin{equation}
	\begin{aligned}
		f(x_{k+1}) &\le T_p(x_{k+1};x_k) + \frac{L}{(p+1)!}\lVert x_{k+1}-x_k\rVert^{p+1}\\
		&= \underbrace{T_p(x_{k+1};x_k) + \frac{M}{(p+1)!}\lVert x_{k+1}-x_k\rVert^{p+1}}_{\le f(x_k)} + \frac{(L-M)}{(p+1)!}\lVert x_{k+1}-x_k\rVert^{p+1}
	\end{aligned}
\end{equation}
where the inequality $\le f(x_k)$ is due to the decrease of $f$ and equation \eqref{eq:argmin}. 
\textcolor{blue}{Suppose that $M>2L$}. After some algebraic manipulations, we get
\begin{equation}\label{eq:bound}
	f(x_k)-f(x_{k+1}) \ge \frac{L}{(p+1)!}\lVert x_{k+1}-x_k\rVert^{p+1}
\end{equation}
On the other hand, using the triangular inequality, 
\begin{equation}
	\begin{aligned}
		\lVert \nabla f(x_{k+1})\rVert & \le \lVert \nabla f(x_{k+1})-\nabla_y T_p(x_{k+1};x_k)\lVert\\
		& + \underbrace{\left \lVert \nabla_y T_p(x_{k+1};x_k)+\nabla \left.\left(\frac{M}{(p+1)!} \lVert \cdot -x_k\rVert^{p+1}\right)\right|_{y=x_{k+1}}\right \rVert}_{=0}	\\
		& + \left\lVert \nabla \left.\left(\frac{M}{(p+1)!}\lVert \cdot -x_k\rVert^{p+1}\right)\right|_{y=x_{k+1}}\right \rVert \\
		& \le \frac{L}{p!}\lVert x_{k+1}-x_k\rVert^p + \frac{M}{p!} \lVert x_{k+1}-x_k\rVert^p
	\end{aligned}
\end{equation}
\begin{equation}\label{eq:bound_on_x}
	\Longrightarrow \lVert x_{k+1}-x_k\rVert \ge \left(\frac{p!}{L+M}\right)^{1/p}\lVert \nabla f(x_{k+1})\rVert^{1/p}
\end{equation}
Combining equations \eqref{eq:bound} and \eqref{eq:bound_on_x}, 
\begin{equation}\label{eq:next_bound}
	f(x_k)-f(x_{k+1}) \ge \underbrace{\frac{L}{(p+1)!}\left(\frac{p!}{L+M}\right)^{\frac{p+1}{p}}}_{\eqcolon C(L)} \lVert \nabla f(x_{k+1})\rVert^{\frac{p+1}{p}}
\end{equation}
Let $T(\varepsilon) = \inf \{k\in \mathbb{N}:\: \lVert \nabla f(x_k)\rVert\le \varepsilon\}$. \textcolor{blue}{Assume that $T(\varepsilon)\ge 2$ and $f(x)\ge f_{low}$ $\forall x\in \R^n$.} Summing up \eqref{eq:next_bound} for $k=0,\dots, T(\varepsilon)-2$,
\begin{equation}
	\begin{aligned}
		f(x_0)-f_{low} &\ge f(x_0)-f(x_{T(\varepsilon)-1}) = \sum_{k=0}^{T(\varepsilon)-2} f(x_k)-f(x_{k+1}) \\ 
		& \ge (T(\varepsilon)-1)C(L) \varepsilon^{\frac{p+1}{p}}\\
		\Longrightarrow T(\varepsilon) &\le 1 + \frac{f(x_0)-f_{low}}{C(L)} \varepsilon^{-\frac{p+1}{p}} \equiv \O\left(\varepsilon^{-\frac{p+1}{p}}\right)
	\end{aligned}
\end{equation}
\chapter{Gradient descent without gradient}\label{chap:}
For this problem consider an adversarial attack on block-based image classifier. We have a machine learning model that given an image $a\in\R^p$ it returns $c(a)\in\R^m$, where $c_j(a) \in [0,1]$ is the probability of image $a$ to be in class $j$. The classifier prediction is: $j(a) = \arg\max_{j\in [1,\dots,m]} c_j(a)$.\\
\textcolor{red}{TODO - Add mise en situation ou pas?}
\newline

Given $x_k$ let us decide:
\begin{equation}
	x_{k+1} = x_k - \frac{1}{\sigma} g_{h_k}(x_k) \quad \quad \quad h_k > 0, \: \sigma > 0
\end{equation}
where $g_{h_k}(x_k) \in \R^n$ is given by:
\begin{equation}
	[g_{h_k}(x_k)]_j = \frac{f(x_k+he_j)-f(x_k)}{h_k} \quad \quad \quad \forall j\in [1,\dots,m]
\end{equation}
\textcolor{blue}{Suppose that $f\in \C^1_L(\R^n)$}. Then,
\begin{equation}
	\|\nabla f(x_k) - g_{h_k}(x_k)\| \leq \frac{L \sqrt{n}}{2} h_k
\end{equation}
Thus 
\begin{equation}
	\begin{aligned}
		f(x_{k+1}) &\leq f(x_k) &&+ \langle \nabla f(x_k), x_{k+1}-x_k\rangle + \frac{L}{2}\lVert x_{k+1}-x_k\rVert^2\\
		&= f(x_k) &&+ \langle g_{h_k}(x_k), x_{k+1}-x_k\rangle + \frac{\sigma}{2} \|x_{k+1}-x_k\|\\
		& &&+ \langle \nabla f(x_k) - g_{h_k}(x_k), x_{k+1}-x_k\rangle + \frac{(L-\sigma)}{2}\lVert x_{k+1}-x_k\rVert^2\\
		&\leq f(x_k) &&- \frac{1}{\sigma} \|g_{h_k}(x_k)\|^2 + \frac{1}{2\sigma} \|g_{h_k}(x_k)\|^2 \\
		& &&+ \| \nabla f(x_k) - g_{h_k}(x_k)\| \frac{1}{\sigma} \|g_{h_k}(x_k)\| + \frac{(L-\sigma)}{2\sigma^2} \|g_{h_k}\|^2\\
		& \leq f(x_k) &&- \frac{1}{2\sigma} \|g_{h_k}(x_k)\|^2 + \frac{L \sqrt{n}}{2} h_k \frac{1}{\sigma} \|g_{h_k}\| + \frac{(L-\sigma)}{2\sigma^2} \|g_{h_k}\|^2\\
		&\leq f(x_k) &&- \frac{1}{2\sigma} \|g_{h_k}(x_k)\|^2 + \frac{L}{2} \left( \frac{nh_k^2}{2}+ \frac{1}{2 \sigma} \|g_{h_k}(x_k)\|^2 \right) + \frac{(L-\sigma)}{2\sigma^2} \|g_{h_k}\|^2\\
		&= f(x_k) &&- \left( \frac{2\sigma - L -2(L - \sigma)}{4 \sigma^2} \right) \|g_{h_k}(x_k)\|^2 + \frac{Ln}{4} h_k^2\\
		&= f(x_k) &&- \frac{(4\sigma -3L)}{4 \sigma} \|g_{h_k}(x_k)\|^2  + \frac{Ln}{4} h_k^2
	\end{aligned}
\end{equation}
\begin{equation}
	\Longrightarrow \frac{(4\sigma -3L)}{4 \sigma} \|g_{h_k}(x_k)\|^2 \leq f(x_k) - f(x_{k+1}) + \frac{Ln}{4} h_k^2
\end{equation}
\textcolor{blue}{If $\sigma \gg L$, then}
\begin{equation}
	\frac{1}{4\sigma} \|g_{h_k}(x_k)\|^2 \leq f(x_k) - f(x_{k+1}) + \frac{\sigma n}{4} h_k^2
\end{equation}
On the other hand, we have
\begin{equation}
	\begin{aligned}
		\|\nabla f(x_k)\| &\leq \|\nabla f(x_k) - g_{h_k}(x_k)\| + \|g_{h_k}(x_k)\|\\
		&\leq \frac{L \sqrt{n}}{2} h_k + \|g_{h_k}(x_k)\|\\
	\end{aligned}
\end{equation}
Using trick \eqref{eq:sq2} in chapter \ref{chap:tricks},
\begin{equation}
	\begin{aligned}
		\Longrightarrow \|\nabla f(x_k)\|^2 &\leq 2 \left( \frac{L \sqrt{n}}{2} h_k \right)^2 + 2 \|g_{h_k}(x_k)\|^2\\
		&\leq \frac{L^2 n}{2} h_k^2 + 2 \|g_{h_k}(x_k)\|^2 
	\end{aligned}
\end{equation}
\begin{equation}
	\Longrightarrow \frac{1}{8 \sigma} \|\nabla f(x_k)\|^2 \leq \frac{L^2n}{16\sigma} h_k^2 + \frac{1}{4\sigma} \|g_{h_k}(x_k)\|^2
\end{equation}
\begin{equation}\label{eq:brigitte}
	\Longrightarrow \frac{1}{8 \sigma} \|\nabla f(x_k)\|^2 \leq f(x_k) - f(x_{k+1}) + \frac{\sigma n}{4} h_k^2 + \frac{\sigma n}{16}h_k^2
\end{equation}
Let $T(\varepsilon) = \inf \{k\in \mathbb{N}: \|\nabla f(x_k)\| \leq \varepsilon \}$, \textcolor{blue}{with $f(x)$ bounded below by $f_{low}$}, summing up \eqref{eq:brigitte} for $k=0,\dots, T(\varepsilon)-1$:
\begin{equation}
	\frac{T(\varepsilon)}{8 \sigma} \varepsilon^2 \leq f(x_0) - f_{low} + \frac{5 \sigma n}{4} \sum_{k=0}^{T(\varepsilon)-1} h_k^2
\end{equation}
\textcolor{blue}{If $\{h_k^2\}$ is summable}
\begin{equation}
	\Longrightarrow T(\varepsilon) \leq 8 \sigma \left( f(x_0) - f_{low} + \frac{5 \sigma n}{4} \sum_{k=0}^{T(\varepsilon)-1} h_k^2\right) \varepsilon^2 = \O(\varepsilon^2)
\end{equation}
In terms of call to the oracle, we have a complexity bound of $\O(n\varepsilon^2)$.

\chapter{Local rates of convergence}
\section{Linear rate of GM}
Let \textcolor{blue}{$f \in \C_M^{2,2}(\R^n)$}. Assume \textcolor{blue}{$f$ has a local minimizer $x^*$ such that 
\begin{equation}\label{eq:hessian_bound}
	\mu I_n \preceq \nabla^2 f(x^*) \preceq MI_n
\end{equation}}
Let $x_{k+1} = x_k - \frac{1}{L} \nabla f(x_k)$ for a given $x_0 \in \R^n$.\\
Notice that
\begin{equation}\label{eq:G_k}
	\begin{aligned}
		\nabla f(x_k) &= \nabla f(x_k) - \nabla f(x^*)\\
		&= \int_{0}^{1} \nabla^2 f(x^* + \tau(x_k-x^*)) (x_k-x^*) d\tau\\
		&= \int_{0}^{1} \nabla^2 f(x^* + \tau(x_k-x^*)) d\tau (x_k-x^*) \\
		&= G_k (x_k-x^*)
	\end{aligned}
\end{equation}
Then,
\begin{equation}
	\begin{aligned}
		\|x_{k+1} - x^*\| &= \|x_k - \frac{1}{L} \nabla f(x_k) - x^*\|\\
		&= \|(I_n-\frac{1}{L}G_k)(x_k-x^*)\|\\
		&\leq \|I_n - \frac{1}{L}G_k\| \|x_k-x^*\|
	\end{aligned}
\end{equation}
Since $f \in C_M^{2,2}(\R^n)$, we have $\|\nabla^2f(x^*+\tau(x_k-x^*))-\nabla^2f(x^*)\| \leq \tau M \|x_k -x^*\|$ and using this we get:
\begin{equation}
	|\langle \nabla^2f(x^*+\tau(x_k-x^*))-\nabla^2f(x^*) v, v\rangle| \leq \tau M \|x_k -x^*\| \|v\|^2 \quad \forall v \in \R^n
\end{equation}
Using the bound \eqref{eq:hessian_bound} and the previous inequality, we get:
\begin{equation*}
		\tau M \|x_k -x^*\| \|v\|^2 \leq |\langle \nabla^2f(x^*+\tau(x_k-x^*))-\nabla^2f(x^*) v, v\rangle| \leq \tau M \|x_k -x^*\| \|v\|^2\\
\end{equation*}
\begin{equation*}
	\nabla^2 f(x^*) - \tau M \|x_k -x^*\| I_n \preceq \nabla^2f(x^*+\tau(x_k-x^*)) \preceq \nabla^2 f(x^*) + \tau M \|x_k -x^*\|I_n 
\end{equation*}
\begin{equation*}
	(\mu - \tau M \|x_k -x^*\|)I_n \preceq \nabla^2f(x^*+\tau(x_k-x^*)) \preceq (L + \tau M \|x_k -x^*\|) I_n 
\end{equation*}
By the properties of the semi-definite matrices, and the trick \eqref{eq:vtrick}, we have:
\begin{equation}
	\begin{aligned}
		\int_{0}^{1} (\mu - \tau M \|x_k -x^*\|) \|v\|^2 d\tau &\leq \int_{0}^{1} \langle \nabla^2f(x^*+\tau(x_k-x^*)) v,v \rangle d\tau\\ 
		&\leq \int_{0}^{1} (L + \tau M \|x_k -x^*\|) \|v\|^2 d\tau \quad \forall v \in \R^n
	\end{aligned}
\end{equation}
By using $G_k$ and some constants, we get:
\begin{equation}
	-\frac{1}{L}(L+\frac{M}{2}\|x_k -x^*\|)I_n \preceq - \frac{1}{L} G_k \preceq -\frac{1}{L}(\mu - \frac{M}{2}\|x_k -x^*\|)I_n
\end{equation}
\begin{equation}
	\left(1-\frac{1}{L}(L+\frac{M}{2}\|x_k -x^*\|)\right)I_n \preceq I_n - \frac{1}{L}G_k \preceq \left(1-\frac{1}{L}(\mu - \frac{M}{2}\|x_k -x^*\|)\right)I_n
\end{equation}
And finally, we get:
\begin{equation}\label{eq:eq3cm3}
	\begin{aligned}
		\|I_n - \frac{1}{L}G_k\| &\leq \max\left\{\left|1-\frac{1}{L}(L + \frac{M}{2}\|x_k -x^*\|)\right|,\left|1-\frac{1}{L}(\mu - \frac{M}{2}\|x_k -x^*\|)\right|\right\}\\
		&= \max\left\{\frac{M}{2L}\|x_k -x^*\|, 1-\frac{\mu}{L}+\frac{M}{2L}\|x_k -x^*\|\right\}\\
		&= 1-\frac{\mu}{L}+\frac{M}{2L}\|x_k -x^*\|
	\end{aligned}
\end{equation}
\textcolor{blue}{Suppose that $\frac{M}{2L} \|x_k -x^*\| \leq \frac{\mu}{2L} \Longleftrightarrow \|x_k -x^*\| \leq \frac{\mu}{M}$}\\
Then, in \eqref{eq:eq3cm3}, we get:
\begin{equation}
	\|I_n - \frac{1}{L}G_k\| \leq 1-\frac{\mu}{2L} < 1
\end{equation}
And so, by \eqref{eq:G_k}
\begin{equation}\label{eq:bound_by_G_k}
	\|x_{k+1} - x^*\| \leq \|I_n - \frac{1}{L}G_k\| \|x_k -x^*\| < \|x_k -x^*\|
\end{equation}
If $\|x_0 - x^* \|< \frac{\mu}{M}$, it follows from the previous reasoning that:
\begin{equation}
	\|x_2 - x^*\| \leq  (1-\frac{\mu}{2L}) \|x_1-x^*\| \leq (1-\frac{\mu}{2L})^2 \|x_0-x^*\|\leq \frac{\mu}{M}
\end{equation} 
And so by induction, we can conclude that:
\begin{equation}\label{eq:bound_x_k_induction}
	\|x_k - x^*\| \leq \left(1-\frac{\mu}{2L}\right)^k \|x_0-x^*\| \quad \forall k \geq 0
\end{equation}
\begin{center}
	\color{red}\boxed{\textcolor{black}{\Rightarrow \text{Linear rate of convergence}}}\color{black}\\
\end{center}

Given $\varepsilon > 0$, let $T(\varepsilon) = \inf\{k\in \mathbb{N} : \|x_k -x^*\|\leq \varepsilon\}$. Then, if $T(\varepsilon) \geq 1$ and using \eqref{eq:bound_x_k_induction}, we get:
\begin{equation}
	\begin{aligned}
		\varepsilon < \|x_{T(\varepsilon)-1} - x^*\| &\leq \left(1-\frac{\mu}{2L}\right)^{T(\varepsilon)-1} \|x_0 - x^*\|\\
		\log \left(\frac{\varepsilon}{\|x_0 - x^*\|}\right) &\leq (T(\varepsilon)-1) \log \left(1-\frac{\mu}{2L}\right)\\
		T(\varepsilon)-1 &\leq \frac{\log \left(\frac{\varepsilon}{\|x_0 - x^*\|}\right)}{\log \left(1-\frac{\mu}{2L}\right)} = \frac{\log \left(\|x_0 - x^*\|\varepsilon^{-1}\right)}{|\log \left(1-\frac{\mu}{2L}\right)|}
	\end{aligned}
\end{equation}
\begin{center}
	\color{red}
	\boxed{\color{black} T(\varepsilon) \leq \mathcal{O}(\log(\varepsilon^{-1})) }
	\color{black}\\
\end{center}
\begin{itemize}
	\item [$\rightarrow$] Note: convexity was never assumed!
\end{itemize}

\section{Local quadratic convergence of Newton's method}
\textcolor{blue}{Let $f \in \C_M^{2,2}(\R^n)$. Assume $f$ has a local minimizer $x^*$ such that
\begin{equation}\label{eq:hessian_bound2}
	\mu I_n \preceq \nabla^2 f(x^*) \quad \mu > 0
\end{equation}}
Given $x_0 \in \R^n$, let:
\begin{equation}\label{eq:newton}
	x_{k+1} = x_k - \nabla^{-2} f(x_k) \nabla f(x_k)
\end{equation}
We have, by the previous equation and the definition of $G_k$ \eqref{eq:G_k}:
\begin{equation}\label{eq:bound_x_k+1}
	\begin{aligned}
		\|x_{k+1}-x^*\| &= \|x_k - \nabla^{-2} f(x_k) \nabla f(x_k) - x^*\|\\
		&= \|(x_k-x^*)-\nabla^{-2} f(x_k)G_k(x_k-x^*)\|\\
		&= \|\nabla^{-2} f(x_k) \left(\nabla^2f(x_k)-\int_{0}^{1}\nabla^2f(x^*+\tau(x_k-x^*))d\tau\right)(x_k-x^*)\|\\
		&= \|\nabla^{-2} f(x_k) \left( \int_{0}^{1} \nabla^2f(x_k)-\nabla^2f(x^*+\tau(x_k-x^*))d\tau\right)(x_k-x^*)\|\\
		&\leq \|\nabla^{-2} f(x_k)\| \left( \int_{0}^{1} \left\|\nabla^2f(x_k)-\nabla^2f(x^*+\tau(x_k-x^*))\right\|d\tau\right)\|x_k-x^*\|\\
		&\leq \|\nabla^{-2} f(x_k)\| \left( \int_{0}^{1} M(1-\tau) \|x_k-x^*\|d\tau\right)\|x_k-x^*\|\\
		&\leq \|\nabla^{-2} f(x_k)\|\|x_k-x^*\|^2\frac{M}{2}
	\end{aligned}
\end{equation}
Since $f\in \C^{2,2}_M(\R^n)$, we have
\begin{equation}
	\nabla^2 f(x^*+\tau(x_k-x^*)) - \nabla^2 f(x^*) \succeq \tau M \|x_k-x^*\|I_n
\end{equation}
\begin{equation}
	\begin{aligned}
		\nabla^2 f(x_k) &\succeq \nabla^2 f(x^*) - M \|x_k-x^*\|I_n\\
		&\succeq (\mu - M \|x_k-x^*\|)I_n\\
		\lambda_{\min}(\nabla^2 f(x_k)) &\geq \mu - M \|x_k-x^*\|
	\end{aligned}
\end{equation}
Suppose that $-M \|x_k-x^*\| \geq - \frac{\mu}{2} \Leftrightarrow \|x_k-x^*\| \leq \frac{\mu}{2M}$\\
Then,
\begin{equation}
	\begin{aligned}
		\lambda_{\min}(\nabla^2 f(x_k)) &\geq \frac{\mu}{2}\\
		\lambda_{\max}(\nabla^{-2} f(x_k)) &\leq \frac{2}{\mu}\\
		\Rightarrow \|\nabla^{-2} f(x_k)\| &\leq \frac{2}{\mu}
	\end{aligned}
\end{equation}
Therefore, by \eqref{eq:bound_x_k+1}, we conclude that:
\begin{equation}
	\begin{aligned}
		\|x_{k+1}-x^*\| &\leq \frac{M}{2}\|\nabla^{-2}f(x_k)\|\|x_k-x^*\|\\
		&\leq \frac{M}{\mu}\|x_k-x^*\|^2
	\end{aligned}
\end{equation}


\chapter{Tips and Tricks}\label{chap:tricks}
\begin{enumerate}
	\item Approximation of the max:
	\begin{equation}\label{eq:approx_max}
		\begin{aligned}
			\max\{z,0\} = \frac{z+|z|}{2} = \frac{z + \sqrt{z^2}}{2} \approx \frac{z + \sqrt{z^2 + \delta}}{2}
		\end{aligned}	
	\end{equation}
	\item \begin{equation}\label{eq:sq1}
		ab \leq \frac{a^2+b^2}{2}	
	\end{equation}
	\item \begin{equation}\label{eq:sq2}
		(a+b)^2 \leq 2a^2+2b^2
	\end{equation}
	\item V-trick:
	\begin{equation}\label{eq:vtrick}
		\langle xv,v\rangle \leq \|x\|\|v\|^2
	\end{equation}
\end{enumerate}
\end{document}