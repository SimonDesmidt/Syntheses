\documentclass[12pt, openany]{report}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage[a4paper,left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage[english]{babel}
\usepackage{libertine}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{float}
\usepackage{enumitem}
\usepackage{pythonhighlight}
\usepackage[]{titletoc}
\usepackage{empheq}
\usepackage{titlesec}
\usepackage{mathpazo}
\usepackage{xfrac}
\usepackage{textcomp}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{tabularray}
\usepackage{subcaption}
\usepackage{cancel}
\usepackage[bottom]{footmisc}
\usepackage{pdfpages}
\usepackage{tabularx}
\usepackage{amsthm}
\usepackage[skins]{tcolorbox}
\titleformat{\chapter}[display]
  {\normalfont\bfseries}{}{0pt}{\Huge}
\usepackage{hyperref}
\newcommand{\hsp}{\hspace{20pt}}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathcal{C}}
\newcommand{\N}{\mathbb{N}}
\renewcommand{\O}{\mathcal{O}}
\theoremstyle{definition}
\newtheorem{thm}{Theorem}[chapter]
\newtheorem{definition}[thm]{Definition}
\newtheorem{lem}[thm]{Lemma}

\hbadness=100000
\begin{document}
\begin{titlepage}
    \begin{sffamily}
    \begin{center}
        \includegraphics[scale=0.3]{img/page_de_garde.png} \\[1cm]
        \HRule \\[0.4cm]
        { \huge \bfseries LINMA2460 Nonlinear Programming \\[0.4cm] }
    
        \HRule \\[1.5cm]
        \textsc{\LARGE Simon Desmidt\\ Issambre L'Hermite Dumont}\\[3cm]
        {This summary may not be up-to-date, the newer version is available at this address: \hyperlink{https://github.com/SimonDesmidt/Syntheses}{https://github.com/SimonDesmidt/Syntheses}}
        \vfill
        \vspace{2cm}
        {\large Academic year 2024-2025 - Q2}
        \vspace{0.4cm}
         
        \includegraphics[width=0.15\textwidth]{img/epl.png}
        
        UCLouvain\\
    
    \end{center}
    \end{sffamily}
\end{titlepage}

\setcounter{tocdepth}{1}
\tableofcontents
\chapter{Definitions, notations and random properties}
\begin{itemize}
	\item The Taylor expansion of order $p$ of the function $f$ around $x_k$ and evaluated at $y$ is: 
	\begin{equation}
		T_p(y;x_k) = f(x_k) + \sum_{i=1}^{p} \frac{1}{i!} D^i f(x_k) (y-x_k)^i
	\end{equation}
	\item We can thus define the gradient w.r.t. $y$ of the Taylor expansion of order $p$ of $f$ around $x_k$ and evaluated at $x_{k+1}$:
	\begin{equation}
		\nabla_y T_p(x_{k+1};x_k) = \left. \nabla_y T_p(y;x_k) \right|_{y=x_{k+1}}
	\end{equation}
	\item An oracle is a "black box" that gives information about the derivatives based on $x$. The general form of an oracle is:
	\begin{equation}
		\text{p-order oracle:} \quad x \mapsto \{D^if(x)\}_{i=0}^p
	\end{equation}
	And so we have the following simple oracles examples:
	\begin{equation}
		\begin{aligned}
			\text{Zero}^{th} \text{-order oracle:} \quad &x \mapsto \{f(x)\} \\
			\text{First-order oracle:} \quad &x \mapsto \{ f(x), \nabla f(x) \} \\
			\text{Second-order oracle:} \quad &x \mapsto \{ f(x), \nabla f(x), \nabla^2 f(x) \}
		\end{aligned}
	\end{equation}
    \item $\C^p_L(\R^n)$: Class of functions p-times continuously differentiable with L-Lipschitz continuous p-order derivative, i.e. $\| D^pf(x) - D^pf(y) \| \leq L \|x-y \|$, $\forall x,y\in \R^n$. And so we have the following simple classes of problems:
    \begin{itemize}
		\item $\C^1_L(\R^n)$: Class of continuously differentiable functions with L-Lipschitz gradient;
    \item $\C^2_L(\R^n)$: Class of continuously differentiable functions with L-Lipschitz hessian.
	\end{itemize}
	\item  pth-order method (generalization of GM):
	\begin{equation}
		x_{k+1} = \arg\min_{y\in \R^n} \Omega_{x_k,y,p}(y) \equiv T_{x_k,p}(y) + \frac{M}{(p+1)!}\|y-x_k\|^{p+1}
	\end{equation}
	where $M$ is an approximation of the Lipschitz constant $L$ for the pth-order derivative of $f$.
	\item Convergence rate: 
	\begin{itemize}
		\item Linear:
		\begin{equation}\label{eq:linear_convergence_rate}
			\|x_{k+1}-x^*\| \leq \alpha \|x_k-x^*\| \quad \forall k\geq 0, \alpha \in (0,1)
		\end{equation}
		\item Super Linear:
		\begin{equation}\label{eq:super_linear_convergence_rate}
			\lim_{k \to +\infty} \frac{\|x_{k+1}-x^*\|}{\|x_k-x^*\|} = 0
		\end{equation}
		\item Quadratic:
		\begin{equation}\label{eq:quadratic_convergence_rate}
			\|x_{k+1}-x^*\| \leq \beta \|x_k-x^*\|^2 \quad \forall k\geq 0, \beta > 0
		\end{equation}
	\end{itemize}
\end{itemize}
\section{Properties}
\begin{itemize}
	\item For a function $f\in \C^1(\Omega)$ and $\Omega$ is bounded, the following holds: $\lVert \nabla f(x)\rVert \le L$ for all $x\in \Omega$ for some $L\ge 0$.
	\item By the mean value theorem, for a continuously differentiable function $f$, $\forall x,y\in \Omega,\: \exists \ z\in \Omega:$ $f(y)-f(x) = \langle \nabla f(z),y-x\rangle$.
	\item For a matrix $A$ and a scalar $b$, $\lVert A\rVert \le b\Longrightarrow |\lambda (A)|\le b \Longrightarrow |A| \preceq bI_n$, where the absolute value of the matrix is taken component wise. 
\end{itemize}
\section{Complexity table}
\begin{center}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		Method & Lipschitz & $\nabla f$ & $\nabla^2 f$ & \dots & $\nabla^p f$ \\
		\hline
		Zero order & & $O(n\varepsilon^{-2})$ & & & \\
		\hline
		First order & $p=1$ & $O(\varepsilon^{-2})$ & & & \\
		\hline
		Second order & $p=2$ & \textcolor{red}{\cancel{X}} & $O(\varepsilon^{-3/2})$ & & \\
		\hline
		\vdots & & \textcolor{red}{\cancel{X}} & \textcolor{red}{\cancel{X}} & $\ddots$ & \\
		\hline
		p order & & \textcolor{red}{\cancel{X}} & \textcolor{red}{\cancel{X}} & \textcolor{red}{\cancel{X}} & $O(\varepsilon^{-\frac{p+1}{p}})$ \\
		\hline
	\end{tabular}
\end{center}
\section{GM VS Newton}
\begin{center}
	\begin{tabular}{|c|c|c|c|}
		\hline
		& cost per iteration & cost of memory & Local rate\\
		\hline
		GM & $\O (n)$ & $\O (n)$ & Linear\\
		\hline
		Quasi-Newton & $\O (n^2)$ & $\O (n^2)$ & Super Linear\\
		\hline
		Newton & $\O (n^3)$ & $\O (n^2)$ & Quadratic\\
		\hline
	\end{tabular}
\end{center}
\begin{itemize}
	\item [$\rightarrow$] For the GM, we assume that we don't need to compute the gradient at each iteration.
\end{itemize}
\chapter{TODO}\label{chap:right_oracle}
We can generalise the property of a L-Lipschitz function to $f\in \C_L^p(\R^n)$. For $p=1$, we had 
\begin{equation}
	f(y)\le f(x_k) + \langle \nabla f(x_k),y-x_k\rangle + \frac{L}{2}\lVert y-x_k\rVert^2 \qquad \forall y\in \R^n 
\end{equation}
For a general value of $p$, it becomes 
\begin{equation}
	f(y) \le T_p(y;x_k) + \frac{L}{(p+1)!}\lVert y-x_k\rVert^{p+1} \qquad \forall y\in \R^n 
\end{equation}
Using this, \textcolor{blue}{we need a $p$-th order oracle} for the method to work. \\
To solve $\min_{x\in \R^n} f(x)$, we can use the iteration 
\begin{equation}\label{eq:argmin}
	x_{k+1} = \arg\min_{y\in \R^n} T_p(y;x_k)+\frac{M}{(p+1)!}\lVert y-x_k\rVert^{p+1}
\end{equation}
where the constant $M$ is an approximation of the Lipschitz constant $L$. \textcolor{blue}{Assuming $f\in \C^p_L(\R^n)$}, we have 
\begin{equation}
	\begin{aligned}
		f(x_{k+1}) &\le T_p(x_{k+1};x_k) + \frac{L}{(p+1)!}\lVert x_{k+1}-x_k\rVert^{p+1}\\
		&= \underbrace{T_p(x_{k+1};x_k) + \frac{M}{(p+1)!}\lVert x_{k+1}-x_k\rVert^{p+1}}_{\le f(x_k)} + \frac{(L-M)}{(p+1)!}\lVert x_{k+1}-x_k\rVert^{p+1}
	\end{aligned}
\end{equation}
where the inequality $\le f(x_k)$ is due to the decrease of $f$ and equation \eqref{eq:argmin}. 
\textcolor{blue}{Suppose that $M>2L$}. After some algebraic manipulations, we get
\begin{equation}\label{eq:bound}
	f(x_k)-f(x_{k+1}) \ge \frac{L}{(p+1)!}\lVert x_{k+1}-x_k\rVert^{p+1}
\end{equation}
On the other hand, using the triangular inequality, 
\begin{equation}
	\begin{aligned}
		\lVert \nabla f(x_{k+1})\rVert & \le \lVert \nabla f(x_{k+1})-\nabla_y T_p(x_{k+1};x_k)\lVert\\
		& + \underbrace{\left \lVert \nabla_y T_p(x_{k+1};x_k)+\nabla \left.\left(\frac{M}{(p+1)!} \lVert \cdot -x_k\rVert^{p+1}\right)\right|_{y=x_{k+1}}\right \rVert}_{=0}	\\
		& + \left\lVert \nabla \left.\left(\frac{M}{(p+1)!}\lVert \cdot -x_k\rVert^{p+1}\right)\right|_{y=x_{k+1}}\right \rVert \\
		& \le \frac{L}{p!}\lVert x_{k+1}-x_k\rVert^p + \frac{M}{p!} \lVert x_{k+1}-x_k\rVert^p
	\end{aligned}
\end{equation}
\begin{equation}\label{eq:bound_on_x}
	\Longrightarrow \lVert x_{k+1}-x_k\rVert \ge \left(\frac{p!}{L+M}\right)^{1/p}\lVert \nabla f(x_{k+1})\rVert^{1/p}
\end{equation}
Combining equations \eqref{eq:bound} and \eqref{eq:bound_on_x}, 
\begin{equation}\label{eq:next_bound}
	f(x_k)-f(x_{k+1}) \ge \underbrace{\frac{L}{(p+1)!}\left(\frac{p!}{L+M}\right)^{\frac{p+1}{p}}}_{\eqqcolon C(L)} \lVert \nabla f(x_{k+1})\rVert^{\frac{p+1}{p}}
\end{equation}
Let $T(\varepsilon) = \inf \{k\in \mathbb{N}:\: \lVert \nabla f(x_k)\rVert\le \varepsilon\}$. \textcolor{blue}{Assume that $T(\varepsilon)\ge 2$ and $f(x)\ge f_{low}$ $\forall x\in \R^n$.} Summing up \eqref{eq:next_bound} for $k=0,\dots, T(\varepsilon)-2$,
\begin{equation}
	\begin{aligned}
		f(x_0)-f_{low} &\ge f(x_0)-f(x_{T(\varepsilon)-1}) = \sum_{k=0}^{T(\varepsilon)-2} f(x_k)-f(x_{k+1}) \\ 
		& \ge (T(\varepsilon)-1)C(L) \varepsilon^{\frac{p+1}{p}}\\
		\Longrightarrow T(\varepsilon) &\le 1 + \frac{f(x_0)-f_{low}}{C(L)} \varepsilon^{-\frac{p+1}{p}} \equiv \O\left(\varepsilon^{-\frac{p+1}{p}}\right)
	\end{aligned}
\end{equation}
\chapter{Gradient descent without gradient}\label{chap:}
For this problem, consider an adversarial attack on block-based image classifier. We have a machine learning model that given an image $a\in\R^p$ it returns $c(a)\in\R^m$, where $c_j(a) \in [0,1]$ is the probability of image $a$ to be in class $j$. The classifier prediction is: $j(a) = \arg\max_{j\in [1,\dots,m]} c_j(a)$.\\
\textcolor{red}{TODO - Add mise en situation ou pas?}
\newline

Given $x_k$, let us decide:
\begin{equation}
	x_{k+1} = x_k - \frac{1}{\sigma} g_{h_k}(x_k) \quad \quad \quad h_k > 0, \: \sigma > 0
\end{equation}
where $g_{h_k}(x_k) \in \R^n$ is given by:
\begin{equation}
	[g_{h_k}(x_k)]_j = \frac{f(x_k+he_j)-f(x_k)}{h_k} \quad \quad \quad \forall j\in [1,\dots,m]
\end{equation}
\textcolor{blue}{Suppose that $f\in \C^1_L(\R^n)$}. Then,
\begin{equation}
	\|\nabla f(x_k) - g_{h_k}(x_k)\| \leq \frac{L \sqrt{n}}{2} h_k
\end{equation}
Thus 
\begin{equation}
	\begin{aligned}
		f(x_{k+1}) &\leq f(x_k) &&+ \langle \nabla f(x_k), x_{k+1}-x_k\rangle + \frac{L}{2}\lVert x_{k+1}-x_k\rVert^2\\
		&= f(x_k) &&+ \langle g_{h_k}(x_k), x_{k+1}-x_k\rangle + \frac{\sigma}{2} \|x_{k+1}-x_k\|^2\\
		& &&+ \langle \nabla f(x_k) - g_{h_k}(x_k), x_{k+1}-x_k\rangle + \frac{(L-\sigma)}{2}\lVert x_{k+1}-x_k\rVert^2\\
		&\leq f(x_k) &&- \frac{1}{\sigma} \|g_{h_k}(x_k)\|^2 + \frac{1}{2\sigma} \|g_{h_k}(x_k)\|^2 \\
		& &&+ \| \nabla f(x_k) - g_{h_k}(x_k)\| \frac{1}{\sigma} \|g_{h_k}(x_k)\| + \frac{(L-\sigma)}{2\sigma^2} \|g_{h_k}\|^2\\
		& \leq f(x_k) &&- \frac{1}{2\sigma} \|g_{h_k}(x_k)\|^2 + \frac{L \sqrt{n}}{2} h_k \frac{1}{\sigma} \|g_{h_k}\| + \frac{(L-\sigma)}{2\sigma^2} \|g_{h_k}\|^2\\
		&\leq f(x_k) &&- \frac{1}{2\sigma} \|g_{h_k}(x_k)\|^2 + \frac{L}{2} \left( \frac{nh_k^2}{2}+ \frac{1}{2 \sigma} \|g_{h_k}(x_k)\|^2 \right) + \frac{(L-\sigma)}{2\sigma^2} \|g_{h_k}\|^2\\
		&= f(x_k) &&- \left( \frac{2\sigma - L -2(L - \sigma)}{4 \sigma^2} \right) \|g_{h_k}(x_k)\|^2 + \frac{Ln}{4} h_k^2\\
		&= f(x_k) &&- \frac{(4\sigma -3L)}{4 \sigma} \|g_{h_k}(x_k)\|^2  + \frac{Ln}{4} h_k^2
	\end{aligned}
\end{equation}
\begin{equation}
	\Longrightarrow \frac{(4\sigma -3L)}{4 \sigma} \|g_{h_k}(x_k)\|^2 \leq f(x_k) - f(x_{k+1}) + \frac{Ln}{4} h_k^2
\end{equation}
\textcolor{blue}{If $\sigma \gg L$, then}
\begin{equation}
	\frac{1}{4\sigma} \|g_{h_k}(x_k)\|^2 \leq f(x_k) - f(x_{k+1}) + \frac{\sigma n}{4} h_k^2
\end{equation}
On the other hand, we have
\begin{equation}
	\begin{aligned}
		\|\nabla f(x_k)\| &\leq \|\nabla f(x_k) - g_{h_k}(x_k)\| + \|g_{h_k}(x_k)\|\\
		&\leq \frac{L \sqrt{n}}{2} h_k + \|g_{h_k}(x_k)\|\\
	\end{aligned}
\end{equation}
Using trick \eqref{eq:sq2} in chapter \ref{chap:tricks},
\begin{equation}
	\Longrightarrow \|\nabla f(x_k)\|^2 \leq \frac{L^2 n}{2} h_k^2 + 2 \|g_{h_k}(x_k)\|^2 
\end{equation}
\begin{equation}
	\Longrightarrow \frac{1}{8 \sigma} \|\nabla f(x_k)\|^2 \leq \frac{L^2n}{16\sigma} h_k^2 + \frac{1}{4\sigma} \|g_{h_k}(x_k)\|^2
\end{equation}
\begin{equation}\label{eq:brigitte}
	\Longrightarrow \frac{1}{8 \sigma} \|\nabla f(x_k)\|^2 \leq f(x_k) - f(x_{k+1}) + \frac{\sigma n}{4} h_k^2 + \frac{\sigma n}{16}h_k^2
\end{equation}
Let $T(\varepsilon) = \inf \{k\in \mathbb{N}: \|\nabla f(x_k)\| \leq \varepsilon \}$, \textcolor{blue}{with $f(x)$ bounded from below by $f_{low}$}. Summing up \eqref{eq:brigitte} for $k=0,\dots, T(\varepsilon)-1$:
\begin{equation}
	\frac{T(\varepsilon)}{8 \sigma} \varepsilon^2 \leq f(x_0) - f_{low} + \frac{5 \sigma n}{16} \sum_{k=0}^{T(\varepsilon)-1} h_k^2
\end{equation}
\textcolor{blue}{If $\{h_k^2\}_{k\ge0}$ is summable}
\begin{equation}
	\Longrightarrow T(\varepsilon) \leq 8 \sigma \left( f(x_0) - f_{low} + \frac{5 \sigma n}{16} \sum_{k=0}^{T(\varepsilon)-1} h_k^2\right) \varepsilon^2 = \O(\varepsilon^2)
\end{equation}
In terms of call to the oracle, we have a complexity bound of $\O(n\varepsilon^2)$.

\chapter{Local rates of convergence}
\section{Linear rate of GM}
Let \textcolor{blue}{$f \in \C_M^{2,2}(\R^n)$}. Assume \textcolor{blue}{$f$ has a local minimizer $x^*$ such that 
\begin{equation}\label{eq:hessian_bound}
	\mu I_n \preceq \nabla^2 f(x^*) \preceq MI_n
\end{equation}}
Let $x_{k+1} = x_k - \frac{1}{L} \nabla f(x_k)$ for a given $x_0 \in \R^n$.\\
Notice that
\begin{equation}\label{eq:G_k}
	\begin{aligned}
		\nabla f(x_k) &= \nabla f(x_k) - \nabla f(x^*)\\
		&= \int_{0}^{1} \nabla^2 f(x^* + \tau(x_k-x^*)) (x_k-x^*) d\tau\\
		&= \int_{0}^{1} \nabla^2 f(x^* + \tau(x_k-x^*)) d\tau (x_k-x^*) \\
		&= G_k (x_k-x^*)
	\end{aligned}
\end{equation}
Then,
\begin{equation}
	\begin{aligned}
		\|x_{k+1} - x^*\| &= \|x_k - \frac{1}{L} \nabla f(x_k) - x^*\|\\
		&= \|(I_n-\frac{1}{L}G_k)(x_k-x^*)\|\\
		&\leq \|I_n - \frac{1}{L}G_k\| \|x_k-x^*\|
	\end{aligned}
\end{equation}
Since $f \in C_M^{2,2}(\R^n)$, we have $\|\nabla^2f(x^*+\tau(x_k-x^*))-\nabla^2f(x^*)\| \leq \tau M \|x_k -x^*\|$ and using this we get:
\begin{equation}
	|\langle \nabla^2f(x^*+\tau(x_k-x^*))-\nabla^2f(x^*) v, v\rangle| \leq \tau M \|x_k -x^*\| \|v\|^2 \quad \forall v \in \R^n
\end{equation}
Using the bound \eqref{eq:hessian_bound} and the previous inequality, we get:
\begin{equation*}
		-\tau M \|x_k -x^*\| \|v\|^2 \leq \left\langle \left(\nabla^2f(x^*+\tau(x_k-x^*))-\nabla^2f(x^*)\right) v, v\right\rangle \leq \tau M \|x_k -x^*\| \|v\|^2\\
\end{equation*}
\begin{equation*}
	\nabla^2 f(x^*) - \tau M \|x_k -x^*\| I_n \preceq \nabla^2f(x^*+\tau(x_k-x^*)) \preceq \nabla^2 f(x^*) + \tau M \|x_k -x^*\|I_n 
\end{equation*}
\begin{equation*}
	(\mu - \tau M \|x_k -x^*\|)I_n \preceq \nabla^2f(x^*+\tau(x_k-x^*)) \preceq (L + \tau M \|x_k -x^*\|) I_n 
\end{equation*}
By the properties of the semi-definite matrices, and the trick \eqref{eq:vtrick}, we have:
\begin{equation}
	\begin{aligned}
		\int_{0}^{1} (\mu - \tau M \|x_k -x^*\|) \|v\|^2 d\tau &\leq \int_{0}^{1} \langle \nabla^2f(x^*+\tau(x_k-x^*)) v,v \rangle d\tau\\ 
		&\leq \int_{0}^{1} (L + \tau M \|x_k -x^*\|) \|v\|^2 d\tau \quad \forall v \in \R^n
	\end{aligned}
\end{equation}
By using $G_k$ and some constants, we get:
\begin{equation}
	-\frac{1}{L}(L+\frac{M}{2}\|x_k -x^*\|)I_n \preceq - \frac{1}{L} G_k \preceq -\frac{1}{L}(\mu - \frac{M}{2}\|x_k -x^*\|)I_n
\end{equation}
\begin{equation}
	\left(1-\frac{1}{L}(L+\frac{M}{2}\|x_k -x^*\|)\right)I_n \preceq I_n - \frac{1}{L}G_k \preceq \left(1-\frac{1}{L}(\mu - \frac{M}{2}\|x_k -x^*\|)\right)I_n
\end{equation}
And finally, 
\begin{equation}\label{eq:eq3cm3}
	\begin{aligned}
		\|I_n - \frac{1}{L}G_k\| &\leq \max\left\{\left|1-\frac{1}{L}(L + \frac{M}{2}\|x_k -x^*\|)\right|,\left|1-\frac{1}{L}(\mu - \frac{M}{2}\|x_k -x^*\|)\right|\right\}\\
		&= \max\left\{\frac{M}{2L}\|x_k -x^*\|, 1-\frac{\mu}{L}+\frac{M}{2L}\|x_k -x^*\|\right\}\\
		&= 1-\frac{\mu}{L}+\frac{M}{2L}\|x_k -x^*\|
	\end{aligned}
\end{equation}
\textcolor{blue}{Suppose that $\frac{M}{2L} \|x_k -x^*\| \leq \frac{\mu}{2L} \Longleftrightarrow \|x_k -x^*\| \leq \frac{\mu}{M}$}\\
Then, in \eqref{eq:eq3cm3}, we get:
\begin{equation}
	\|I_n - \frac{1}{L}G_k\| \leq 1-\frac{\mu}{2L} < 1
\end{equation}
And so, by \eqref{eq:G_k}
\begin{equation}\label{eq:bound_by_G_k}
	\|x_{k+1} - x^*\| \leq \|I_n - \frac{1}{L}G_k\| \|x_k -x^*\| < \|x_k -x^*\|
\end{equation}
If $\|x_0 - x^* \|< \frac{\mu}{M}$, it follows from the previous reasoning that:
\begin{equation}
	\|x_2 - x^*\| \leq  (1-\frac{\mu}{2L}) \|x_1-x^*\| \leq (1-\frac{\mu}{2L})^2 \|x_0-x^*\|\leq \frac{\mu}{M}
\end{equation} 
And so by induction, we can conclude that:
\begin{equation}\label{eq:bound_x_k_induction}
	\|x_k - x^*\| \leq \left(1-\frac{\mu}{2L}\right)^k \|x_0-x^*\| \quad \forall k \geq 0
\end{equation}
\begin{center}
	\color{red}\boxed{\textcolor{black}{\Rightarrow \text{Linear rate of convergence}}}\color{black}\\
\end{center}

Given $\varepsilon > 0$, let $T(\varepsilon) = \inf\{k\in \mathbb{N} : \|x_k -x^*\|\leq \varepsilon\}$. Then, \textcolor{blue}{if $T(\varepsilon) \geq 1$} and using \eqref{eq:bound_x_k_induction}, we get:
\begin{equation}
	\begin{aligned}
		\varepsilon < \|x_{T(\varepsilon)-1} - x^*\| &\leq \left(1-\frac{\mu}{2L}\right)^{T(\varepsilon)-1} \|x_0 - x^*\|\\
		\log \left(\frac{\varepsilon}{\|x_0 - x^*\|}\right) &\leq (T(\varepsilon)-1) \log \left(1-\frac{\mu}{2L}\right)\\
		T(\varepsilon)-1 &\leq \frac{\log \left(\frac{\varepsilon}{\|x_0 - x^*\|}\right)}{\log \left(1-\frac{\mu}{2L}\right)} = \frac{\log \left(\|x_0 - x^*\|\varepsilon^{-1}\right)}{|\log \left(1-\frac{\mu}{2L}\right)|}
	\end{aligned}
\end{equation}
\begin{center}
	\color{red}
	\boxed{\color{black} T(\varepsilon) \leq \mathcal{O}(\log(\varepsilon^{-1})) }
	\color{black}\\
\end{center}
\begin{itemize}
	\item [$\rightarrow$] Note: convexity was never assumed!
\end{itemize}

\section{Local quadratic convergence of Newton's method}
\textcolor{blue}{Let $f \in \C_M^{2,2}(\R^n)$. Assume $f$ has a local minimizer $x^*$ such that
\begin{equation}\label{eq:hessian_bound2}
	\mu I_n \preceq \nabla^2 f(x^*) \quad \mu > 0
\end{equation}}
Given $x_0 \in \R^n$, let:
\begin{equation}\label{eq:newton}
	x_{k+1} = x_k - \nabla^{-2} f(x_k) \nabla f(x_k)
\end{equation}
We have, by the previous equation and the definition of $G_k$ \eqref{eq:G_k}:
\begin{equation}\label{eq:bound_x_k+1}
	\begin{aligned}
		\|x_{k+1}-x^*\| &= \|x_k - \nabla^{-2} f(x_k) \nabla f(x_k) - x^*\|\\
		&= \|(x_k-x^*)-\nabla^{-2} f(x_k)G_k(x_k-x^*)\|\\
		&= \|\nabla^{-2} f(x_k) \left(\nabla^2f(x_k)-\int_{0}^{1}\nabla^2f(x^*+\tau(x_k-x^*))d\tau\right)(x_k-x^*)\|\\
		&= \|\nabla^{-2} f(x_k) \left( \int_{0}^{1} \nabla^2f(x_k)-\nabla^2f(x^*+\tau(x_k-x^*))d\tau\right)(x_k-x^*)\|\\
		&\leq \|\nabla^{-2} f(x_k)\| \left( \int_{0}^{1} \left\|\nabla^2f(x_k)-\nabla^2f(x^*+\tau(x_k-x^*))\right\|d\tau\right)\|x_k-x^*\|\\
		&\leq \|\nabla^{-2} f(x_k)\| \left( \int_{0}^{1} M(1-\tau) \|x_k-x^*\|d\tau\right)\|x_k-x^*\|\\
		&\leq \|\nabla^{-2} f(x_k)\|\|x_k-x^*\|^2\frac{M}{2}
	\end{aligned}
\end{equation}
Since $f\in \C^{2,2}_M(\R^n)$, we have
\begin{equation}
	\nabla^2 f(x^*+\tau(x_k-x^*)) - \nabla^2 f(x^*) \succeq \tau M \|x_k-x^*\|I_n
\end{equation}
\begin{equation}
	\begin{aligned}
		\nabla^2 f(x_k) &\succeq \nabla^2 f(x^*) - M \|x_k-x^*\|I_n\\
		&\succeq (\mu - M \|x_k-x^*\|)I_n\\
		\lambda_{\min}(\nabla^2 f(x_k)) &\geq \mu - M \|x_k-x^*\|
	\end{aligned}
\end{equation}
\textcolor{blue}{Suppose that $-M \|x_k-x^*\| \geq - \frac{\mu}{2} \Leftrightarrow \|x_k-x^*\| \leq \frac{\mu}{2M}$}. Then,
\begin{equation}
	\begin{aligned}
		\lambda_{\min}(\nabla^2 f(x_k)) &\geq \frac{\mu}{2}\\
		\lambda_{\max}(\nabla^{-2} f(x_k)) &\leq \frac{2}{\mu}\\
		\Rightarrow \|\nabla^{-2} f(x_k)\| &\leq \frac{2}{\mu}
	\end{aligned}
\end{equation}
Therefore, by \eqref{eq:bound_x_k+1}, we conclude that:
\begin{equation}
	\begin{aligned}
		\|x_{k+1}-x^*\| &\leq \frac{M}{2}\|\nabla^{-2}f(x_k)\|\|x_k-x^*\|^2\\
		&\leq \frac{M}{\mu}\|x_k-x^*\|^2
	\end{aligned}
\end{equation}
If $\|x_k-x^*\| \leq \frac{\mu}{2M}$ then,
\begin{equation}\label{eq:bound_of_x_k1_by_x_k}
	\|x_{k+1}-x^*\| \leq \frac{M}{\mu} \|x_k - x^*\|^2 = \frac{1}{2} \|x_k - x^*\|
\end{equation}
If $\|x_0-x^*\| \leq \frac{\mu}{2M}$ then $\{x_k\}_{k\geq0} \subset B[x^*,\frac{\mu}{2M}]$.\\
Denote $\delta_k = \frac{M}{\mu} \|x_k-x^*\|$, then we have $\delta_0 = \frac{M}{\mu} \|x_0-x^*\| \leq \frac{1}{2}$, and if we combine this with \eqref{eq:bound_of_x_k1_by_x_k}, we get:
\begin{equation}
	\delta_{k+1} \leq \delta_k^2 \quad \forall k \geq 0
\end{equation}
And if we proceed by recurrence, we get:
\begin{equation}
	\begin{aligned}
		\delta_1 \leq &\delta_0^2 \leq \left(\frac{1}{2}\right)^2\\
		\delta_2 \leq &\delta_1^2 \leq \left(\frac{1}{2}\right)^4\\
		&\vdots\\
		&\delta_k \leq \left(\frac{1}{2}\right)^{2^k} \quad \forall k \geq 0
	\end{aligned}
\end{equation}
\begin{equation}\label{eq:quadratic_rate_of_convergence}
	\Rightarrow \|x_k-x^*\|\leq \frac{\mu}{M} \left(\frac{1}{2}\right)^{2^k}
\end{equation}
Let $T(\varepsilon) = \inf\{k\in\N: \|x_k-x^*\|\leq \varepsilon\}$ and \textcolor{blue}{suppose that $T(\varepsilon) \geq 1$}. Then using the convergence rate \eqref{eq:quadratic_rate_of_convergence}, we can state the maximal number of iterations:
\begin{equation}
	\varepsilon \leq \|x_{T(\varepsilon)-1}-x^*\| \leq \frac{\mu}{M} \left(\frac{1}{2}\right)^{2^{T(\varepsilon)-1}}
\end{equation}
\begin{equation}
	2^{2^{T(\varepsilon)-1}} \leq \frac{\mu}{M} \varepsilon^{-1}
\end{equation}
\begin{center}
	\color{red}
	\boxed{\color{black} \Rightarrow T(\varepsilon) \leq \log_2(\log_2(\frac{\mu}{M} \varepsilon^{-1}))}
	\color{black}
\end{center}
	
\section{Quasi Newton methods}
\subsection{SR1 Update}
One step of a Quasi-Newton method is given by:
\begin{equation}\label{eq:quasi_newton}
	x_{k+1} = x_k -B_k \nabla f(x_k)
\end{equation}
With \textcolor{blue}{$B_k \in \R^{n\times n}$, symmetric and non-singular}.\\
\textcolor{blue}{Suppose that $x_k \to x^*$ when $k \to \infty$, and that $\nabla^2f(x_k) \succeq \mu I_n$ with $\mu \geq 0$}.\\
We want the condition on $B_k$ to have a Super Linear convergence \eqref{eq:super_linear_convergence_rate} of the Quasi-Newton method. So let us \textcolor{blue}{assume that $f \in \C_M^{2,2}(\R^n)$}.\\
Then,
\begin{equation}\label{eq:bound_hessian_difference}
	\|\nabla^2f(x_{k+1})-\nabla^2f(x_k)\| \leq M \|x_{k+1}-x_k\|
\end{equation}
\textcolor{red}{GOOD LABEL ?}
\begin{equation}\label{eq:M_smoothness}
	\|\nabla f(x_{k+1})-\nabla f(x_k)-\nabla^2 f(x_k)(x_{k+1}-x_k)\| \leq \frac{M}{2} \|x_{k+1}-x_k\|^2
\end{equation}
Therefore
\begin{equation}
	\begin{aligned}
		\nabla f(x_{k+1}) = \nabla f(x_{k+1}) &  \color{red}  - \nabla f(x_k) - \nabla^2 f(x_k) (x_{k+1}-x_k)  \color{black} \\ &  \color{red} + \nabla f(x_k) + \nabla^2 f(x_k) (x_{k+1}-x_k) \color{black}
	\end{aligned}
\end{equation}
Using the relation \eqref{eq:quasi_newton} we get:
\begin{equation}
	\begin{aligned}
		\nabla f(x_{k+1}) &= \nabla f(x_{k+1}) &&- \nabla f(x_k)- \nabla^2 f(x_k) (x_{k+1}-x_k)\\ 
		& &&- B_k^{-1}  (x_{k+1}-x_k)\\ 
		& &&+ \nabla^2 f(x_k) (x_{k+1}-x_k)\\
		&= \nabla f(x_{k+1})  &&- \nabla f(x_k) - \nabla^2 f(x_k) (x_{k+1}-x_k)\\
		& &&- \left(B_k^{-1} - \nabla^2 f(x^*)\right) (x_{k+1}-x_k)\\
		& &&+ \left( \nabla^2 f(x_k) - \nabla^2 f(x^*)\right) (x_{k+1}-x_k)\\
		\|\nabla f(x_{k+1})\| &\leq \|\nabla f(x_{k+1})  &&- \nabla f(x_k) - \nabla^2 f(x_k) (x_{k+1}-x_k) \| \\
		& &&+ \|\left(B_k^{-1} - \nabla^2 f(x^*)\right) (x_{k+1}-x_k)\|\\
		& &&+ \|\left( \nabla^2 f(x_k) - \nabla^2 f(x^*)\right) \| \|(x_{k+1}-x_k)\|\\ 
		&\leq \frac{M}{2} \|x_{k+1}-x_k\|^2 &&+ M \|x_k-x^*\| \|x_{k+1}-x_k\|\\
		& &&\color{red}+ \|\left(B_k^{-1}-\nabla^2f(x_k)\right)(x_{k+1}-x_k)\|
	\end{aligned}
\end{equation}
On the line before we used \eqref{eq:bound_hessian_difference} and \eqref{eq:M_smoothness}. And so we can write:
\begin{equation}\label{eq:eq4cm4}
	\frac{\|\nabla f(x_{k+1})\|}{\|x_{k+1}-x_k\|} \leq \frac{M}{2} \|x_{k+1}-x_k\| + M \|x_k - x^* \| + \color{red} \frac{\|\left(B_k^{-1}-\nabla^2f(x_k)\right)(x_{k+1}-x_k)\|}{\|x_{k+1}-x_k\|}\color{black}
\end{equation}
From now on , suppose that this condition (Dimis-Mori condition) is true:
\begin{equation}\label{eq:dimis_mori_condition}
	\lim_{k\to \infty} \frac{\|\left(B_k^{-1}-\nabla^2f(x_k)\right)(x_{k+1}-x_k)\|}{\|x_{k+1}-x_k\|} = 0
\end{equation}
Under this condition and by \eqref{eq:eq4cm4}, we have:
\begin{equation}
	\lim_{k \to \infty} \frac{\|\nabla f(x_{k+1})\|}{\|x_{k+1}-x_k\|} = 0
\end{equation}
As $\|x_{k+1}-x_k\| \to 0$, we conclude that $\lim_{x \to \infty} \|\nabla f(x_{k+1})\| = 0$ and so $\|\nabla f(x^*)\| = 0 \Rightarrow \nabla f(x^*) = 0$, meaning that $x^*$ is a stationary point of $f(\cdot)$.\\
We have $\nabla^2 f(x^*) \succeq \mu I_n$ and given $y \in \R^n$, we have:
\begin{equation}
	\begin{aligned}
		\nabla^2f(y)-\nabla^2f(x^*) &\succeq - M \|y-x^*\| I_n\\
		\nabla^2f(y) &\succeq (\mu - M \|y-x^*\|) I_n\\
	\end{aligned}
\end{equation}
Thus, if $-M \|y-x^*\| \geq - \frac{\mu}{2}$ then $\nabla^2f(y) \succeq \frac{\mu}{2} I_n$.\\
Since $x_k \to x^*$, there exists $k_0 \in \N$ such that $\|x_{k+1}-x^*\| \leq \frac{\mu}{2M}$ $\forall k \geq k_0$. Thus for any $\tau \in [0,1]$:
\begin{equation}
	\|x^*+\tau(x_{k+1}-x^*)-x^*\| \leq \frac{\mu}{2M}, \quad \forall k \geq k_0
\end{equation}
and so $\nabla^2f(x^*+\tau(x_{k+1}-x^*)) \succeq \frac{\mu}{2} I_n$ $\forall k \geq k_0$.
\begin{equation}
	\begin{aligned}
		\|x_{k+1}-x^*\|\|\nabla f(x_{k+1})\| &\geq (x_{k+1}-x^*)^T\nabla f(x_{k+1})\\
		&= (x_{k+1}-x^*)^T \left( \nabla f(x_{k+1}) - \nabla f(x^*)\right)\\
		&= (x_{k+1}-x^*)^T \int_{0}^{1} \nabla^2 f(x^*+\tau(x_{k+1}-x^*)) (x_{k+1}-x^*) d\tau\\
		&\geq \int_{0}^{1} (x_{k+1}-x^*)^T \frac{\mu}{2} I_n (x_{k+1}-x^*) d\tau\\
		&= \frac{\mu}{2} \|x_{k+1}-x^*\|^2
	\end{aligned}
\end{equation}
\begin{equation}\label{eq:bound_norm_hessian}
	\|\nabla f(x_{k+1})\| \geq \frac{\mu}{2} \|x_{k+1}-x^*\|
\end{equation}
Let $\rho_k = \frac{\|x_{k+1}-x^*\|}{\|x_k-x^*\|}$ then, using \eqref{eq:triangular_inequality_by_min}, we obtain:
\begin{equation}\label{eq:bound_rho_k}
	\begin{aligned}
		\frac{\|\nabla f(x_{k+1})\|}{\|x_{k+1}-x_k\|} &\geq \frac{(\frac{\mu}{2})\|x_{k+1}-x^*\|}{\|x_{k+1}-x_k\|}\\
		&\geq \frac{(\frac{\mu}{2})\|x_{k+1}-x^*\|}{\|x_{k+1}-x^*\| + \|x_k-x^*\|}\\
		&= \frac{(\frac{\mu}{2}) \rho_k}{\rho_k + 1}
	\end{aligned}
\end{equation}
Combining \eqref{eq:bound_rho_k} and \eqref{eq:eq4cm4}, we get:
\begin{equation}
	\frac{\mu}{2} \frac{\rho_k}{\rho_k + 1} \leq \frac{M}{2} \|x_{k+1}-x_k\| + M \|x_k-x^*\| + \frac{\|\left(B^{-1}_k - \nabla^2f(x^*)\right) (x_{k+1}-x_k)\|}{\|x_{k+1}-x_k\|}
\end{equation}
Since the right hand side goes to zero when $k \to +\infty$, then we have:
\textcolor{red}{IDK how to write that}
\begin{equation}
	\begin{aligned}
		\lim_{k \to \infty} \frac{\rho_k}{1 + \rho_k} &= 0\\
		\lim_{k \to \infty} \frac{1}{\frac{1}{\rho_k} + 1} &= 0\\
		\Rightarrow \lim_{k \to \infty} \frac{\|x_{k+1}-x^*\|}{\|x_k-x^*\|} &\Rightarrow \lim_{k \to \infty} \rho_k = 0  
	\end{aligned}
\end{equation}
For $n=1$, the Quasi-Newton update is written:
\begin{equation}
	x_{k+1}=x_k-b_kf'(x_k), \quad k \geq 0
\end{equation}
with $b_k \in \R$. We want $b_k \approx f''(x_k)^{-1}$ and by finite difference we can express it like that $b_k^{-1} \approx \frac{f'(x_{k-1}+h)-f'(x_{k-1})}{h}$. And with $h = x_h-x_{k-1}$, we can define:
\begin{equation}\label{eq:def_b_k_1D}
	b_k^{-1} = \frac{f'(x_k)-f'(x_{k-1})}{x_{k}-x_{k-1}}
\end{equation}
Thus if $x_k \to x^*$ then:
\begin{equation}
	\lim_{k \to \infty} \frac{|(b_k^{-1}-f''(x^*))(x_k-x_{k-1})|}{|x_k-x_{k-1}|} = 0
\end{equation}
Because we can notice that:
\begin{equation}
	\frac{|(b_k^{-1}-f''(x^*))(x_k-x_{k-1})|}{|x_k-x_{k-1}|} = |b_k^{-1}-f''(x_{k-1})|+|f''(x_{k-1})-f''(x^*)|
\end{equation}
Since $x_k \to x^*$, we have $h=x_k-x_{k-1}$ and so:
\begin{equation}
	b_k^{-1} = \frac{f'(x_k)-f'(x_{k-1})}{x_k-x_{k-1}} \to f''(x_{k-1})
\end{equation}
Thus, $\lim_{k \to \infty} |b_k^{-1} - f''(x_k)| = 0$.\\ 
\textcolor{blue}{Assuming that $f''$ is continuous}, we have $\lim_{k \to \infty} |f''(x_k) - f''(x^*)| = 0$.\\
If we define $s_{k-1} = x_k-x_{k-1}$ and $y_{k-1}=f'(x_k)-f'(x_{k-1})$ and knowing \eqref{eq:def_b_k_1D}, we can write:
\begin{equation}
	\begin{aligned}
		b_k (f'(x_k)-f'(x_{k-1})) &= x_k-x_{k-1}\\
		b_k y_{k-1} &= s_{k-1}
	\end{aligned}
\end{equation}
This suggests that for $n > 1$, we should define the secant condition, $B_k$ such that:
\begin{equation}\label{eq:secant_condition}
	B_k y_{k-1} = s_{k-1}
\end{equation}
Let us define $f(x) = \frac{1}{2} \|Ax-b\|^2 = \frac{1}{2} x^TA^TAx - (A^Tb)^Tx + \frac{1}{2} b^Tb$. If $A$ is full rank then $f$ is a strongly convex quadratic function. And we have $\nabla f(x_k) = A^TAx_k-A^Tb$. Then,
\begin{equation}
	y_{k-1} = \nabla f(x_k) - \nabla f(x_{k-1}) = A^TA(x_k-x_{k-1}) = \nabla^2 f(x_{k}) s_{k-1}
\end{equation} 
And so
\begin{equation}
	\nabla^2 f(x_k) y_{k-1} = s_{k-1}
\end{equation}
Therefore, $\nabla^{-2} f$ satisfies the secant condition \eqref{eq:secant_condition}, when $f$ is a strongly convex quadratic function. Thus it is reasonnable to require the secant for any approximation to $\nabla^{-2}f(x_k)$.\\
\newline
Now, how can we compute $B_k$ such that it satisfies the secant condition \eqref{eq:secant_condition}?\\
Given a matrix $B_{k-1}$, our goal is to find a perturbation matrix $P_{k-1} \in \R^{n \times n}$ such that:
\begin{equation}
	\left(B_{k-1} + P_{k-1}\right) y_{k-1} = s_{k-1}
\end{equation}
If we get such $P_{k-1}$, we can define $B_k = B_{k-1} + P_{k-1}$, which would satisfy the secant condition \eqref{eq:secant_condition}.\\
For that we need at least $n$ degrees of freedom and a symmetric matrix, so it is natural to try:
\begin{equation}
	P_{k-1} = v_{k-1} v_{k-1}^T, \quad v_{k-1} \in \R^{n}
\end{equation}
So we get:
\begin{equation}
	\left(B_{k-1} + v_{k-1} v_{k-1}^T\right) y_{k-1} = s_{k-1}
\end{equation}
By algebraic manipulations, we get:
\begin{equation}\label{eq:secant_condition_v}
	\begin{aligned}
		\left(v_{k-1}^T y_{k-1}\right) v_{k-1} &= s_{k-1} - B_{k-1} y_{k-1}\\
		v_{k-1} &= \frac{s_{k-1}-B_{k-1} y_{k-1}}{\beta} \quad \text{for } \beta = v_{k-1}^T y_{k-1} 
	\end{aligned}
\end{equation}
Combining the two previous equations, we get:
\begin{equation}
	\begin{aligned}
		\left(\frac{1}{\beta} \left(s_{k-1} - B_{k-1} y_{k-1}\right)^T y_{k-1} \right)\frac{1}{\beta} \left(s_{k-1} - B_{k-1} y_{k-1}\right) &= s_{k-1} - B_{k-1} y_{k-1}\\
		\frac{1}{\beta^2} \left(s_{k-1} - B_{k-1} y_{k-1}\right)^T y_{k-1} &= 1
	\end{aligned}
\end{equation}
We can isolate $\beta$:
\begin{equation}\label{eq:beta_secant}
	\beta = \sqrt{\left(s_{k-1} - B_{k-1} y_{k-1}\right)^T y_{k-1}}
\end{equation}
Combining \eqref{eq:secant_condition_v} and \eqref{eq:beta_secant}, we get:
\begin{equation}
	v_{k-1} = \frac{s_{k-1}-B_{k-1} y_{k-1}}{\sqrt{\left(s_{k-1} - B_{k-1} y_{k-1}\right)^T y_{k-1}}}
\end{equation}
This leads us to the following update for $B_k$:
\begin{equation}
	\begin{aligned}
		B_k &= B_{k-1} + v_{k-1} v_{k-1}^T\\
		&= B_{k-1} + \frac{\left(s_{k-1}-B_{k-1} y_{k-1}\right)\left(s_{k-1}-B_{k-1} y_{k-1}\right)^T}{\left(s_{k-1} - B_{k-1} y_{k-1}\right)^T y_{k-1}}
	\end{aligned}
\end{equation}
This is called the \textbf{SR1 update} (symmetric rank 1 update).\\
\subsection{BFGS Update}
Let's take back $B_{k+1} y_k = s_k$ and defining $H_{k+1} = B_{k+1}^{-1} \approx \nabla^2 f(x_{k+1})$, we get $H_{k+1} s_k = y_k$.\\
The idea is to find a rank 2 update that consists in finding $a,b \in \R$ and $v,u \in \R^n$ such that:
\begin{equation}\label{eq:rank2_update_idea}
	\left(H_k + a uu^T + b vv^T\right)s_k = y_k
\end{equation}
Noticing that $u^Ts_k$ and $v^Ts_k$ are scalars, we can impose that:
\begin{equation}
	\begin{cases}
		a(u^Ts_k) u = -H_ks_k\\
		b(v^Ts_k) v = y_k
	\end{cases}
\end{equation}
It suggests that we should take $a= \frac{1}{u^Ts_k}$ and $b = \frac{1}{v^Ts_k}$. Which gives us:
\begin{equation}
	\begin{cases}
		u = -H_ks_k\\
		v = y_k
	\end{cases}
\end{equation}
Combining the two equations, we get:
\begin{equation}
	H_{k+1} = H_k - \frac{H_ks_ks_k^TH_k}{s_k^TH_ks_k} + \frac{y_ky_k^T}{y_k^Ts_k}
\end{equation}
Using linear algebra, we can compute:\\
\begin{equation}\label{eq:BFGS_update}
	\color{red}
	\boxed{
	\color{black}
	\begin{aligned}
		B_{k+1} &= H_{k+1}^{-1}\\
		&= \left(I-\rho_ks_ky_k^T\right) B_k \left(I-\rho_ky_ks_k^T\right) + \rho_k s_k s_k^T \text{ with } \rho_k = \frac{1}{y_k^Ts_k}
	\end{aligned}}
	\color{black}
\end{equation}
\textbf{Remarks:}
\begin{itemize}
	\item If $B_k \succ 0$ and $s_k^Ty_k > 0$ then $B_{k+1} \succ 0$.
	\item If $B_k \succ 0$ and $d_k = - B_k \nabla f(x_k)$, then 
	\begin{equation}
		\langle \nabla f(x_k), d_k \rangle = - \langle \nabla f(x_k), B_k \nabla f(x_k) \rangle < 0
	\end{equation}
	and so $d_k$ is a descent direction for $f$ at $x_k$.
	\item The LBFGS is a low memory of BFGS, that does not require the storage of the matrices $B_k$. Given a vector $v \in \R^n$, it computes $B_kv$, which is all that we need to implement QN method.
\end{itemize}
\section{Cubically regularized Newton's method}
The cubically regularized Newton's method is defined as follows:
\begin{equation}\label{eq:cubically_regularized_newton}
	x_{k+1} = \arg \min_{y \in \R^n} f(x_k) + \nabla f(x_k)^T (y-x_k) + \frac{1}{2} (y-x_k)^T \nabla^2 f(x_k) (y-x_k) + \frac{M}{6} \|y-x_k\|^3
\end{equation}
Based on the following equation, seen in the first lecture:
\begin{equation}
	f(x_k) - f(x_{k+1}) \geq \frac{L_p}{(p+1)!} \left(\frac{p!}{(M+L_p)}\right)^{\frac{p+1}{p}} \|\nabla f(x_{k+1})\|^{\frac{p+1}{p}}
\end{equation}
We can get the following expression a cubically regularized Newton's method (assuming $M=0$):
\begin{equation}
	f(x_k) - f(x_{k+1}) \geq \frac{1}{\sqrt{L_p}} \frac{\sqrt{2}}{3} \|\nabla f(x_{k+1})\|^{\frac{3}{2}}
\end{equation}
\textcolor{red}{NOT SURE ABOUT THAT}
\chapter{Constrained nonlinear programming problems}
Consider the constrained problem 
\begin{equation}\label{eq:constrained_problem}
	\min_{x \in \R^n} f(x) \quad \text{subject to} \quad c_i(x) = 0, \quad i \in \{1,\dots,m\}	
\end{equation}
where \textcolor{blue}{$f,c_i : \R^n \to \R$ are $\C^1$ and there exists at least a $\hat{x}$ such that $c_i(\hat{x}) = 0$}.\\
A natural approach to solve this problem is to consider the related unconstrained problem in which we try to minimize $f(x)$ plus a term that penalizes the violation of the constraints (quadratic penalty function).
\begin{equation}\label{eq:penalty_function}
	\min_{x \in \R^n} Q_{\sigma} (x) \equiv f(x) + \frac{\sigma}{2} \|c(x)\|_2^2
\end{equation}
For the problem \eqref{eq:constrained_problem}, we would like to find a KKT point $x^*$ for which there exists $\lambda^* \in \R^m$ such that:
\begin{equation}\label{eq:KKT}
	\begin{cases}
		\nabla f(x^*) - \displaystyle \sum_{i=1}^m \lambda_i^* \nabla c_i(x^*) = 0  \quad \quad &\textcolor{blue}{\text{(stationarity)}}\\
		c(x^*) = 0 &\textcolor{blue}{\text{(feasibility)}}
	\end{cases}
\end{equation}
In practice, we are happy if we can find an $(\varepsilon_1,\varepsilon_2)$-KKT point for \eqref{eq:constrained_problem}, i.e. a point $x^+$ such that there exists $\lambda^+$ with:
\begin{equation}
	\begin{cases}
		\|\nabla f(x^+) - \displaystyle \sum_{i=1}^m \lambda_i^+ \nabla c_i(x^+) \| \leq \varepsilon_1\\
		\|c(x^+)\| \leq \varepsilon_2\\
	\end{cases}
\end{equation}
Let us relate \eqref{eq:penalty_function} and \eqref{eq:constrained_problem}. Notice that\footnote{$J_c(\cdot)$ is the Jacobian of $c(\cdot)$.}:
\begin{equation}
	\begin{aligned}
		\| \nabla Q_{\sigma} (x) \| &= \|\nabla f(x) + \sigma \mathbf{J}_c(x)^T c(x)\|\\
		&= \|\nabla f(x) + \sigma \sum_{i=1}^{m} c_i(x) \nabla c_i(x)\|\\
		&= \|\nabla f(x) - \sum_{i=1}^{m} \lambda_i^+ \nabla c_i(x)\| \quad \text{with}\quad \lambda_i^+ = - \sigma c_i(x^+)\\
	\end{aligned}
\end{equation}
Therefore, if $\| \nabla Q_{\sigma} (x^+) \| \leq \varepsilon_1$, then there exists $\lambda^+ \in \R^m, \lambda^+ = - \sigma c(x^+)$ such that $\|\nabla f(x^+) - \displaystyle \sum_{i=1}^{m} \lambda_i^+ \nabla c_i(x^+)\| \leq \varepsilon_1$.\\
Given $\bar{x} \in \R^n$, suppose that we compute $x^+$ such that
\begin{equation}
	\begin{aligned}
		Q_\sigma(x^+) &\leq Q_\sigma(\bar{x})\\
		f(x^+) + \frac{\sigma}{2} \|c(x^+)\|^2 &\leq f(\bar{x}) + \frac{\sigma}{2} \|c(\bar{x})\|^2\\
		\frac{\sigma}{2} \|c(x^+)\|^2 &\leq f(\bar{x}) - f(x^+) + \frac{\sigma}{2} \|c(\bar{x})\|^2\\
		\|c(x^+)\|^2 &\leq \frac{2}{\sigma} \left( f(\bar{x}) - f(x^+) \right) + \|c(\bar{x})\|^2
	\end{aligned}
\end{equation}
\textcolor{blue}{If $f(x) \geq f_{low} \quad \forall x \in \R^n$}, we get $\|c(x^+)\|^2 \leq \frac{2}{\sigma} (f(\bar{x}) - f_{low}) + \|c(\bar{x})\|^2$.\\
If $\|c(\bar{x})\| \leq \frac{\varepsilon_2}{\sqrt{2}}$ and $\sigma \geq \frac{4}{\varepsilon_2^2} (f(\bar{x})-f_{low})$, then $\|c(x^+)\|^2 \leq \varepsilon_2^2$ and so $\|c(x^+)\| \leq \varepsilon_2$.\\
In summary, \textcolor{blue}{if we have $\bar{x} \in \R^n$ such that $\|c(\bar{x})\| \leq \frac{\varepsilon_2}{\sqrt{2}}$}, and using a method for unconstrained optimization (e.g. GM), we compute $x^+$ with
\begin{equation}
	Q_\sigma(x^+) \leq Q_\sigma(\bar{x}) \quad \text{and} \quad \|\nabla Q_\sigma(x^+)\| \leq \varepsilon_1
\end{equation}
for $\sigma \geq \frac{4}{\varepsilon_2^2} (f(\bar{x})-f_{low})$, then $x^+$ is a $(\varepsilon_1,\varepsilon_2)$-KKT point for the unconstrained problem \eqref{eq:constrained_problem}.\\
\newline
% \textbf{Example:}
% \begin{equation}
% 	\min f(x) \quad \text{such that} \quad Ax=b  \quad \text{and} \quad \nabla f(x)  \quad L_f \text{-Lipschitz}
% \end{equation}
% \begin{equation}
% 	\begin{aligned}
% 		Q_{\sigma}(x) &= f(x) + \frac{\sigma}{2} \|Ax-b\|_2^2\\
% 		\nabla Q_{\sigma}(x) &= \nabla f(x) + \sigma A^T(Ax-b)\\
% 		\nabla^2 Q_{\sigma}(x) &= \nabla^2 f(x) + \sigma A^TA\\
% 	\end{aligned}
% \end{equation}
% so $\nabla Q_\sigma$ is $L_{Q_\sigma}$-Lipschitz with $L_{Q_\sigma} = L_f + \sigma \|A^TA\|$ so GM takes $\O (L_{Q_\sigma} \varepsilon_1^{-2})$ iterations to find $x^+$ with $\|\nabla Q_\sigma(x^+)\| \leq \varepsilon_1$, and $Q_\sigma (x^+) \leq Q_\sigma(\bar{x})$, if initialized with $\bar{x}$. Since $L_{Q_\sigma}=\O(\sigma)$ and $\sigma = \O(\varepsilon_2^{-2})$, we get a complexity bound of $\O(\varepsilon_1^{-2}\varepsilon_2^{-2})$.\\
\begin{algorithm}
	\caption{Quadratic Penalty Method}
	\begin{algorithmic}[1]
		\State \textbf{Input:} $\varepsilon_1, \varepsilon_2 \in (0,1)$, $x_0 \in \R^n$ such that $\|c(x_0)\|_2 \leq \frac{\varepsilon_2}{\sqrt{2}}$, $\sigma_0 > 0$
		\State $k=0$
		\While{$\|c(x_{k+1})\| > \varepsilon_1$}
			\State Compute $x_{k+1} \in \R^n$ as an approximate solution to 
			\begin{equation}\label{eq:step1}
				\begin{aligned}
					&\min_{x \in \R^n} Q_{\sigma_k}(x)\\
					\text{such that} \quad &Q_{\sigma_k}(x_{k+1}) \leq Q_{\sigma_k}(x_0)\\
					\text{and} \quad &\|\nabla Q_{\sigma_k}(x_{k+1})\| \leq \varepsilon_2 
				\end{aligned}
			\end{equation}
			\State $\sigma_{k+1} \gets 2\sigma_k$
			\State $k \gets k + 1$
		\EndWhile
	\end{algorithmic}
\end{algorithm}
\begin{itemize}
	\item [$\to$] Note: We can compute $x_{k+1}$ satisfying \eqref{eq:step1} by using any monotone optimization method starting from:
	\begin{equation}
		x_k^* = \arg \min \{Q_{\sigma_k}(x_0), Q_{\sigma_k}(x_k)\}
	\end{equation}
	\item For a constrained problem of the form $\displaystyle \min_{x \in \R^n} f(x) \quad \text{s.t. } c_i \leq 0 \quad i=0, \dots, m$, we can add slack variables to obtain an equivalent equality constrained problem:
	\begin{equation}
		\begin{aligned}
			&\min_{x \in \R^n, s \in \R^m} f(x)\\
			\text{s.t. } &c_i(x) + s^2_i= 0 \quad i=1,\dots,m\\
		\end{aligned}
	\end{equation}
\end{itemize}
\chapter{Accelerated Gradient Method}
\section{Derivation of the algorithm}
\begin{equation}
	\min_{x \in \R^n} f(x) 
\end{equation}
where \textcolor{blue}{$f: \R^n \to \R$ is convex, $\nabla f$ is $L$-Lipschitz and has a minimizer $x^*$}. The Accelerated Gradient Method combines present and past information to obtain a point $y_k$ (prediction) and then perform a gradient step using this point as reference point.\\
\begin{equation}
	\begin{cases}
		y_k = (1-\gamma_k)x_k + \gamma_k v_k, \quad \gamma_k \in (0,1)\\
		x_{k+1} = x_k - \frac{1}{L} \nabla f(y_k)
	\end{cases}
\end{equation}
We will identify ways to define $v_k$ and $\gamma_k$ based on the following guiding inequalities:
\begin{equation}\label{eq:guiding_inequalities}
	\begin{aligned}
		&v_k = \arg \min_{x \in \R^n} \Psi_k (x)\\
		&\Psi_k (x) \leq A_k f(x) + \frac{1}{2} \|x-x_0\|^2\\
		&A_k f(x_k) \leq \min_{x \in \R^n} \Psi_k (x) \equiv \Psi_k^*, \quad A_k \geq 0\\
		&A_k \geq c(k-1)^2 \quad \forall k \geq 2
	\end{aligned}
\end{equation}
Assuming the 3 last guiding inequalities \eqref{eq:guiding_inequalities} hold, we have:
\begin{equation}
	\begin{aligned}
		A_kf(x_k) &\leq \min_{x \in \R^n} \Psi_k (x)\\
		&\leq \Psi_k(x^*)\\
		&\leq A_k f(x^*) + \frac{1}{2} \|x^*-x_0\|^2\\
		\left(f(x_k)-f(x^*)\right) &\leq \frac{\|x_k-x^*\|^2}{2 A_k} \quad \forall k \geq 2\\
		&\leq \frac{\|x_k-x^*\|^2}{2 C(k-1)^2} = \O(k^{-2}) = \O(\varepsilon^{-1/2}) \quad \forall k \geq 2
	\end{aligned}
\end{equation}
If we take $A_0=0$ and $\Psi_0(x) = \frac{1}{2} \|x-x_0\|^2$, then the second inequality from \eqref{eq:guiding_inequalities} is true for $k=0$. Let us assume the inequality is true for some $k \geq 0$. Looking at the case $k=1$, it appears that we can define:
\begin{equation}
	\Psi_{k+1} (x) = \Psi_k(x) + b_k \left(f(y_k)+\langle \nabla f(y_k),x-y_k\rangle \right)
\end{equation}
with $b_k > 0$ (to be determined). \\
Suppose that the inequality holds for $k \geq 0$. Then, by the convexity of $f$ and doing an induction assumption:
\begin{equation}
	\begin{aligned}
		\Psi_{k+1}(x) &\leq \Psi_k(x) + b_k f(x)\\
		&\leq A_k f(x) + \frac{1}{2} \|x-x_0\|^2 +b_kf(x)\\
		&= (A_k+b_k)f(x) + \frac{1}{2} \|x-x_0\|^2
	\end{aligned}
\end{equation}
Therefore, if we define $A_{k+1}=A_k+b_k$, then the second inequality of \eqref{eq:guiding_inequalities} will also hold for $k+1$. Regarding of the third inequality of \eqref{eq:guiding_inequalities}, notice that:
\begin{equation}
	\begin{aligned}
		A_0f(x_0) = 0 &= \min_{x \in \R^n} \frac{1}{2} \|x-x_0\|^2\\
		&= 	\min_{x \in \R^n} \Psi_0(x)
	\end{aligned}
\end{equation}
It holds for $k=0$, suppose that it still holds for $k\geq 0$. We want to show that it is also true for $k+1$. Notice that:
\begin{equation}
	\begin{aligned}
		\Psi_1 &= \frac{1}{2} \|x-x_0\|^2 + b_0 \left(f(y_0)+ \langle \nabla f(y_0), x-y_0 \rangle\right)\\
		\Psi_2 &= \frac{1}{2} \|x-x_0\|^2 + \sum_{i=0}^{1} b_0 \left(f(y_i)+ \langle \nabla f(y_i), x-y_i \rangle\right)\\
		&\vdots\\
		\Psi_k &= \frac{1}{2} \|x-x_0\|^2 + \sum_{i=0}^{k-1} b_0 \left(f(y_i)+ \langle \nabla f(y_i), x-y_i \rangle\right)
	\end{aligned}
\end{equation}
Thus, $\Psi_k(x)$ is a $\mu$-strongly convex function with $\mu=1$. Therefore:
\begin{equation}
	\begin{aligned}
		\Psi_k(x) &\geq \Psi_k(v_k) + \frac{1}{2} \|v_k-x_0\|^2\\
		&= \min_{x\in\R^n} \Psi_k(x) + \frac{1}{2} \|v_k-x_0\|^2\\
		&\geq A_k f(x_k) + \frac{1}{2} \|v_k-x_0\|^2
	\end{aligned}
\end{equation}
And so:
\begin{equation}\label{eq:star}
	\begin{aligned}
		\min_x \Psi_{k+1}(x) &= \min_x \Psi_k + b_k \left( f(y_k)+ \langle \nabla , x-y_k \rangle \right)\\
		&\geq \min_x A_kf(x_k) + \frac{1}{2} \|v_k-x_0\|^2 + b_k \left( f(y_k)+ \langle \nabla , x-y_k \rangle \right)\\
		&\geq \min_x A_k \left( f(x_k) + \langle \nabla , x_k-y_k \rangle \right) + b_k \left( f(y_k)+ \langle \nabla , x-y_k \rangle \right)\\
		&\geq (A_k+b_k) f(y_k) + \langle \nabla f(y_k), A_kx_k+b_kx-A_{k+1}y_k \rangle + \frac{1}{2} \|v_k -x_0\|^2\\
		&\geq (A_{k+1}) f(y_k) + \langle \nabla f(y_k), A_kx_k+b_kx-A_{k+1}y_k \rangle + \frac{1}{2} \|v_k -x_0\|^2
	\end{aligned}
\end{equation}
To make things consistent, let us impose 
\begin{equation}
	A_k x_k - A_{k+1}y_k + b_kx = b_k(x-v_k) \Longleftrightarrow y_k = \frac{A_k}{A_{k+1}}x_k + \frac{b_k}{A_{k+1}}v_k
\end{equation}
And so we can continue equation \eqref{eq:star}:
\begin{equation}
	\begin{aligned}
		\min_{x\in \R^n} \Psi_{k+1}(x) &A_{k+1}\min_{x\in \R^n}\geq f(y_k) + \langle \nabla f(y_k), \gamma_k(x-v_k) \rangle + \frac{1}{2A_{k+1}\gamma_k^2} \|\gamma_k(v_k -x)\|^2\\
	\end{aligned}
\end{equation}
To verify the Lipschitz condition, we impose 
\begin{equation}
	\frac{1}{2A_{k+1}\gamma_k^2} =\frac{L}{2}  \Longleftrightarrow b_k^2 - \frac{1}{L} b_k - \frac{A_k}{L} = 0 \Longrightarrow b_k = \frac{1+\sqrt{1+4A_kL}}{2L}
\end{equation}
From all that have been computed previously, we can find a bound in terms of iterations needed. If $x^* = \arg\min f(x)$, we have 
\begin{equation}
	\begin{aligned}
		A_k f(x_k) &\le \min_{x \in \R^n} \Psi_k(x) &\leq \Psi_k(x^*)\le A_k f(x^*) + \frac{1}{2}\|x^*-x_k\|^2\\
		\Rightarrow A_k(f(x_k)-f(x^*)) &\leq \frac{1}{2}\|x^*-x_k\|^2\\
		\Rightarrow f(x_k)-f(x^*) &\leq \frac{1}{2A_k}\|x^*-x_k\|^2\\	
	\end{aligned}
\end{equation}
From the relation $A_{k+1} = A_k + b_k$ and the definition of $b_k$, we can show that $A_k\ge C(k-1)^2$ with $C>0$ and $k\ge2$. Thus, we get 
\begin{equation}
	f(x_k) - f(x^*) \le \frac{\|x_0-x^*\|^2}{2C(k-1)^2} = \mathcal{O}(1/k^2) \qquad \forall k\ge1
\end{equation}
A recap is given in algorithm \ref{alg:AGM}.\\
\begin{algorithm}[H]
	\caption{Accelerated Gradient Method}
	\label{alg:AGM}
	\begin{algorithmic}[1]
		\State \textbf{Input:} Given $x_0 \in \R^n$, define $\Psi_0(x)=\frac{1}{2}\|x-x_0\|^2$, $A_0=0$, $b_0=0$, $k=0$;
		\State Compute 
		\begin{equation}
			b_k = \frac{1+\sqrt{1+4A_kL}}{2L}>0;
		\end{equation}
		\State Set $\gamma_k = \frac{b_k}{A_{k+1}}\in (0,1]$ and compute $y_k = (1-\gamma_k)x_k + \gamma_k v_k$;
		\State Set 
		\begin{equation}
			x_{k+1} = \arg\min_{x \in \R^n} f(y_k) + \langle \nabla f(y_k),x-y_k\rangle + \frac{L}{2}\|x-y_k\|^2
		\end{equation}
		and $A_{k+1} = A_k + b_k$;
		\State Define 
		\begin{equation}
			\Psi_{k+1}(x) = \Psi_k(x) + b_k \left(f(y_k)+\langle \nabla f(y_k),x-y_k\rangle \right) \qquad \forall x \in \R^n
		\end{equation}
		and set 
		\begin{equation}
			v_{k+1} = \arg\min_{x \in \R^n} \Psi_{k+1}(x)
		\end{equation}
		\State $k \gets k+1$ and go back to Step 1;
	\end{algorithmic}
\end{algorithm}
\section{Accelerated Proximal Gradient Method}
In this section, we consider the minimisation of a function over a nonempty, closed and convex set $\Omega$. We decompose \textcolor{blue}{the objective function $F$ into a smooth and a possibly non smooth part}:
\begin{equation}
	\min_{x \in \Omega\subseteq \R^n} F(x)\equiv f(x) + \varphi(x)
\end{equation}
The accelerated proximal gradient method consists in using the proximal operator of the non smooth part $\varphi$ to define $x_{k+1}$:
\begin{algorithm}
	\caption{Accelerated Proximal Gradient Method}
	\label{alg:APGM}
	\begin{algorithmic}[1]
		\State \textbf{Input:} Given $x_0 \in domF$, define $\Psi_0(x)=\frac{1}{2}\|x-x_0\|^2$, $A_0=0$, $b_0=0$, $k=0$;
		\State Compute 
		\begin{equation}
			b_k = \frac{1+\sqrt{1+4A_kL}}{2L}>0;
		\end{equation}
		\State Set $\gamma_k = \frac{b_k}{A_{k+1}}\in (0,1]$ and compute $y_k = (1-\gamma_k)x_k + \gamma_k v_k$;
		\State Set 
		\begin{equation}
			x_{k+1} = Prox_{\frac{1}{L}\varphi}(y_k - \frac{1}{L}\nabla f(y_k))
		\end{equation}
		and $A_{k+1} = A_k + b_k$;
		\State Define 
		\begin{equation}
			\Psi_{k+1}(x) = \Psi_k(x) + b_k \left(f(y_k)+\langle \nabla f(y_k),x-y_k\rangle \right) \qquad \forall x \in \R^n
		\end{equation}
		and set 
		\begin{equation}
			v_{k+1} = \arg\min_{x \in \R^n} \Psi_{k+1}(x)
		\end{equation}
		\State $k \gets k+1$ and go back to Step 1;
	\end{algorithmic}
\end{algorithm}
\begin{thm}
	If $\{x_k\}_{k\ge0}$ is generated by the accelerated proximal gradient method, then 
	\begin{equation}
		F(x_k) - F(x^*) \le \frac{8L\|x_0-x^*\|^2}{(k-1)^2} \qquad \forall k\ge2 
	\end{equation}
\end{thm}
\chapter{Path following Interior Point Method}
\section{Self concordant functions}
\subsection{Definition}
\begin{definition}
	Given a convex function $f\in \mathcal{C}^3 (dom f)$, with $domf\subseteq \R^n$ open and convex, $f(\cdot)$ is said to be self-concordant with constant $M_f$ when 
	\begin{equation}
		\left| D^3 f(x)[u,u,u] \right| \le 2M_f \|u\|_x^3 \qquad \forall x\in domf \qquad \forall u\in \R^n
	\end{equation}
	where $\|u\|_x \coloneqq \sqrt{\langle \nabla^2 f(x)u,u\rangle}$.
\end{definition}
From this definition, we can derive two lemmas:
\begin{itemize}
	\item Let $f_1,f_2$ be self-concordant functions with constants $M_1$ and $M_2$ respectively. Then, given constants $\alpha,\beta>0$, the function $f=\alpha f_1 + \beta f_2$ is self-concordant with constant $M_f = \max\left\{\frac{M_1}{\sqrt{\alpha}}, \frac{M_2}{\sqrt{\beta}}\right\}$.
	\item Let $f(\cdot)$ be a self-concordant function with constant $M_f\ge 0$. Given $x,y\in domf$, we have 
	\begin{equation}
		\|y-x\|_y \ge \frac{\|y-x\|_x}{1+M_f \|y-x\|_x}
	\end{equation}
\end{itemize}
\subsection{With \texorpdfstring{$\mu$} --strongly convex}
As a reminder, a function $f$ is said to be $\mu$-strongly convex if
\begin{equation}
	\begin{aligned}
		f(y)&\ge f(x) + \langle \nabla f(x),y-x\rangle + \frac{\mu}{2}\|y-x\|^2 \qquad \forall x,y\in domf\\
		&\Rightarrow \langle \nabla f(x)-\nabla f(y),x-y\rangle \ge \mu \|x-y\|^2
	\end{aligned}
\end{equation}
Taking $y=x^*=\arg\min f(x)$, we find 
\begin{equation}
	\|\nabla f(x)\| \ge \mu \|x-x^*\| \qquad \forall x\in domf
\end{equation}
after using the Cauchy-Schwarz inequality. This implies that the norm of the gradient tends to 0 as $x$ approaches the minimizer $x^*$.\\
We can show that, for a self concordant function $f$ with constant $M_f$, given $x,y\in domf$, we have 
\begin{equation}\label{eq:sc}
	\langle \nabla f(y)-\nabla f(x),y-x\rangle \ge \frac{\|y-x\|_x^2}{1+M_f\|y-x\|_x}
\end{equation}
\begin{thm}
	Let $f(\cdot)$ be a self-concordant function with constant $M_f$. Consider $x_f^* = \arg\min_{x\in domf}f(x)$. Given $x\in domf$, with $\nabla^2 f(x)$ is nonsingular, we have 
	\begin{equation}\label{eq:sc2}
		\|x-x^*\|_x \le \frac{\|\nabla f(x)\|_x^*}{1-M_f\|\nabla f(x)\|_x^*}
	\end{equation}
	whenever $M_f\|\nabla f(x)\|_x^* < 1$, with $\|\nabla f(x)\|_x^* = \sqrt{\langle h, \nabla^{-2}f(x)h\rangle}$.
\end{thm}
\begin{itemize}
	\item [$\to$] Note: $|\langle h,u\rangle| \le \|h\|_x^* \|u\|_x$ if $\nabla^2 f(x)$ is nonsingular.
\end{itemize}
\subsection{Self-concordant barrier}
\begin{definition}
	Let $F(\cdot)$ be a self-concordant function with constant $M_f=1$. We say that $F(\cdot)$ is a $\nu$-self-concordant barrier for the set $\overline{dom F}$ when 
	\begin{equation}
		\langle \nabla F(x),u\rangle^2 \le \nu \langle \nabla^2F(x)u,u\rangle \qquad x\in domF\qquad \forall u\in \R^n
	\end{equation}
\end{definition}
The typical example is $F(x)=-\log(x)$. 
\begin{itemize}
	\item [$\to$] Note: If $F(\cdot)$ is a $\nu$-self-concordant barrier for the set $\overline{dom F}$, then $\langle \nabla F(x),y-x\rangle < \nu$ $\forall x,y \in dom F$.
	\item [$\to$] If, in addition, $\nabla^2F(x)$ is nonsingular, then $\|\nabla F(x)\|_x^*\le \sqrt{\nu}$.
\end{itemize}
\section{Path-following Interior-point Method}
Consider the optimization problem 
\begin{equation}\label{eq:problem}
	\min_{x\in \R^n} f_0(x)\equiv \langle c,x\rangle \quad x\in \Omega
\end{equation}
where $\Omega = \overline{domF}$ for some $\nu$-self-concordant barrier $F$ and it is bounded. From these assumptions, it follows from the Weierstra theorem that it has a solution $x^*$. \\

The barrier strategy consists in solving the problem iteratively by solving unconstrained optimization problems of the form
\begin{equation}
	\color{red}\boxed{\color{black}\min_{x\in domF}tf_0(x) + F(x)\qquad t>0}\color{black}
\end{equation}
Let us denote $f(t;x)\equiv t\langle c,x\rangle +F(x)$, and $x^*(t) = \arg\min_{x\in domF}f(t;x)$, which we call the central path function. Then, 
\begin{equation}\label{eq:central_path}
	\nabla_x f(t;x^*(t)) = tc + \nabla F(x^*(t)) = 0 \Longrightarrow c = -\frac{1}{t}\nabla F(x^*(t))
\end{equation}
Consequently, 
\begin{equation}
	f_0(x^*(t)) - f_0(x) = \langle c,x^*(t)-x\rangle = \frac{1}{t} \langle \nabla F(x^*(t)),x^*-x^*(t)\rangle  < \frac{\nu}{t}
\end{equation}
The last inequality following equation \eqref{eq:sc}. This means that 
\begin{equation}
	\lim_{t\to \infty} f_0(x^*(t)) = f_0(x^*)
\end{equation}
And in particular, for $\epsilon>0$, if $t\ge \nu \epsilon^{-1}$, then
\begin{equation}
	f_0(x^*(t)) - f_0(x^*) < \epsilon
\end{equation}
But, since $x^*(t)$ is not computable, one way to get an implementable method is to compute $\bar x(t)$ such that 
\begin{equation}\label{eq:beta}
	\|\nabla_x f(t;\bar x(t))\|_x^* \le \beta\qquad \beta \in (0,1)
\end{equation}
This implies 
\begin{equation}
	\begin{aligned}
		f_0(\bar x(t)) - f_0(x^*) &= f_0(\bar x(t)) - f_0(x^*(t)) -(f_0(x^*) - f_0(x^*(t)))\\
		& < \frac{\nu}{t} + f_0(\bar x(t)) - f_0(x^*(t))\\
		& = \frac{\nu}{t} + \frac{1}{t} \langle tc, \bar x(t)-x^*(t)\rangle\\
		& = \frac{\nu}{t} + \frac{1}{t} \langle \nabla_x f(t;\bar x(t))-\nabla F(\bar x(t)), \bar x(t)-x^*(t)\rangle\\
	\end{aligned}
\end{equation}
To get to the next line, we use the Cauchy-Schwarz and triangular inequalities:
\begin{equation}
	\le \frac{\nu}{t} + \frac{1}{t} \left[ \|\nabla_x f(t;\bar x(t))\|_x^* + \|\nabla F(\bar x(t))\|_x^* \right] \|\bar x(t)-x^*(t)\|_x
\end{equation}
From equations \eqref{eq:beta} and \eqref{eq:sc2}, and a property of self-concordant barriers, this means that 
\begin{equation}
	f_0(\bar x(t)) - f_0(x^*) < \frac{\nu}{t} + \frac{1}{t}(\beta +\sqrt{\nu}) \underbrace{\frac{\|\nabla_xf(t;\bar x(t))\|_x^*}{1-\|\nabla_xf(t;\bar x(t))\|_x^*}}_{\eqqcolon \omega(\|\nabla_xf(t;\bar x(t))\|_x^*)}
\end{equation}
where $\omega(x) = \frac{x}{1-x}$ is a monotone increasing function, meaning that 
\begin{equation}
	\omega(\beta) > \omega(\|\nabla_xf(t;\bar x(t))\|_x^*)
\end{equation}
and thus 
\begin{equation}\label{eq:bounding}
	f_0(\bar x(t)) - f_0(x^*) < \frac{1}{t}\left(\nu + (\beta +\sqrt{\nu}) \frac{\beta}{1-\beta}\right)
\end{equation}
\section{Intermediate Newton method}
Let us consider the problem \eqref{eq:problem}, and let $\hat f(\cdot)$ be a self-concordant function with constant $M_{\hat f} = 1$. Consider $x\in dom\hat f$ with \textcolor{blue}{$\nabla^2\hat f(x)$ nonsingular}. Assume that \textcolor{blue}{$\|\nabla \hat f(x)\|_x^* \le \tau$ with $\tau+\tau^2 +\tau^3\le 1$}. The iterate of the intermediate Newton method is given by 
\begin{equation}
	x^+ = x - \frac{1}{1+\xi} \nabla^{-2}\hat f(x)\nabla \hat f(x) \qquad \xi = \frac{(\|\nabla \hat f(x)\|_x^*)^2}{1+\|\nabla \hat f(x)\|_x^*}
\end{equation}
Then, $x^+\in dom\hat f$ and 
\begin{equation}\label{eq:bernadette}
	\|\nabla \hat f(x^+)\|_{x^+}^* \le \tau^2\left(1+\tau+\frac{\tau}{1+\tau+\tau^2}\right)
\end{equation}
Consider now the function $f(t;x) \equiv t\langle c,x\rangle + F(x)$, a self-concordant function with constant $M_f=1$. The gradient and hessian are 
\begin{equation}
	\nabla_x f(t;x) = tc + \nabla F(x) \qquad \nabla^2_x f(t;x) = \nabla^2F(x)
\end{equation}
Let us define the iterate $t^+ = t + \frac{\gamma}{\|c\|_x^*}$ with $\gamma>0$. The iterate of the intermediate Newton method becomes
\begin{equation}
	x^+ = x - \frac{1}{1-\xi}\nabla^{-2}_xf(t^+;x)\nabla_xf(t^+; x) = x-\frac{1}{1+\xi}\nabla^{-2}F(x)(t^+ c + \nabla F(x))
\end{equation}
As previously, suppose that \textcolor{blue}{$\|\nabla_x f(t;x)\|_x^* \le \beta$}. Then, 
\begin{equation}
	\begin{aligned}
		\|\nabla_xf(t^+;x)\|_x^* & = \|t^+c + \nabla F(x)\|_x^* = \|t^+c-tc+tc + \nabla F(x)\|_x^*\\
		& \le (t^+-t)\|c\|_x^* + \|\nabla_xf(t;x)\|_x^* = \gamma +\beta\\
	\end{aligned}
\end{equation}
This inequality is derived using the hypothesis and the definition of $t^+$. This means that, \textcolor{blue}{choosing $\gamma \le \tau - \beta$ for $\tau+\tau^2+\tau^3\le 1$}, we get 
\begin{equation}
	\| \nabla_x f(t^+;x)\|_x^* \le \tau
\end{equation}
By equation \eqref{eq:bernadette}, we have
\begin{equation}
	\|\nabla_x f(t^+; x^+)\|_{x^+}^* \le \tau^2\left(1+\tau+\frac{\tau}{1+\tau+\tau^2}\right) = \frac{\tau^2(1+\tau)}{1-\tau^3}
\end{equation}
And so taking $\beta = \tau^2 \left(1+\tau+\frac{\tau}{1+\tau+\tau^2}\right)$ seems reasonable. 
\begin{itemize}
	\item [$\to$] Note: notice that $\tau>\beta$ for every $\tau\in (0,1/2]$ and verifies $\tau+\tau^2+\tau^3\le 1$.
\end{itemize}
From all those inequalities and properties, we can derive an algorithm. 
\section{Path-following Interior point Algorithm}
\subsection{Algorithm}
\begin{algorithm}[H]
	\caption{Path-following Interior Point Algorithm}
	\label{alg:PFIP}
	\begin{algorithmic}[1]
		\State \textbf{Input:} Given $\tau\in (0,1/2]$, define $\beta = \tau^2\left(1+\tau+\frac{\tau}{1+\tau+\tau^2}\right)$. Choose $0<\gamma\le \tau-\beta$. Find $x_0\in domF$ such that $\|\nabla F(x_0)\|_{x_0}^* \le \beta$ and set $t_0=0$ and $k\coloneqq0$;
		\State \textbf{Step 1: }Compute 
		\begin{equation}
			\begin{aligned}
				t_{k+1} &= t_k + \frac{\gamma}{\|c\|_x^*}\\
				x_{k+1} &= x_k - \frac{1}{1+\xi_k}\nabla^{-2}F(x_k)(t_{k+1}c+\nabla F(x_k))\\
				\xi_k &= \frac{(\|\nabla f(t_k;x_k)\|_{x_k}^*)^2}{1+\|\nabla f(t_k;x_k)\|_{x_k}^*}
			\end{aligned}
		\end{equation}
		\State \textbf{Step 2:} $k\gets k+1$ and go back to Step 1.
	\end{algorithmic}
\end{algorithm}
\subsection{Complexity bound}
Notice that, by construction, $\|\nabla_xf(t_k;x_k)\|_{x_k}^*\le \beta$, $\forall k\ge 0$, and so 
\begin{equation}
	t_k \|c\|_{x_k}^* = \|\nabla_x f(t_k;x_k)-\nabla F(x_k)\|_{x_k}^* \le \beta +\sqrt{\nu}
\end{equation}
This can be used to bound $t_{k+1}$: 
\begin{equation}
	t_{k+1}-t_k = \frac{\gamma}{\|c\|_{x_k}^*}\ge \frac{\gamma t_k}{\beta+\sqrt{\nu}} \Longleftrightarrow \left(1+\frac{\gamma}{\beta+\sqrt{\nu}}\right)t_k \qquad \forall k\ge 0
\end{equation}
Thus,
\begin{equation}\label{eq:t_k}
	t_k \ge \left(1+\frac{\gamma}{\beta+\sqrt{\nu}}\right)^{k-1}t_1 = \left(1+\frac{\gamma}{\beta+\sqrt{\nu}}\right)^{k-1} \frac{\gamma}{\|c\|_{x_0}^*}
\end{equation}
Combining this to \eqref{eq:bounding}, it follows that 
\begin{equation}
	\begin{aligned}
		f_0(x_k)-f_0^* &\le \frac{1}{t_k}\left(\nu+\frac{(\beta+\sqrt{\nu})\beta}{1-\beta}\right)\\
		& \le \frac{\|c\|_{x_0}^* \left(\nu +\frac{(\beta+\sqrt{\nu})\beta}{1-\beta}\right)}{\gamma \left(1+\frac{\gamma}{\beta+\sqrt{\nu}}\right)^{k-1}}
	\end{aligned}
\end{equation}
Thus, to obtain a point $x_k$ with $f_0(x_k)-f_0^* \le \epsilon$, it is sufficient to have 
\begin{equation}
	\begin{aligned}
		\frac{\|c\|_{x_0}^*\left(\nu + \frac{(\beta+\sqrt{\nu})\beta}{1-\beta}\right)}{\gamma\left(1+\frac{\gamma}{\beta+\sqrt{\nu}}\right)^{k-1}}&\le \epsilon \\
		(k-1) \ln\left(1+\frac{\gamma}{\beta+\sqrt{\nu}}\right) &\ge \ln \left(\frac{\|c\|_{x_0}^* \left(\nu +\frac{(\beta+\sqrt{\nu})\beta}{1-\beta}\right)}{\gamma}\epsilon^{-1}\right)\\
		&\Longrightarrow \color{red}\boxed{\color{black}k \ge \mathcal{O}(\epsilon^{-1})}\color{black}
	\end{aligned}
\end{equation}
Notice that $\ln(1+x)\ge cx$ for $x>0$ and $c$ a constant \textcolor{red}{TO BE CHECKED}. We can apply it to $x=\frac{\gamma}{\beta+\sqrt{\nu}}$ to find a bound on the number of iterations: we will have $f_0(x_k)-f_0^* \le \epsilon$ whenever
\begin{equation}
	(k-1)c\left(\frac{\gamma}{\beta+\sqrt{\nu}}\right)\ge \ln \left(\|c\|_{x_0}^* \left(\nu + \frac{(\beta+\sqrt{\nu})\beta}{1-\beta}\right)\gamma^{-1} \epsilon^{-1}\right)
\end{equation}
Therefore, to find a $\epsilon$-approximate solution of problem \eqref{eq:problem}, the algorithm \ref{alg:PFIP} takes no more than $\mathcal{O}(\sqrt{\nu}\ln(\epsilon^{-1}))$ iterations.
\subsection{Example}
Consider the following problem:
\begin{equation}
	\begin{aligned}
		\min_{x\in \R^n} q_0(x)&\equiv c_0 + \langle b_0,x\rangle + \frac{1}{2}\langle A_0x,x\rangle \\
		\text{s.t.}\quad  q_i(x)&\equiv c_i + \langle b_i,x\rangle + \frac{1}{2}\langle A_ix,x\rangle \le \beta_i \qquad i=1,\dots,m
	\end{aligned}
\end{equation}
where $A_i=A_i^T \succeq 0$ for $i=0,\dots,m$. To be able to used the algorithm derived previously, we need to change the objective function:
\begin{equation}\label{eq:new_prob}
	\min_{(x,\beta)\in \R^n\times \R} \beta_0 \equiv f_0(x,\beta_0) \quad \text{ s.t. }\quad q_i(x) \le \beta_i \qquad i=0,\dots,m
\end{equation}
The feasible set of this problem is the closure of the domain of the following self-concordant barrier, with constant $\nu=m+1$:
\begin{equation}
	F(x,\beta_0) = -\sum_{i=0}^m \ln(\beta_i-q_i(x))
\end{equation}
From the complexity of algorithm \ref{alg:PFIP}, it takes at most $\mathcal{O}\left(\sqrt{m+1}\ln (\epsilon^{-1})\right)$ iterations to find $x_k$ such that 
\begin{equation}
	f_0(x_k,\beta_{0,k}) - f_0^* \le \epsilon
\end{equation}
and the operation complexity multiplies it by $\mathcal{O}(m^3)$ because it solves a linear system at each iteration. 
\chapter{Tips and Tricks}\label{chap:tricks}
\begin{enumerate}
	\item $\mu$-strongly convex function:
	\begin{equation}
		f(y) \ge f(x) + \langle \nabla f(x),y-x\rangle + \frac{\mu}{2}\|y-x\|^2 \qquad \forall x,y\in \R^n
	\end{equation}
	\item Approximation of the max:
	\begin{equation}\label{eq:approx_max}
		\begin{aligned}
			\max\{z,0\} = \frac{z+|z|}{2} = \frac{z + \sqrt{z^2}}{2} \approx \frac{z + \sqrt{z^2 + \delta}}{2}
		\end{aligned}	
	\end{equation}
	\item \begin{equation}\label{eq:sq1}
		ab \leq \frac{a^2+b^2}{2}	
	\end{equation}
	\item \begin{equation}\label{eq:sq2}
		(a+b)^2 \leq 2a^2+2b^2
	\end{equation}
	\item V-trick:
	\begin{equation}\label{eq:vtrick}
		\langle xv,v\rangle \leq \|x\|\|v\|^2
	\end{equation}
	\item Triangular inequality by the minimizer: 
	\begin{equation}\label{eq:triangular_inequality_by_min}
		\|x_{k+1}-x_k\| \leq \|x_{k+1}-x^*\|+\|x_k-x^*\|
	\end{equation}
	\item Mean Value Theorem $\forall x,y \in \Omega, \exists z \in \Omega$ s.t.:
	\begin{equation}
		f(y)-f(x) = \langle \nabla f(z),y-x\rangle \qquad z \in [x,y]
	\end{equation}
	\item By definition if a function is $C^p_M$, then 
	\begin{equation}
		|f(y) - T_p(y;x)| \leq \frac{M}{(p+1)!}\|y-x\|^{p+1}
	\end{equation}
	\item Fundamental theorem of calculus:	
	\begin{equation}
		\nabla f(y) - \nabla f(x) = \int_0^1 \nabla^2 f(x + \tau(y-x))(y-x)d\tau
	\end{equation}
	\item Cauchy-Schwarz inequality;
	\item Triangular inequality;
	\item Dimis-Mori condition for Quasi Newton SR1:
	\begin{equation}
		\lim_{k\to \infty} \frac{\|\left(B_k^{-1}-\nabla^2f(x_k)\right)(x_{k+1}-x_k)\|}{\|x_{k+1}-x_k\|} = 0
	\end{equation}
	\item KKT conditions:
	\begin{equation}
	\begin{cases}
		\nabla f(x^*) - \displaystyle \sum_{i=1}^m \lambda_i^* \nabla c_i(x^*) = 0  \quad \quad &\textcolor{blue}{\text{(stationarity)}}\\
		c(x^*) = 0 &\textcolor{blue}{\text{(feasibility)}}
	\end{cases}
	\end{equation}
	\item For a function $f\in \C^{2,2}_M$, 
	\begin{equation}
		\left|f(y)-f(x)-\nabla f(x)^T (y-x)-\frac{1}{2}(y-x)^T \nabla^2f(x)(y-x)\right| \le \frac{M}{6}\|y-x\|^3
	\end{equation}
\end{enumerate}
\chapter{Final results and important theorems}
\section{TODO}
\begin{itemize}
	\item Generalisation the property of a L-Lipschitz function to $f\in \C_L^p(\R^n)$. For $p=1$, we had 
	\begin{equation}
		f(y)\le f(x_k) + \langle \nabla f(x_k),y-x_k\rangle + \frac{L}{2}\lVert y-x_k\rVert^2 \qquad \forall y\in \R^n 
	\end{equation}
	For a general value of $p$, it becomes 
	\begin{equation}
		f(y) \le T_p(y;x_k) + \frac{L}{(p+1)!}\lVert y-x_k\rVert^{p+1} \qquad \forall y\in \R^n 
	\end{equation}
	\item Gradient method of order $p$: To solve $\min_{x\in \R^n} f(x)$, we can use the iteration 
	\begin{equation}
		x_{k+1} = \arg\min_{y\in \R^n} T_p(y;x_k)+\frac{M}{(p+1)!}\lVert y-x_k\rVert^{p+1}
	\end{equation}
	where the constant $M$ is an approximation of the Lipschitz constant $L$. 
	\item Bound on the number of iterations of the $p$-th order gradient method:
	\begin{equation}
		T(\varepsilon) \le 1 + \frac{f(x_0)-f_{low}}{C(L)} \varepsilon^{-\frac{p+1}{p}} \equiv \O\left(\varepsilon^{-\frac{p+1}{p}}\right) \qquad C(L) = \frac{L}{(p+1)!}\left(\frac{p!}{L+M}\right)^{\frac{p+1}{p}}
	\end{equation} 
\end{itemize}
\section{Gradient descent without gradient}
We want to minimize a function $f$ without computing its gradient.
\begin{equation}
	x_{k+1} = x_k - \frac{1}{\sigma} g_{h_k}(x_k) \quad \quad \quad h_k > 0, \: \sigma > 0
\end{equation}
where $g_{h_k}(x_k) \in \R^n$ is given by:
\begin{equation}
	[g_{h_k}(x_k)]_j = \frac{f(x_k+he_j)-f(x_k)}{h_k} \quad \quad \quad \forall j\in [1,\dots,m]
\end{equation}
\textcolor{blue}{Suppose that $f\in \C^1_L(\R^n)$}. Then,
\begin{equation}
	\|\nabla f(x_k) - g_{h_k}(x_k)\| \leq \frac{L \sqrt{n}}{2} h_k
\end{equation}
And the convergence rate is 
\begin{equation}
	\Longrightarrow T(\varepsilon) \leq 8 \sigma \left( f(x_0) - f_{low} + \frac{5 \sigma n}{16} \sum_{k=0}^{T(\varepsilon)-1} h_k^2\right) \varepsilon^2 = \O(\varepsilon^2)
\end{equation}
\section{Local rates of convergence}
\subsection{Linear rate of GM}
As a reminder, the gradient method follows the iterate 
\begin{equation}
	x_{k+1} = x_k - \frac{1}{L}\nabla f(x_k)
\end{equation}
We define in some proves the quantity $G_k$ as 
\begin{equation}
	G_k = \int_0^1 \nabla^2 f(x^*+\tau(x_k-x^*))d\tau
\end{equation}
The local convergence rate, i.e. for iterates such that $\|x_k -x^*\| \leq \frac{\mu}{M}$, of the gradient method is linear:
\begin{equation}
	\begin{aligned}
		\|x_k - x^*\| &\leq \left(1-\frac{\mu}{2L}\right)^k \|x_0-x^*\| \quad \forall k \geq 0\\
		T(\varepsilon)&\leq 1+ \frac{\log \left(\|x_0 - x^*\|\varepsilon^{-1}\right)}{|\log \left(1-\frac{\mu}{2L}\right)|} \equiv \mathcal{O}(\log(\varepsilon^{-1}))
	\end{aligned}
\end{equation}
\subsection{Local quadratic convergence of Newton's method}
As a reminder, the Newton's method follows the iterate
\begin{equation}
	x_{k+1} = x_k - \nabla^{-2}f(x_k)\nabla f(x_k)	
\end{equation}
The local convergence rate, i.e. for iterates such that $\|x_k -x^*\| \leq \frac{\mu}{\textcolor{red}{2}M}$, of the Newton's method is quadratic:
\begin{equation}
	\begin{aligned}
		\|x_{k+1}-x^*\| &\leq \frac{M}{\mu}\|x_k-x^*\|^2\\
		T(\varepsilon) &\leq \log_2(\log_2(\frac{\mu}{M} \varepsilon^{-1}))
	\end{aligned}
\end{equation}
\subsection{Quasi Newton methods}
\subsubsection{SR1 Update}
As a reminder, the SR1 update is given by
\begin{equation}
	x_{k+1} = x_k - B_k\nabla f(x_k) \qquad B_k = B_{k-1} + \frac{\left(s_{k-1}-B_{k-1} y_{k-1}\right)\left(s_{k-1}-B_{k-1} y_{k-1}\right)^T}{\left(s_{k-1} - B_{k-1} y_{k-1}\right)^T y_{k-1}}
\end{equation}
where $B_k$ is found using Dimis-Mori and the secant condition:
\begin{equation}
	\begin{aligned}
		\lim_{k\to \infty} \frac{\|\left(B_k^{-1}-\nabla^2f(x_k)\right)(x_{k+1}-x_k)\|}{\|x_{k+1}-x_k\|} = 0\\
		B_k y_{k-1} = s_{k-1}\\
	\end{aligned}
\end{equation}
\subsubsection{BFGS Update}
BFGS uses an approximation of the hessian instead of its inverse:
\begin{equation}
	H_k = B_k^{-1} = \nabla^2 f(x_k)
\end{equation}
The secant condition becomes 
\begin{equation}
	H_{k+1}s_k = y_k
\end{equation}
The idea is to use a rank 2 update:
\begin{equation}
	H_{k+1} = H_k + auu^T + bvv^T \qquad a,b\in \R \qquad u,v\in \R^n 
\end{equation}
The update is equation \eqref{eq:BFGS_update}.
\section{Constrained nonlinear programming problems}
Consider the constrained problem 
\begin{equation}
	\min_{x \in \R^n} f(x) \quad \text{subject to} \quad c_i(x) = 0, \quad i \in \{1,\dots,m\}	
\end{equation}
where $f,c_i : \R^n \to \R$ are $\C^1$ and there exists at least a $\hat{x}$ such that $c_i(\hat{x}) = 0$.\\
To do that, we use a quadratic penalty term:
\begin{equation}
	\min_{x \in \R^n} Q_{\sigma} (x) \equiv f(x) + \frac{\sigma}{2} \|c(x)\|_2^2
\end{equation}
The initial problem is constrained. We suppress those constraints by adding their norm in the objective function, with a parameter $\sigma$. We define $\varepsilon_1$ as the tolerance on the norm of the constraints, and $\varepsilon_2$ as the tolerance on the objective function. The concept of the algorithm is to solve the unconstrained problem, and increase $\sigma$ until the constraints are satisfied with tolerance $\varepsilon_1$. 
\section{Accelerated Gradient Method}
\subsection{Derivation of the algorithm}
This algorithm minimizes a convex function $f$ with $L$-Lipschitz gradient. The method combines past and present information for the step of the gradient method. 
\begin{equation}
	\begin{cases}
		y_k = (1-\gamma_k)x_k + \gamma_k v_k, \quad \gamma_k \in (0,1)\\
		x_{k+1} = x_k - \frac{1}{L} \nabla f(y_k)
	\end{cases}
\end{equation}
where $v_k$ and $\gamma_k$ are defined in the algorithm \ref{alg:AGM}. The method is based on the following inequalities, which allow to have the convergence rate that we want:
\begin{equation}
	\begin{aligned}
		&v_k = \arg \min_{x \in \R^n} \Psi_k (x)\\
		&\Psi_k (x) \leq A_k f(x) + \frac{1}{2} \|x-x_0\|^2\\
		&A_k f(x_k) \leq \min_{x \in \R^n} \Psi_k (x) \equiv \Psi_k^*, \quad A_k \geq 0\\
		&A_k \geq c(k-1)^2 \quad \forall k \geq 2
	\end{aligned}
\end{equation}
This convergence rate is 
\begin{equation}
		\left(f(x_k)-f(x^*)\right) \leq \frac{\|x_k-x^*\|^2}{2 C(k-1)^2} = \O(k^{-2}) = \O(\varepsilon^{-1/2}) \quad \forall k \geq 2
\end{equation}
\subsection{Accelerated Proximal Gradient Method}
In this algorithm, the function to minimise is defined over a nonempty, closed and convex set $\Omega$. The function has a smooth part $f(\cdot)$ and a possibly nonsmooth part $\phi(\cdot)$. 
\begin{equation}
	\min_{x \in \Omega\subseteq \R^n} F(x)\equiv f(x) + \varphi(x)
\end{equation}
The APGM consists in changing the iterate $x_{k+1}$ to 
\begin{equation}
	x_{k+1} = Prox_{\frac{1}{L}\phi}\left(y_k - \frac{1}{L}\nabla f(y_k)\right)
\end{equation}
The convergence rate is 
\begin{equation}
	F(x_k)-F(x^*) \le \frac{8L\|x_0-x^*\|^2}{(k-1)^2} \qquad \forall k\ge 2
\end{equation}
\section{Path following Interior Point Method}
\subsection{Self-concordant functions}
We have 2 lemmas:
\begin{itemize}
	\item Let $f_1,f_2$ be self-concordant functions with constants $M_1$ and $M_2$ respectively. Then, given constants $\alpha,\beta>0$, the function $f=\alpha f_1 + \beta f_2$ is self-concordant with constant $M_f = \max\left\{\frac{M_1}{\sqrt{\alpha}}, \frac{M_2}{\sqrt{\beta}}\right\}$.
	\item Let $f(\cdot)$ be a self-concordant function with constant $M_f\ge 0$. Given $x,y\in domf$, we have 
	\begin{equation}
		\|y-x\|_y \ge \frac{\|y-x\|_x}{1+M_f \|y-x\|_x}
	\end{equation}
\end{itemize}
For a self concordant function $f$ with constant $M_f$, given $x,y\in domf$, we have 
\begin{equation}
	\langle \nabla f(y)-\nabla f(x),y-x\rangle \ge \frac{\|y-x\|_x^2}{1+M_f\|y-x\|_x}
\end{equation}
\begin{thm}
	Let $f(\cdot)$ be a self-concordant function with constant $M_f$. Consider $x_f^* = \arg\min_{x\in domf}f(x)$. Given $x\in domf$, with $\nabla^2 f(x)$ is nonsingular, we have 
	\begin{equation}
		\|x-x^*\|_x \le \frac{\|\nabla f(x)\|_x^*}{1-M_f\|\nabla f(x)\|_x^*}
	\end{equation}
	whenever $M_f\|\nabla f(x)\|_x^* < 1$, with $\|\nabla f(x)\|_x^* = \sqrt{\langle h, \nabla^{-2}f(x)h\rangle}$.
\end{thm}
\subsection{Path-following Interior Point Method}
We consider the optimization problem 
\begin{equation}
	\min_{x\in \R^n} f_0(x)\equiv \langle c,x\rangle \quad x\in \Omega
\end{equation}
where $\Omega = \overline{domF}$ for some self-concordant barrier $F$. The method consists in solving 
\begin{equation}
	\min_{x\in domF}tf_0(x) + F(x)\qquad t>0
\end{equation}
With this method, 
\begin{equation}
	f_0(\bar x(t)) - f_0(x^*) < \frac{1}{t}\left(\nu + (\beta +\sqrt{\nu}) \frac{\beta}{1-\beta}\right)
\end{equation}
\subsection{Intermediate Newton method}
The iterate of the intermediate Newton method for a self-concordant function $\hat f$ with $M_{\hat f} = 1$ is 
\begin{equation}
	x^+ = x - \frac{1}{1+\xi} \nabla^{-2}\hat f(x)\nabla \hat f(x) \qquad \xi = \frac{(\|\nabla \hat f(x)\|_x^*)^2}{1+\|\nabla \hat f(x)\|_x^*}
\end{equation}
This is used in algorithm \ref{alg:PFIP} to solve the problem. The complexity bound is 
\begin{equation}
	k\ge \mathcal{O}(\sqrt{\nu}\ln(\epsilon^{-1}))
\end{equation}
\end{document}