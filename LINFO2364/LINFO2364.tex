\documentclass[12pt, openany]{report}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage[a4paper,left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage[english]{babel}
\usepackage{libertine}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{float}
\usepackage{enumitem}
\usepackage{pythonhighlight}
\usepackage[]{titletoc}
\usepackage{empheq}
\usepackage{titlesec}
\usepackage{mathpazo}
\usepackage{xfrac}
\usepackage{textcomp}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{tabularray}
\usepackage{subcaption}
\usepackage[bottom]{footmisc}
\usepackage{pdfpages}
\usepackage{tabularx}
\usepackage{amsthm}
\usepackage{listings}
\usepackage[skins]{tcolorbox}
\titleformat{\chapter}[display]
  {\normalfont\bfseries}{}{0pt}{\Huge}
\usepackage{hyperref}
\newcommand{\hsp}{\hspace{20pt}}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\theoremstyle{definition}
\newtheorem{thm}{Theorem}[chapter]
\newtheorem{definition}[thm]{Definition}
\newtheorem{lem}[thm]{Lemma}


\definecolor{mGreen}{rgb}{0,0.6,0}
\definecolor{mGray}{rgb}{0.5,0.5,0.5}
\definecolor{mPurple}{rgb}{0.58,0,0.82}
\definecolor{backgroundColour}{rgb}{0.95,0.95,0.92}
\definecolor{light-gray}{gray}{0.95}
\newcommand{\code}[1]{\colorbox{light-gray}{\texttt{#1}}}

\lstdefinestyle{CppStyle}{
    backgroundcolor=\color{backgroundColour},   
    commentstyle=\color{mGreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{mGray},
    stringstyle=\color{mPurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    language=C
}

\hbadness=100000
\begin{document}
\begin{titlepage}
    \begin{sffamily}
    \begin{center}
        \includegraphics[scale=1]{img/page_de_garde.png} \\[1cm]
        \HRule \\[0.4cm]
        { \huge \bfseries LINFO2364 Mining Patterns in Data \\[0.4cm] }
    
        \HRule \\[1.5cm]
        \textsc{\LARGE Issambre L'Hermite Dumont}\\[3cm]
        {This summary may not be up-to-date, the newer version is available at this address: \hyperlink{https://github.com/SimonDesmidt/Syntheses}{https://github.com/SimonDesmidt/Syntheses}}
        \vfill
        \vspace{2cm}
        {\large Academic year 2025-2026 - Q2}
        \vspace{0.4cm}
         
        \includegraphics[width=0.15\textwidth]{img/epl.png}
        
        UCLouvain\\
    
    \end{center}
    \end{sffamily}
\end{titlepage}

\setcounter{tocdepth}{1}
\tableofcontents
\chapter{Introduction}
In our data-driven world, the ability to extract meaningful information from vast datasets is crucial. Understanding all the aspect of this discipline is essential to derive those meaningful information, and this is the goal of this course. First, we need to define some key concepts.
\section{Definitions}
\begin{definition}
    A pattern is a recurring structure in a dataset.
\end{definition}
Patterns can be simple or complex, relevant or irrelevant. Their advantages is that they are interpretable. When found, relevant patterns can be used to make predictions, to understand the underlying structure of the data, and to make informed decisions.  
\begin{definition}
	Data mining is the process of discovering interesting patterns, models, and other kinds of knowledge in large data sets.
\end{definition}
\subsection{Type of data}
We can mine data out of various types of structure of data:
\begin{itemize}
    \item \textbf{Tabular data}: Data is organized in rows and columns. Example: spreadsheets, databases.
    \item \textbf{Sequences}: Data points are ordered in a sequence. Example: DNA sequences, text data.
    \item \textbf{Graphs, trees, networks}: Data is represented as nodes and edges. Example: social networks, web graphs.
    % \item \textbf{Transactional data}: Each record is a transaction, which is a set of items. Example: market basket data.
    % \item \textbf{Relational data}: Data is organized in tables with relationships between them. Example: customer databases.
    % \item \textbf{Time-series data}: Data points are collected over time. Example: stock prices, sensor data.    
    % \item \textbf{Spatial data}: Data with geographical or spatial components. Example: maps, satellite images.
\end{itemize}
Those structures can be discretes, continuous, enumerable data, etc. Those structures, can be combined to form more complex data types. And they can be highly structured, semi-structured, or unstructured.
\begin{definition}
    Highly structured data are relational databases, with uniform record or table-like structures, with a fixed set of well-defined attributes. This is rarely the case in real-world data.
\end{definition}
\begin{definition}
    Semi-structured data are not as structured as in relational databases, but presents some structure with clearly defined semantic meaning. For example:
    \begin{itemize}
        \item Transactional dataset: structured into transactions, but each transaction is an unstructured set of values
        \item Sequence data set: unstructured collection of ordered sequences of values
        \item Graphs: set of nodes connected by a set of edges, with edges labelled given some semantic
    \end{itemize}
\end{definition}
\begin{definition}
    Unstructured data have no predefined structure or organization. For example: text documents, images, audio files, videos.
\end{definition}
Those requires advanced techniques to extract patterns, like deep learning or domain-specific methods.
We can also categorize data based on the way they are generated:
\begin{itemize}
    \item \textbf{Stored data}: Data is collected and stored in a finite set.
    \item \textbf{Streamed data}: Data is continuously generated and updated over time. Example: video surveillance, etc.
\end{itemize}
\subsection{Types of data mining}
Techniques and algorithms may vary depending on the data but also on way we will use mined patterns. We can categorize data mining tasks into two main types:
\begin{itemize}
    \item \textbf{Descriptive data mining}: finds patterns that characterize the properties of the data. 
    \item \textbf{Predictive data mining}: finds patterns that can be used to make predictions by induction.
\end{itemize}
We can identify patterns by multiples ways:
\begin{itemize}
    \item \textbf{Frequent patterns}: patterns that identify a recurring structure in the data.
    \item \textbf{Associations patterns}: patterns that show rules of implications between different attributes
    \item \textbf{Correlations}: patterns that has positive or negative correlation between different attributes.
\end{itemize}
We can uses patterns for predictions:
\begin{itemize}
    \item \textbf{Classification}: assign new data to classes based on similarities with historic data.
    \item \textbf{Regression}: predict numerical values based on the new data and the historic data.
    \item \textbf{Feature selection}: identify the most relevant features.
\end{itemize}
\chapter{Preprocessing}
To analyse data, and retrieve meaningful patterns, data often needs to be preprocessed. This step is crucial as raw data is often incomplete, inconsistent, and noisy. Preprocessing improves the quality of the data and the efficiency of the mining algorithms.
\section{Useful statistical values}
\begin{definition}
    The mean (or average) of a dataset is the sum of all values divided by the number of values.
    \begin{equation}
        \bar{x} = \frac{1}{N} \sum_{i=1}^{N} x_i
    \end{equation}
\end{definition}
\begin{definition}
    The midrange is the average of the largest and smallest values in the set.
\end{definition}
\begin{definition}
    The variance of a dataset measures is defined like this:
    \begin{equation}
        \sigma^2 = \frac{1}{N} \sum_{i=1}^{N} (x_i - \bar{x})^2
    \end{equation}
\end{definition}
\begin{definition}
    The standard deviation of a dataset is the square root of the variance:
    \begin{equation}
        \sigma = \sqrt{\sigma^2} = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (x_i - \bar{x})^2}
    \end{equation}
\end{definition}
\begin{definition}
    The mode for a set of data is the value that occurs most frequently. When there are multiple values with the same highest frequency, the dataset can be unimodal, bimodal, trimodal or multimodal.
\end{definition}
\begin{definition}
    Quantiles are points taken at regular intervals of a data distribution, dividing it into essentially equal-sized consecutive sets. The $k$th $q$-quantile for a given data distribution is the value $x$ such that at most $\frac{k}{q}$ of the data values are less than $x$ and at most $\frac{q-k}{q}$ of the data values are more than $x$, where $k$ is an integer such that $0 < k < q$. There are $q-1$ $q$-quantiles.
\end{definition}
Usually we use quartiles ($q=4$), where $Q_2$ is the median, or percentiles ($q=100$).
\section{Data plot}
Plotting the data can be useful to observe some characteristics of the dataset. we can represent the data under multiples forms:
\begin{itemize}
    \item \textbf{Scatter plot}, each pair of values is treated as a pair of coordinates in an algebraic sense and plotted as points in the plane. It helps visually determine whether there is a relationship, pattern, or trend between the two numeric attributes.
    \item \textbf{Histogram} is a bar chart representing the count of values present in each bucket. The buckets can be defined for each values, for ranges of values (equal-width) or for buckets containing the same number of values (equal-frequency).
    \item \textbf{Boxplots} are a popular way of visualizing a distribution. Typically, the ends of the box are at the quartiles so that the box length is the interquartile range. The median is marked by a line within the box. Two lines (called whiskers) outside the box extend to the smallest (minimum) and largest (maximum) observations. To highlight outliers, the whiskers are usually extended to extreme low and high observations only if these values are less than 1.5 times the interquartile range beyond the quartiles. Otherwise, the whiskers terminate at the most extreme observations occurring within 1.5 times the interquartile range.
    \item \textbf{Quantile-quantile plot}, or q-q plots are graphs that plot the quantiles of one univariate distribution against the corresponding quantiles of another.
\end{itemize}

\section{Distances and similarities}
To obtain relation between data, we can use distance and similarity measures. Distance measures the dissimilarity between two data points, while similarity measures the closeness between them. The choice of distance or similarity measure depends on the type of data and the specific problem at hand.
\begin{definition}
    The norm of a vector $x$ is:
    \begin{equation}
        ||x|| = \sqrt{\sum_{i=1}^{n} x_i^2}
    \end{equation}
\end{definition}
\begin{definition}
    The Minkowski distance between two vectors $x$ and $y$ is:
    \begin{equation}
        d(x,y) = \sqrt[p]{\sum_{i=1}^{n} |x_i - y_i|^p}
    \end{equation}
\end{definition}
It is a generalization of the Euclidean distance ($p=2$) and the Manhattan distance ($p=1$).
\begin{definition}
    The supremum distance (Minkowski when $p \rightarrow \infty$) between two vectors $x$ and $y$ is:
    \begin{equation}
        d(x,y) = \max_{i} |x_i - y_i|
    \end{equation}
\end{definition}
So two vectors are similar if their distances is close to 0.
\begin{definition}
    The cosine similarity between two vectors $x$ and $y$ is:
    \begin{equation}
        s(x,y) = \frac{x \cdot y}{||x|| \cdot ||y||} = \frac{\sum_{i=1}^{n} x_i y_i}{\sqrt{\sum_{i=1}^{n} x_i^2} \cdot \sqrt{\sum_{i=1}^{n} y_i^2}}
    \end{equation}
\end{definition}
Here two vectors are similar if their cosine similarity is close to 1, and dissimilar if it is close to 0.
\section{Preprocessing techniques}
First we can smooth data using binning methods. Binning methods smooth a sorted data value by consulting its neighborhood. We can smooth using bins by partitioning the sorted data into a number of equal-frequency or equal-width bins, and then replacing each data value in a bin by the mean, median, or boundary values of the bin. Another type of method to manipulate data is to use normalization. There exists multiple normalization techniques:
\begin{itemize}
    \item \textbf{Vector normalization}: scales a vector $x$ by it's norm to have a unit norm vector:
    \begin{equation}
        x' = \frac{x}{||x||}
    \end{equation}
    \item \textbf{Min-max normalization:} scales the vector linearly to fit in a specific range $[new\_min, new\_max]$:
    \begin{equation}
        x' = \frac{(x - min(x))(new\_max - new\_min)}{max(x) - min(x)} + new\_min 
    \end{equation}
    \item \textbf{Z-score normalization:} (or standardization) rescales the data to have a mean of 0 and a standard deviation of 1:
    \begin{equation}
        x' = \frac{x - \bar{x}}{\sigma}
    \end{equation}
    \item \textbf{Normalization by decimal scaling}:
    \begin{equation}
        x' = \frac{x}{10^j} \quad \text{where } j \text{ is the smallest integer such that } \max(|x'|) < 1
    \end{equation}
    $j$ can be computed as $j = \lceil \log_{10}(max(|x|)) + \epsilon \rceil$ with $\epsilon$ a small value to assure $\max(|x'|) < 1$.
\end{itemize}
For missing values, we can (last 3 methods introduce bias in the data): 
\begin{itemize}
    \item Ignore the tuple: usually done when the class label is missing or when multiple values are missing in the same tuple.
    \item Fill the missing value manually: time-consuming if lots of missing data
    \item Use a global constant (Unknown or $-\infty$): mining algorithms might mistakenly think that they form an interesting concept by having this "missing" value in common
    \item Use a measure of the central tendency of the attribute (e.g., mean or median)
    \item Use the measure of the central tendency for all samples belonging to the same class
    \item Use the most probable value to fill: using regression, inference-based tools, or decision tree induction.
\end{itemize}
We can also reduce the size of the data using sampling:
\begin{itemize}
    \item \textbf{SRSWOR} (Simple random sample without replacement of size s): this sampling is created by drawing samples from D, and every time a sample is drawn, it is not to be placed back into the data set D.
    \item \textbf{SRSWR} (Simple random sample with replacement of size s): this sampling is similar to SRSWOR, except that when a sample is drawn, it can be redrawn again, potentially.
    \item \textbf{Stratified sampling}: If D is divided into mutually disjoint parts called strata, a stratified sample of D is generated by obtaining a sample at each stratum. This helps ensure a representative sample, each stratum need to be defined manually.
\end{itemize} 
\chapter{Itemset mining}
\section{Definitions}
\begin{definition}
    An itemset is a set of items and an itemset containing $k$ items is called a k-itemset.
\end{definition}
Frequent itemset mining finds associations and correlations between items in large transactional or relational datasets. It is widely used in market basket analysis for example. It is the analysis of the buying habits of customers by finding associations between the different items that customers place in their shopping basket.
\begin{definition}
    A transactionnal dataset is a collection of transactions, where each transaction is a set of items.
\end{definition}
With $\mathcal{L}$ the set of  possible items, $\mathcal{T}$ the set of possible transactions, a transactional dataset $\mathcal{D}$ can be seen as the function $\mathcal{D}:\mathcal{T} \rightarrow 2^{\mathcal{L}}$. It can also be represented as a binary matrix, where rows represent transactions and columns represent items, mathematically it is like the function $\mathcal{D}:\mathcal{T} \times \mathcal{L} \rightarrow \{0,1\}$. 
\begin{definition}
    An itemset $l \subset \mathcal{L}$ covers (or matches) a transaction $t \subset \mathcal{T}$ if every item from $l$ is in $t$. Consider the match function, with $D(t,l)$ the function representing the transactional dataset:
    \begin{equation}
        \texttt{match}(l,t) = \begin{cases}
        1 & \text{if } \forall i \in l, D(t,i) = 1 \\
        0 & \text{otherwise}
        \end{cases}
    \end{equation}
\end{definition}
\begin{definition}
    The occurence frequency of an itemset (also called frequency, support count, count or absolute support) is the number of transactions that contain the itemset (i.e., the size of the cover)
    \begin{equation}
        \texttt{support}_\texttt{count}(l) = \sum_{t \in \mathcal{T}} \texttt{match}(l,t)
    \end{equation}
\end{definition}
\begin{definition}
    The relative support of an itemset $l$ in a database $\mathcal{D}$ is the percentage of transactions containing the itemset:
    \begin{equation}
        \texttt{support}_{\mathcal{D}}(l) = \frac{\texttt{support}_\texttt{count}(l)}{|\mathcal{T}|}
    \end{equation}
\end{definition}
\begin{definition}
    An association rule represents an implication of the form $X \Rightarrow Y$, where $X$ and $Y$ are itemsets.
\end{definition}
\begin{definition}
    The rule support is the support of the itemset containing all the elements of the rule $\texttt{support}_\texttt{count}(X \Rightarrow Y) = \texttt{support}_\texttt{count}(X \cup Y)$.
\end{definition}
We consider an association rule or an itemset as frequent if its support is greater than a user-defined minimum support threshold $\theta$.
\begin{definition}
    The rule confidence is the percentage of transactions containing $X$ that contains $Y$ too
    \begin{equation}
        \texttt{confidence}(X \Rightarrow Y) = P(Y|X) = \frac{\texttt{support}_\texttt{count}(X \cup Y)}{\texttt{support}_\texttt{count}(X)}
    \end{equation}
\end{definition}
\section{Apriori algorithm}
The key idea behind the Apriori algorithm is that all nonempty subsets of a frequent itemset must also be frequent. 
\begin{algorithm}[H]
    \caption{Apriori algorithm}
    \begin{algorithmic}[1]
        \State \textbf{Input:} Transactional dataset $\mathcal{D}$, minimum support threshold $\theta$
        \State \textbf{Output:} Set $F$ of frequent itemsets
        \State $F = \emptyset$
        \State $C_1 = \{\{i\}: i \in \mathcal{L}\}$
        \State $L_1 = \{c \in C_1: \texttt{support}_\texttt{count}(c) \geq \theta\}$
        \State $F = F \cup L_1$
        \State $k = 2$
        \While{$L_k$ is not empty}
            % \State $C_{k} = $ generate candidate (k+1)-itemsets from $L_{k-1}$ by joining them
            \State $C_k = \texttt{apriori\_gen}(L_{k-1})$
            \State $L_k = \{c \in C_k: \texttt{support}_\texttt{count}(c) \geq \theta\}$
            \State $F = F \cup L_k$
            \State $k = k + 1$
        \EndWhile
        \State \textbf{return} $F$



        % \State \textbf{Input:} Transactional dataset $\mathcal{D}$, minimum support threshold $\theta$
        % \State \textbf{Output:} Set of frequent itemsets
        % \State $k \gets 1$
        % \State $F_k \gets$ all frequent 1-itemsets in $\mathcal{D}$
        % \While{$F_k$ is not empty}
        %     \State $C_{k+1} \gets$ generate candidate (k+1)-itemsets from $F_k$
        %     \For{each transaction $t$ in $\mathcal{D}$}
        %         \For{each candidate itemset $c$ in $C_{k+1}$}
        %             \If{$c$ covers $t$}
        %                 \State increment the count of $c$
        %             \EndIf
        %         \EndFor
        %     \EndFor
        %     \State $F_{k+1} \gets$ itemsets in $C_{k+1}$ with support count $\geq \theta$
        %     \State $k \gets k + 1$
        % \EndWhile
        % \State \textbf{return} $\bigcup_{i=1}^{k} F_i$
    \end{algorithmic}
\end{algorithm}
The $\texttt{apriori\_gen}(L_{k-1})$ function generates candidate k-itemsets from the frequent (k-1)-itemsets $L_{k-1}$ by joining them and pruning those that have infrequent subsets.
\begin{algorithm}[H]
    \caption{apriori\_gen($L_{k-1}$)}
    \begin{algorithmic}[1]
        \State \textbf{Input:} Set $L_{k-1}$ of frequent (k-1)-itemsets
        \State \textbf{Output:} Set $C_k$ of candidate k-itemsets
        \State $C_k = \emptyset$
        \For{each $l_1 \in L_{k-1}$}
            \For{each $l_2 \in L_{k-1}$}
                \If{$l_1[:k-2] = l_2[:k-2]$ and $l_1[k-1] < l_2[k-1]$}
                    \State $c = l_1 \cup l_2$
                    \If{all (k-1)-subsets of $c$ are in $L_{k-1}$}
                        \State $C_k = C_k \cup \{c\}$
                    \EndIf
                \EndIf
            \EndFor
        \EndFor
        \State \textbf{return} $C_k$
    \end{algorithmic}
\end{algorithm}
The number of candidate itemsets is bounded by $\mathcal{O}\left(\begin{pmatrix}|\mathcal{L}| \\ k \end{pmatrix}\right)$.\\
The two mains weaknesses of the Apriori algorithm are:
\begin{itemize}
    \item Potential generation of huge candidate sets
    \item Repeated scan of the database and checks for pattern matching to the transactions
\end{itemize}
The Apriori algorithm can be improved using:
\begin{itemize}
    \item \textbf{Hash-based techniques}: Instead of building $L_2$ from the join operation, build it similarly to $L_1$. When scanning the transactions, generate the 2-itemset subset of the transaction, hash them and map them to corresponding buckets containings counters to be increased. $L_2$ is the set of the 2-itemsets associated to buckets of a count higher than $\theta$
    \item \textbf{Transaction reduction}: A transaction that does not contains any frequent $k$-itemsets cannot contain any frequent $(k+1)$-itemsets. They can be flagged and avoided at next steps.
    \item \textbf{Partitioning}: The dataset can be partitioned in $N$ non-overlapping sub-database. Given $\theta$, the minimum support count threshold for the whole database, $\theta' = \lfloor \frac{\theta}{N} \rfloor$ is the threshold for each partition. The candidate set $C_k$ is the union of all local frequent $k$-itemset of each partition as any itemset that is potentially frequent with respect to $D$ must occur as a frequent itemset in at least one of the partitions.
    \item \textbf{Sampling}: The frequent itemsets of a sample of the database are computed to serve as $C_k$. Some degree of accuracy is traded for eciency, thus a lower value than $\theta'$ is used to counterbalanced the effect.
\end{itemize}
\end{document}