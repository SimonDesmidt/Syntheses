\documentclass[12pt, openany]{report}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage[a4paper,left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage[english]{babel}
\usepackage{libertine}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{float}
\usepackage{enumitem}
\usepackage{pythonhighlight}
\usepackage[]{titletoc}
\usepackage{empheq}
\usepackage{titlesec}
\usepackage{mathpazo}
\usepackage{xfrac}
\usepackage{textcomp}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{tabularray}
\usepackage{subcaption}
\usepackage[bottom]{footmisc}
\usepackage{pdfpages}
\usepackage{tabularx}
\usepackage{amsthm}
\usepackage{listings}
\usepackage[skins]{tcolorbox}
\titleformat{\chapter}[display]
  {\normalfont\bfseries}{}{0pt}{\Huge}
\usepackage{hyperref}
\newcommand{\hsp}{\hspace{20pt}}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\theoremstyle{definition}
\newtheorem{thm}{Theorem}[chapter]
\newtheorem{definition}[thm]{Definition}
\newtheorem{lem}[thm]{Lemma}


\definecolor{mGreen}{rgb}{0,0.6,0}
\definecolor{mGray}{rgb}{0.5,0.5,0.5}
\definecolor{mPurple}{rgb}{0.58,0,0.82}
\definecolor{backgroundColour}{rgb}{0.95,0.95,0.92}
\definecolor{light-gray}{gray}{0.95}
\newcommand{\code}[1]{\colorbox{light-gray}{\texttt{#1}}}

\lstdefinestyle{CppStyle}{
    backgroundcolor=\color{backgroundColour},   
    commentstyle=\color{mGreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{mGray},
    stringstyle=\color{mPurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    language=C
}

\hbadness=100000
\begin{document}
\begin{titlepage}
    \begin{sffamily}
    \begin{center}
        \includegraphics[scale=1]{img/page_de_garde.png} \\[1cm]
        \HRule \\[0.4cm]
        { \huge \bfseries LINFO2364 Mining Patterns in Data \\[0.4cm] }
    
        \HRule \\[1.5cm]
        \textsc{\LARGE Issambre L'Hermite Dumont}\\[3cm]
        {This summary may not be up-to-date, the newer version is available at this address: \hyperlink{https://github.com/SimonDesmidt/Syntheses}{https://github.com/SimonDesmidt/Syntheses}}
        \vfill
        \vspace{2cm}
        {\large Academic year 2025-2026 - Q2}
        \vspace{0.4cm}
         
        \includegraphics[width=0.15\textwidth]{img/epl.png}
        
        UCLouvain\\
    
    \end{center}
    \end{sffamily}
\end{titlepage}

\setcounter{tocdepth}{1}
\tableofcontents
\chapter{Introduction}
In our data-driven world, the ability to extract meaningful information from vast datasets is crucial. Understanding all the aspect of this discipline is essential to derive those meaningful information, and this is the goal of this course. First, we need to define some key concepts.
\section{Definitions}
\begin{definition}
    A pattern is a recurring structure in a dataset.
\end{definition}
Patterns can be simple or complex, relevant or irrelevant. Their advantages is that they are interpretable. When found, relevant patterns can be used to make predictions, to understand the underlying structure of the data, and to make informed decisions.  
\begin{definition}
	Data mining is the process of discovering interesting patterns, models, and other kinds of knowledge in large data sets.
\end{definition}
\subsection{Type of data}
We can mine data out of various types of structure of data:
\begin{itemize}
    \item \textbf{Tabular data}: Data is organized in rows and columns. Example: spreadsheets, databases.
    \item \textbf{Sequences}: Data points are ordered in a sequence. Example: DNA sequences, text data.
    \item \textbf{Graphs, trees, networks}: Data is represented as nodes and edges. Example: social networks, web graphs.
    % \item \textbf{Transactional data}: Each record is a transaction, which is a set of items. Example: market basket data.
    % \item \textbf{Relational data}: Data is organized in tables with relationships between them. Example: customer databases.
    % \item \textbf{Time-series data}: Data points are collected over time. Example: stock prices, sensor data.    
    % \item \textbf{Spatial data}: Data with geographical or spatial components. Example: maps, satellite images.
\end{itemize}
Those structures can be discretes, continuous, enumerable data, etc. Those structures, can be combined to form more complex data types. And they can be highly structured, semi-structured, or unstructured.
\begin{definition}
    Highly structured data are relational databases, with uniform record or table-like structures, with a fixed set of well-defined attributes. This is rarely the case in real-world data.
\end{definition}
\begin{definition}
    Semi-structured data are not as structured as in relational databases, but presents some structure with clearly defined semantic meaning. For example:
    \begin{itemize}
        \item Transactional dataset: structured into transactions, but each transaction is an unstructured set of values
        \item Sequence data set: unstructured collection of ordered sequences of values
        \item Graphs: set of nodes connected by a set of edges, with edges labelled given some semantic
    \end{itemize}
\end{definition}
\begin{definition}
    Unstructured data have no predefined structure or organization. For example: text documents, images, audio files, videos.
\end{definition}
Those requires advanced techniques to extract patterns, like deep learning or domain-specific methods.
We can also categorize data based on the way they are generated:
\begin{itemize}
    \item \textbf{Stored data}: Data is collected and stored in a finite set.
    \item \textbf{Streamed data}: Data is continuously generated and updated over time. Example: video surveillance, etc.
\end{itemize}
\subsection{Types of data mining}
Techniques and algorithms may vary depending on the data but also on way we will use mined patterns. We can categorize data mining tasks into two main types:
\begin{itemize}
    \item \textbf{Descriptive data mining}: finds patterns that characterize the properties of the data. 
    \item \textbf{Predictive data mining}: finds patterns that can be used to make predictions by induction.
\end{itemize}
We can identify patterns by multiples ways:
\begin{itemize}
    \item \textbf{Frequent patterns}: patterns that identify a recurring structure in the data.
    \item \textbf{Associations patterns}: patterns that show rules of implications between different attributes
    \item \textbf{Correlations}: patterns that has positive or negative correlation between different attributes.
\end{itemize}
We can uses patterns for predictions:
\begin{itemize}
    \item \textbf{Classification}: assign new data to classes based on similarities with historic data.
    \item \textbf{Regression}: predict numerical values based on the new data and the historic data.
    \item \textbf{Feature selection}: identify the most relevant features.
\end{itemize}
\chapter{Preprocessing}
To analyse data, and retrieve meaningful patterns, data often needs to be preprocessed. This step is crucial as raw data is often incomplete, inconsistent, and noisy. Preprocessing improves the quality of the data and the efficiency of the mining algorithms.
\section{Useful statistical values}
\begin{definition}
    The mean (or average) of a dataset is the sum of all values divided by the number of values.
    \begin{equation}
        \bar{x} = \frac{1}{N} \sum_{i=1}^{N} x_i
    \end{equation}
\end{definition}
\begin{definition}
    The midrange is the average of the largest and smallest values in the set.
\end{definition}
\begin{definition}
    The variance of a dataset measures is defined like this:
    \begin{equation}
        \sigma^2 = \frac{1}{N} \sum_{i=1}^{N} (x_i - \bar{x})^2
    \end{equation}
\end{definition}
\begin{definition}
    The standard deviation of a dataset is the square root of the variance:
    \begin{equation}
        \sigma = \sqrt{\sigma^2} = \sqrt{\frac{1}{N} \sum_{i=1}^{N} (x_i - \bar{x})^2}
    \end{equation}
\end{definition}
\begin{definition}
    The mode for a set of data is the value that occurs most frequently. When there are multiple values with the same highest frequency, the dataset can be unimodal, bimodal, trimodal or multimodal.
\end{definition}
\begin{definition}
    Quantiles are points taken at regular intervals of a data distribution, dividing it into essentially equal-sized consecutive sets. The $k$th $q$-quantile for a given data distribution is the value $x$ such that at most $\frac{k}{q}$ of the data values are less than $x$ and at most $\frac{q-k}{q}$ of the data values are more than $x$, where $k$ is an integer such that $0 < k < q$. There are $q-1$ $q$-quantiles.
\end{definition}
Usually we use quartiles ($q=4$), where $Q_2$ is the median, or percentiles ($q=100$).
\section{Data plot}
Plotting the data can be useful to observe some characteristics of the dataset. we can represent the data under multiples forms:
\begin{itemize}
    \item \textbf{Histogram} is a bar chart representing the count of values present in each bucket. The buckets can be defined for each values, for ranges of values (equal-width) or for buckets containing the same number of values (equal-frequency).
    \item \textbf{Boxplots} are a popular way of visualizing a distribution. Typically, the ends of the box are at the quartiles so that the box length is the interquartile range. The median is marked by a line within the box. Two lines (called whiskers) outside the box extend to the smallest (minimum) and largest (maximum) observations. To highlight outliers, the whiskers are usually extended to extreme low and high observations only if these values are less than 1.5 times the interquartile range beyond the quartiles. Otherwise, the whiskers terminate at the most extreme observations occurring within 1.5 times the interquartile range.
    \item \textbf{Scatter plot}, each pair of values is treated as a pair of coordinates in an algebraic sense and plotted as points in the plane. It helps visually determine whether there is a relationship, pattern, or trend between the two numeric attributes.
    \item \textbf{Quantile-quantile plot}, or q-q plots are graphs that plot the quantiles of one univariate distribution against the corresponding quantiles of another.
\end{itemize}
Here is an example of the four plots for the same dataset:
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.55]{img/preprocess_plot.pdf}
    % \caption{Boxplot example}
\end{figure}
\section{Distances and similarities}
To obtain relation between data, we can use distance and similarity measures. Distance measures the dissimilarity between two data points, while similarity measures the closeness between them. The choice of distance or similarity measure depends on the type of data and the specific problem at hand.
\begin{definition}
    The norm of a vector $x$ is:
    \begin{equation}
        ||x|| = \sqrt{\sum_{i=1}^{n} x_i^2}
    \end{equation}
\end{definition}
\begin{definition}
    The Minkowski distance between two vectors $x$ and $y$ is:
    \begin{equation}
        d(x,y) = \sqrt[p]{\sum_{i=1}^{n} |x_i - y_i|^p}
    \end{equation}
\end{definition}
It is a generalization of the Euclidean distance ($p=2$) and the Manhattan distance ($p=1$).
\begin{definition}
    The supremum distance (Minkowski when $p \rightarrow \infty$) between two vectors $x$ and $y$ is:
    \begin{equation}
        d(x,y) = \max_{i} |x_i - y_i|
    \end{equation}
\end{definition}
So two vectors are similar if their distances is close to 0.
\begin{definition}
    The cosine similarity between two vectors $x$ and $y$ is:
    \begin{equation}
        s(x,y) = \frac{x \cdot y}{||x|| \cdot ||y||} = \frac{\sum_{i=1}^{n} x_i y_i}{\sqrt{\sum_{i=1}^{n} x_i^2} \cdot \sqrt{\sum_{i=1}^{n} y_i^2}}
    \end{equation}
\end{definition}
Here two vectors are similar if their cosine similarity is close to 1, and dissimilar if it is close to 0.
\section{Preprocessing techniques}
First we can smooth data using binning methods. Binning methods smooth a sorted data value by consulting its neighborhood. We can smooth using bins by partitioning the sorted data into a number of equal-frequency or equal-width bins, and then replacing each data value in a bin by the mean, median, or boundary values of the bin. Another type of method to manipulate data is to use normalization. There exists multiple normalization techniques:
\begin{itemize}
    \item \textbf{Vector normalization}: scales a vector $x$ by it's norm to have a unit norm vector:
    \begin{equation}
        x' = \frac{x}{||x||}
    \end{equation}
    \item \textbf{Min-max normalization:} scales the vector linearly to fit in a specific range $[new\_min, new\_max]$:
    \begin{equation}
        x' = \frac{(x - min(x))(new\_max - new\_min)}{max(x) - min(x)} + new\_min 
    \end{equation}
    \item \textbf{Z-score normalization:} (or standardization) rescales the data to have a mean of 0 and a standard deviation of 1:
    \begin{equation}
        x' = \frac{x - \bar{x}}{\sigma}
    \end{equation}
    \item \textbf{Normalization by decimal scaling}:
    \begin{equation}
        x' = \frac{x}{10^j} \quad \text{where } j \text{ is the smallest integer such that } \max(|x'|) < 1
    \end{equation}
    $j$ can be computed as $j = \lceil \log_{10}(max(|x|)) + \epsilon \rceil$ with $\epsilon$ a small value to assure $\max(|x'|) < 1$.
\end{itemize}
For missing values, we can (last 3 methods introduce bias in the data): 
\begin{itemize}
    \item Ignore the tuple: usually done when the class label is missing or when multiple values are missing in the same tuple.
    \item Fill the missing value manually: time-consuming if lots of missing data
    \item Use a global constant (Unknown or $-\infty$): mining algorithms might mistakenly think that they form an interesting concept by having this "missing" value in common
    \item Use a measure of the central tendency of the attribute (e.g., mean or median)
    \item Use the measure of the central tendency for all samples belonging to the same class
    \item Use the most probable value to fill: using regression, inference-based tools, or decision tree induction.
\end{itemize}
We can also reduce the size of the data using sampling:
\begin{itemize}
    \item \textbf{SRSWOR} (Simple random sample without replacement of size s): this sampling is created by drawing samples from D, and every time a sample is drawn, it is not to be placed back into the data set D.
    \item \textbf{SRSWR} (Simple random sample with replacement of size s): this sampling is similar to SRSWOR, except that when a sample is drawn, it can be redrawn again, potentially.
    \item \textbf{Stratified sampling}: If D is divided into mutually disjoint parts called strata, a stratified sample of D is generated by obtaining a sample at each stratum. This helps ensure a representative sample, each stratum need to be defined manually.
\end{itemize} 
\chapter{Itemset mining}
\section{Definitions}
\begin{definition}
    An itemset is a set of items and an itemset containing $k$ items is called a k-itemset.
\end{definition}
Frequent itemset mining finds associations and correlations between items in large transactional or relational datasets. It is widely used in market basket analysis for example. It is the analysis of the buying habits of customers by finding associations between the different items that customers place in their shopping basket.
\begin{definition}
    Itemset $Y$ is called a \textbf{superitemset} of $X$ if $X$ is a \textbf{subitemset} of $Y$, i.e. if $X \subset Y$. Every items of $X$ is in $Y$ but at least one item in $Y$ does not appear in $X$.
\end{definition}
\begin{definition}
    An itemset $X$ is \textbf{closed} in $\mathcal{D}$ if there exists no proper superitemset $Y$ such that $Y$ has the same support count as $X$ in $\mathcal{D}$.
\end{definition}
\begin{definition}
    A transactional dataset is a collection of transactions, where each transaction is a set of items.
\end{definition}
With $\mathcal{L}$ the set of  possible items, $\mathcal{T}$ the set of possible transactions, a transactional dataset $\mathcal{D}$ can be seen as the function $\mathcal{D}:\mathcal{T} \rightarrow 2^{\mathcal{L}}$. It can also be represented as a binary matrix, where rows represent transactions and columns represent items, mathematically it is like the function $\mathcal{D}:\mathcal{T} \times \mathcal{L} \rightarrow \{0,1\}$. 
\begin{definition}
    An itemset $l \subset \mathcal{L}$ covers (or matches) a transaction $t \subset \mathcal{T}$ if every item from $l$ is in $t$. Consider the match function, with $D(t,l)$ the function representing the transactional dataset:
    \begin{equation}
        \texttt{match}(l,t) = \begin{cases}
        1 & \text{if } \forall i \in l, D(t,i) = 1 \\
        0 & \text{otherwise}
        \end{cases}
    \end{equation}
\end{definition}
\begin{definition}
    The occurrence frequency of an itemset (also called frequency, support count, count or absolute support) is the number of transactions that contain the itemset (i.e., the size of the cover)
    \begin{equation}
        \texttt{support}_\texttt{count}(l) = \sum_{t \in \mathcal{T}} \texttt{match}(l,t)
    \end{equation}
\end{definition}
An itemset is called frequent if its support count is greater than a minimum support count threshold $\theta$. If an itemset is frequent then all of its subsets are also frequent.
\begin{definition}
    The relative support of an itemset $l$ in a database $\mathcal{D}$ is the percentage of transactions containing the itemset:
    \begin{equation}
        \texttt{support}_{\mathcal{D}}(l) = \frac{\texttt{support}_\texttt{count}(l)}{|\mathcal{T}|}
    \end{equation}
\end{definition}
\begin{definition}
    An itemset $X$ is a \textbf{maximal frequent itemset} in $\mathcal{D}$ if $X$ is frequent, and there exists no superitemset $Y$ such that $Y$ is frequent in $\mathcal{D}$.
\end{definition}
\begin{definition}
    An association rule represents an implication of the form $X \Rightarrow Y$, where $X$ and $Y$ are itemsets.
\end{definition}
\begin{definition}
    The rule support is the support of the itemset containing all the elements of the rule $\texttt{support}_\texttt{count}(X \Rightarrow Y) = \texttt{support}_\texttt{count}(X \cup Y)$.
\end{definition}
We consider an association rule or an itemset as frequent if its support is greater than a user-defined minimum support threshold $\theta$.
\begin{definition}
    The rule confidence is the percentage of transactions containing $X$ that contains $Y$ too
    \begin{equation}
        \texttt{confidence}(X \Rightarrow Y) = P(Y|X) = \frac{\texttt{support}_\texttt{count}(X \cup Y)}{\texttt{support}_\texttt{count}(X)}
    \end{equation}
\end{definition}
\section{Apriori algorithm}
The key idea behind the Apriori algorithm is that all nonempty subsets of a frequent itemset must also be frequent. 
\begin{algorithm}[H]
    \caption{Apriori algorithm}
    \begin{algorithmic}[1]
        \State \textbf{Input:} Transactional dataset $\mathcal{D}$, minimum support threshold $\theta$
        \State \textbf{Output:} Set $F$ of frequent itemsets
        \State $F = \emptyset$
        \State $C_1 = \{\{i\}: i \in \mathcal{L}\}$
        \State $L_1 = \{c \in C_1: \texttt{support}_\texttt{count}(c) \geq \theta\}$
        \State $F = F \cup L_1$
        \State $k = 2$
        \While{$L_k$ is not empty}
            \State $C_k = \texttt{apriori\_gen}(L_{k-1})$
            \State $L_k = \{c \in C_k: \texttt{support}_\texttt{count}(c) \geq \theta\}$
            \State $F = F \cup L_k$
            \State $k = k + 1$
        \EndWhile
        \State \textbf{return} $F$
    \end{algorithmic}
\end{algorithm}
\newpage
The $\texttt{apriori\_gen}(L_{k-1})$ function generates candidate k-itemsets from the frequent (k-1)-itemsets $L_{k-1}$ by joining them and pruning those that have infrequent subsets.
\begin{algorithm}[H]
    \caption{apriori\_gen($L_{k-1}$)}
    \begin{algorithmic}[1]
        \State \textbf{Input:} Set $L_{k-1}$ of frequent (k-1)-itemsets
        \State \textbf{Output:} Set $C_k$ of candidate k-itemsets
        \State $C_k = \emptyset$
        \For{each $l_1 \in L_{k-1}$}
            \For{each $l_2 \in L_{k-1}$}
                \If{$l_1[:k-2] = l_2[:k-2]$ and $l_1[k-1] < l_2[k-1]$}
                    \State $c = l_1 \cup l_2$
                    \If{all (k-1)-subsets of $c$ are in $L_{k-1}$}
                        \State $C_k = C_k \cup \{c\}$
                    \EndIf
                \EndIf
            \EndFor
        \EndFor
        \State \textbf{return} $C_k$
    \end{algorithmic}
\end{algorithm}
The number of candidate itemsets is bounded by $\mathcal{O}\left(\begin{pmatrix}|\mathcal{L}| \\ k \end{pmatrix}\right)$.\\
The two mains weaknesses of the Apriori algorithm are:
\begin{itemize}
    \item Potential generation of huge candidate sets
    \item Repeated scan of the database and checks for pattern matching to the transactions
\end{itemize}
The Apriori algorithm can be improved using:
\begin{itemize}
    \item \textbf{Hash-based techniques}: Instead of building $L_2$ from the join operation, build it similarly to $L_1$. When scanning the transactions, generate the 2-itemset subset of the transaction, hash them and map them to corresponding buckets containings counters to be increased. $L_2$ is the set of the 2-itemsets associated to buckets of a count higher than $\theta$
    \item \textbf{Transaction reduction}: A transaction that does not contains any frequent $k$-itemsets cannot contain any frequent $(k+1)$-itemsets. They can be flagged and avoided at next steps.
    \item \textbf{Partitioning}: The dataset can be partitioned in $N$ non-overlapping sub-database. Given $\theta$, the minimum support count threshold for the whole database, $\theta' = \lfloor \frac{\theta}{N} \rfloor$ is the threshold for each partition. The candidate set $C_k$ is the union of all local frequent $k$-itemset of each partition as any itemset that is potentially frequent with respect to $D$ must occur as a frequent itemset in at least one of the partitions.
    \item \textbf{Sampling}: The frequent itemsets of a sample of the database are computed to serve as $C_k$. Some degree of accuracy is traded for eciency, thus a lower value than $\theta'$ is used to counterbalanced the effect.
\end{itemize}
\section{FP-growth algorithm}
For the FP-growth algorithm, we first need to create the FP-tree, which is a tree structure that represents all of the transactions in the database.
\begin{algorithm}[H]
    \caption{\texttt{fp\_growth}($\mathcal{D}$, $\theta$)}
    \begin{algorithmic}[1]
        \State \textbf{Input:} Transactional dataset $\mathcal{D}$, minimum support threshold $\theta$
        \State \textbf{Output:} Set $F$ of frequent itemsets
        \State tree = \texttt{fp\_tree}($\mathcal{D}$)
        \State F = \texttt{fp\_growth}(tree, $\theta$)
        \State \textbf{return} F
    \end{algorithmic}
\end{algorithm}
First we need to build the FP-tree, which is done like this:
\begin{algorithm}[H]
    \caption{\texttt{fp\_tree}($\mathcal{D}$, $\theta$)}
    \begin{algorithmic}[1]
        \State \textbf{Input:} Transactional dataset $\mathcal{D}$, minimum support threshold $\theta$
        \State \textbf{Output:} A FP-tree
        \State Scan D and compute the support count of each item, and keep only those with support count $\geq \theta$ in F
        \State Sort F in descending order of support count, creating L the list of frequent items
        \State Create the root of the tree
        \For{each transaction $\tau \in \mathcal{D}$}
            \State Sort the items of $\tau$ following L
            \State Add the transaction $\tau$ to the tree, by incrementing counters along the way and creating new nodes is the path is not present
        \EndFor
        \State \textbf{return} Tree
    \end{algorithmic}
\end{algorithm}
Then we can mine the FP-tree to find all the frequent itemsets. The mining algorithm, works like this:
\begin{algorithm}[H]
    \caption{\texttt{fp\_growth\_mining}(Tree, $\alpha$)}
    \begin{algorithmic}[1]
        \State \textbf{Input:} The FP-tree and a suffix $\alpha$ (initially empty)
        \State \textbf{Output:} Set $F$ of frequent itemsets
        \If{Tree contains a single path P}
            \For{each combination (denoted as $\beta$) of the nodes in the path $P$}
                \State generate pattern $\beta \cup \alpha$ with $\texttt{support}_{\texttt{count}} = \min(\texttt{support}_{\texttt{count}}$ of nodes $\in \beta)$ 
            \EndFor
        \Else
            \For {each $a_i$ in the header of Tree}
                \State generate pattern $\beta = a_i \cup \alpha$ with $\texttt{support}_{\texttt{count}} = \texttt{support}_{\texttt{count}}(a_i)$
                \State construct $\beta$'s conditional pattern base and then $\beta$'s conditional FP-tree $Tree_\beta$
                \If{$Tree_\beta \neq \emptyset$}
                    \State \texttt{fp\_growth\_mining}($Tree_\beta$, $\beta$)
                \EndIf
            \EndFor
        \EndIf
    \end{algorithmic}
\end{algorithm}
And after mining the FP-tree, we will get all the frequent itemsets.
\section{Eclat algorithm}
This algorithm works with a vertical data format, it means that we works with sets of transactions for each item, instead of sets of items for each transaction. The key idea is to use the intersection of transaction sets to compute the support count of itemsets. We can write the algorithm like this:
\begin{algorithm}[H]
    \caption{\texttt{eclat\_algo}($\mathcal{D}$, $\theta$)}
    \begin{algorithmic}[1]
        \State \textbf{Input:} Transactional dataset $\mathcal{D}$, minimum support threshold $\theta$
        \State \textbf{Output:} Set $F$ of frequent itemsets
        \State Transform $\mathcal{D}$ into a vertical data format
        \State $F = \emptyset$
        \State $F \cup \{i: \text{if size}(t(i)) \geq \theta\}$ for all item transaction sets $t(i)$ of item $i$
        \State $C = F$
        \While {C is not empty}
            \State $new\_C = \emptyset$
            \For {each pair of transaction sets $A$ and $B$ in $C$}
                \State c = $A \cap B$
                \If {$\text{size}(c) \geq \theta$} 
                    \State $new\_C \cup c$
                \EndIf
            \EndFor
            \State $F = F \cup new\_C$
            \State $C = new\_C$
        \EndWhile
        \State \textbf{return} F
    \end{algorithmic}
\end{algorithm}
\section{Association rule mining}
\begin{algorithm}[H]
    \caption{\texttt{Association\_rule\_mining\_algo}($\mathcal{D}$, $\theta$, $\epsilon$)}
    \begin{algorithmic}[1]
        \State \textbf{Input:} Transactional dataset $\mathcal{D}$, minimum support threshold $\theta$, minimum confidence threshold $\epsilon$
        \State \textbf{Output:} Set $FC$ of frequent and confident itemsets
        \State $F = \texttt{apriori\_algo}(\mathcal{D}, \theta)$ (OR other frequent itemset mining algorithm)
        \State $FC = \emptyset$
        \State Generate set $S$ the powerset ($\backslash \{\emptyset\} $) of $F$ (i.e., all non empty subsets of $F$)
        \For {each itemset $s \in S$}
            \State Compute confidence of the rule $s \rightarrow (F \backslash s)$ using the support counts of $s$ and $F$:
            \begin{equation}
                \texttt{confidence}(s \rightarrow (F \backslash s)) = \frac{\texttt{support}_\texttt{count}(F)}{\texttt{support}_\texttt{count}(s)}
            \end{equation}  
            \If {confidence $\geq \epsilon$}
                \State $FC = FC \cup \{s\}$
            \EndIf
        \EndFor
        \State \textbf{return} FC
    \end{algorithmic}
\end{algorithm}
\section{Correlation measures}
Even when we observe strong associations rules between items, they might not be interesting. To decide that, we can either manually chose if the pattern is interesting or not, it is called \textbf{Subjective assessement}. Or we can use \textbf{Objective assessment} which is based on statistics. For that, we can introduces the following measures:
\subsection{Lift}
The lift of a pair of item analyses the dependence and the correlation between them. It is defined as:
\begin{equation}
    \texttt{lift}(A,B) = \frac{P(A \cup B)}{P(A) P(B)} = \frac{P(B|A)}{P(B)} = \frac{conf(A \Rightarrow B)}{P(B)}
\end{equation}
\begin{itemize}
    \item If $\texttt{lift}(A,B) < 1$, A and B are negatively correlated, i.e., the occurence of one likely leads to the absence of the other
    \item If $\texttt{lift}(A,B) = 1$, A and B are independant
    \item If $\texttt{lift}(A,B) > 1$, A and B are positively correlated, i.e., the occurence of one implies the occurence of the other
\end{itemize}
\subsection{chi-squared ($\chi^2$)}
Based on the contingency table of the two items, we can compute the $\chi^2$ statistics that tests the independence of the two items. The test is based on statistics table, and degrees of freedom. The degrees of freedom can be computed with:
\begin{equation}
    DF = (r-1)(c-1)
\end{equation}
And the $\chi^2$ metrics is computed with:
\begin{equation}
    \chi^2 = \sum_{i=1}^{r} \sum_{j=1}^{c} \frac{(o_{ij} - e_{ij})^2}{e_{ij}}
\end{equation}
with $o_{ij}$ the observed frequency of the event $(A_i,B_j)$ and $e_{ij}$ the expected frequency of the event $(A_i,B_j)$. $e_{ij}$ is computed as:
\begin{equation}
    e_{i,j} = \frac{\texttt{count}(A=a_i) \times \texttt{count}(B=b_j)}{Total}
\end{equation}
with $n$ the number of data tuples.
Example of a contingency table for two items $A$ and $B$:
\begin{center}
    \begin{tabular}{|c|c|c|c|}
        \hline
        $o_{i,j}$ & $A$ & $\neg A$ & Total \\
        \hline  
        $B$ & 4000 & 3500 & 7500 \\
        \hline
        $\neg B$ & 2000 & 500 & 2500 \\
        \hline
        Total & 6000 & 4000 & 10000 \\
        \hline
    \end{tabular}
\end{center}
And the expected table is:
\begin{center}
    \begin{tabular}{|c|c|c|}
        \hline
        $e_{i,j}$ & $A$ & $\neg A$ \\
        \hline  
        $B$ & 4500 & 3000 \\
        \hline
        $\neg B$ & 1500 & 1000 \\
        \hline
    \end{tabular}
\end{center}
And here is an example of the $\chi^2$, for different confidence values of the pair ($A$, $B$): 
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.33]{img/chi_sq.png}
    \caption{Example of $\chi^2$ table}
\end{figure}
\subsection{All confidence measure}
\textbf{All confidence measure} is the minimum confidence of the two association rules related to $A$ and $B$ ($A \Rightarrow B$ and $B \Rightarrow A$). And it is defined as:
\begin{equation}
    \begin{aligned}
        \texttt{all\_conf}(A,B) &= \frac{\texttt{support}_\texttt{count}(A \cup B)}{\max \{\texttt{support}_\texttt{count}(A), \texttt{support}_\texttt{count}(B) \}}\\
        & = \min\{P(B|A), P(A|B)\}
    \end{aligned}
\end{equation}
\subsection{Max confidence measure}
\textbf{Max confidence measure} is also the maximum confidence of the two association rules related to $A$ and $B$ ($A \Rightarrow B$ and $B \Rightarrow A$). And it is defined as:
\begin{equation}
    \texttt{max\_conf}(A,B) = \max\{P(B|A), P(A|B)\}
\end{equation}
\subsection{Kulczynski measure}
\textbf{Kulczynski measure} can be view as an average confidence of the two association rules related to $A$ and $B$ ($A \Rightarrow B$ and $B \Rightarrow A$). And it is defined as:
\begin{equation}
    \texttt{kulc}(A,B) = \frac{1}{2} \left( P(A|B) + P(B|A) \right)
\end{equation}
\subsection{Cosine measure}
\textbf{Cosine measure} can be view as a harmonized lift measure and is defined as:
\begin{equation}
    \begin{aligned}
        \texttt{cosine}(A,B) &= \frac{\texttt{support}_\texttt{count}(A \cup B)}{\sqrt{\texttt{support}_\texttt{count}(A) \texttt{support}_\texttt{count}(B) }}\\
        &= \frac{P(A \cup B)}{\sqrt{P(A) P(B)}} = \sqrt{P(A|B)p(B|A)} 
    \end{aligned}
\end{equation}
For all thoses measures, from the all confidence to the cosine measure, the measures are in the range $[0,1]$ and they can be interpreted as:
\begin{itemize}
    \item If $\texttt{cosine}(A,B) < 0.5$, $A$ and $B$ are negatively correlated
    \item If $\texttt{cosine}(A,B) = 0.5$, $A$ and $B$ are independant
    \item If $\texttt{cosine}(A,B)> 0.5$, $A$ and $B$ are positively correlated
\end{itemize}
To decide whether two items are correlated or not, we can use all the previous mesures. We can also check the relation between two items by testing their conditional probabilities (i.e. $P(A|B)$ and $P(B|A)$). And with that, we can introduce the imbalance ratio.
\subsection{Imbalance ratio}
\textbf{The imbalance ratio} measures the difference between the two conditional probabilities and return a value near 0 if the two probabilities are close and otherwise the closer it is to 1, the more imbalanced the two probabilities are. It is defined as:
\begin{equation}
    \texttt{IR(A,B)} = \frac{|\texttt{support}_\texttt{count}(A) - \texttt{support}_\texttt{count} (B) |}{\texttt{support}_\texttt{count}(A) + \texttt{support}_\texttt{count}(B) - \texttt{support}_\texttt{count}(A \cup B)}
\end{equation}
Usually it is advised to use kulc and the imbalance ratio together to decide.
\section{Multilevel associations and mining rare patterns}
Items can have multiple level of abstraction. For example, a store can sell different type of goods (fruits, vegetables, drinks, etc) and in those categories, there can be different subcategories (for drink: alcool, softs, milks, etc) and in those subcategories, we can have different items. Patterns can be mined at each level of abstraction, and usually patterns at more detailled levels are the most interesting. From the levels of abstraction, we can draw a tree (which is not unique), to show a way of categorizing the items. For example, we can have this concept hierarchy: 
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.33]{img/hierarchy.png}
    \caption{Example of a concept hierarchy}
\end{figure}
We can mine multilevel association rules by using 3 differents types of support thresholds:
\begin{definition} 
    \textbf{Uniform support} mining uses the same minimum support threshold for all the levels of the hierarchy. With this type of support, we do not consider an itemset without a frequent ancestor.
    % If itemset i is frequent at level k, itemsets composed of subconsept are candidate for layer k + 1
\end{definition}
\begin{definition}
    \textbf{Reduced support} mining uses a lower minimum support threshold for the lower levels of the hierarchy. And so with this, each level as it's own minimum support threshold, and an item might be frequent even though its ancestor is not frequent.
\end{definition}
\begin{definition}
    
\end{definition}
\end{document}