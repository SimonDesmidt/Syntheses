\documentclass[12pt, openany]{report}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage[a4paper,left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage[english]{babel}
\usepackage{libertine}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{float}
\usepackage{enumitem}
\usepackage{pythonhighlight}
\usepackage[]{titletoc}
\usepackage{empheq}
\usepackage{titlesec}
\usepackage{mathpazo}
\usepackage{xfrac}
\usepackage{textcomp}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{tabularray}
\usepackage{subcaption}
\usepackage[bottom]{footmisc}
\usepackage{pdfpages}
\usepackage{tabularx}
\usepackage {tikz}
\usetikzlibrary{positioning}
\usepackage[strict]{changepage} % for adjustwidth environment
\usepackage{framed}             % for formal definitions
\usepackage{fourier}            % for the warning symbol
\usepackage{amsthm}
\usepackage[skins]{tcolorbox}
\titleformat{\chapter}[display]
  {\normalfont\bfseries}{}{0pt}{\Huge}
\usepackage{hyperref}
\newcommand{\hsp}{\hspace{20pt}}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Oo}{\mathcal{O}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\J}{\mathbf{J}}
\newcommand{\He}{\mathbf{H}}
\newcommand{\Lo}{\mathcal{L}}
\theoremstyle{definition}
\newtheorem{exmp}{Example}[chapter]
\newtheorem{thm}{Theorem}[chapter]
\newtheorem{definition}[thm]{Definition}
\newtheorem{lem}[thm]{Lemma}

% environment derived from framed.sty: see leftbar environment definition
\definecolor{formalshade}{rgb}{0.95,0.95,1}
\definecolor{darkblue}{rgb}{0.0, 0.0, 0.55}

\newenvironment{formal}{
  \def\FrameCommand{
    \hspace{1pt}
    {\color{darkblue}\vrule width 2pt}
    {\color{formalshade}\vrule width 4pt}
    \colorbox{formalshade}
  }
  \MakeFramed{\advance\hsize-\width\FrameRestore}
  \noindent\hspace{-4.55pt}% disable indenting first paragraph
  \begin{adjustwidth}{}{7pt}
  \vspace{2pt}\vspace{2pt}
}
{
  \vspace{2pt}\end{adjustwidth}\endMakeFramed
}

\hbadness=100000
\begin{document}
\begin{titlepage}
	\begin{sffamily}
	\begin{center}
		\includegraphics[scale=0.3]{img/page_de_garde.jpg} \\[1cm]
		\HRule \\[0.4cm]
		{ \huge \bfseries LINMA2472 - Algorithm in data science \\[0.4cm] }
	
		\HRule \\[1.5cm]
		\textsc{\LARGE Simon Desmidt}\\[1cm]
		\vfill
		\vspace{2cm}
		{\large Academic year 2025-2026 - Q1}
		\vspace{0.4cm}
		 
		\includegraphics[width=0.15\textwidth]{img/epl.png}
		
		UCLouvain\\
	
	\end{center}
	\end{sffamily}
\end{titlepage}

\setcounter{tocdepth}{1}
\tableofcontents
\chapter{Automatic Differentiation}
Automatic Differentiation (AD) is an algorithmic technique to compute automatically the derivative (gradient) of a function defined in a computer program. 
Unlike symbolic differentiation (\emph{done by hand}) and numerical  differentiation (\emph{finite difference approximation}), 
automatic differentiation exploits the fact that every function can be decomposed into a sequence of elementary operations (addition, multiplication, sine, exponential, etc.) 
By mean of the chain rule, applied to obtain the all of a function's derivatives, its gradient can be exactly and efficiently computed.

Automatic differentiation is widely used in machine learning because neural networks requires the computation of the loss function's gradient with respect 
to the model's parameters (weights and biases). This is necessary to update them during the training process (e.g. with a gradient descent) 
and it would be a pain in the ass to compute this manually for each node.\\
\section{Chain rule}
Assume $f$ is the composition of $m$ functions. The chain rule gives
\begin{equation}
  f'(x) = f_m'(f_{m-1}(f_{m-2}(\dots f_1(x)\dots)))\cdot f'_{m-1}(f_{m-2}(\dots f_1(x)\dots)) \cdot \dots \cdot f_2'(f_1(x)) \cdot f_1'(x)
\end{equation}
Defining
\begin{equation}
  \begin{cases}
	s_0 &= x \\
	s_k &= f_k(s_{k-1})
  \end{cases}
\end{equation}
gives
\begin{equation}
  f'(x) = f'_m(s_{m-1})\cdot f'_{m-1}(s_{m-2}) \cdot \dots
  \cdot f'_2(s_1) \cdot f'_1(s_0)
\end{equation}
Based on this expression, 2 ways of applying the chain rule can be defined: forward and backward differentiation.
\section{Forward differentiation}
Also called forward mode, this algorithm consists in propagating forward the derivative and the values at the same time. 
Propagating the values forward is called a \textbf{forward pass}. 
This process can be represented by the graph on figure \ref{fig:forward_diff}. The blue part represents the values and the green part shows the derivatives.
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{img/forward_diff.png}
	\caption{Forward differentiation}
	\label{fig:forward_diff}
\end{figure}
The algorithm follows:
\begin{equation}
  \begin{cases}
	t_0 &= 1 \\
	t_k &= f'_k(s_{k-1}) \cdot t_{k-1}\\
  \end{cases}
\end{equation}
This process is repeated for every input variables. Consequently, it is \emph{very efficient} for functions with a \emph{small number of input} variables 
and bad for functions with a large number of inputs. 

The forward differentiation algorithm is the easiest to implement.
\section{Backward differentiation}
Also called backward mode, the algorithm consists in propagating the values forward and the derivatives backward \emph{in one pass}. 
Propagating the derivatives backward is called a \textbf{backward pass}. 
This process can be represented by the graph on figure \ref{fig:backward_diff}. The blue part represents the values and the orange part the derivatives.
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{img/backward_diff.png}
	\caption{Backward differentiation}
	\label{fig:backward_diff}
\end{figure}
The idea is to compute all the intermediate values $s_k$ in a forward pass and then compute the derivatives $r_k$ based on the output in a backward pass. 
The algorithm follows the recurrence relation:
\begin{equation}
  \begin{cases}
	r_m &= 1 \\
	r_k &= r_{k+1} \cdot f'_{k+1}(s_{k})\\
  \end{cases}
\end{equation}
This method is heavier to implement but it is very efficient for functions with a large number of input variables and a small number of output variables. 
Typically, the backward mode is preferred for neural networks, where there is typically \emph{only one} output variable: the loss.

\section{Computational graph and multivariate differentiation}
\subsection{Computational graph}  
To represent the computation of a function, a computational graph can be used. It is a directed acyclic graph where the nodes represent the operations 
and the edges represent the variables. For instance, consider the function with $f_1(x)=x=s_1$ and $f_2(x)=x^2=s_2$:
\begin{equation}
  f_3(s_1,s_2) = s_1 + s_2 = x + x^2
\end{equation}
The computational graph is:\\
\begin{figure}[H]
  \centering
  \begin{tikzpicture}[
	roundnode/.style={circle, draw=green!60, fill=green!5, very thick, minimum size=7mm},]
	%Nodes
	\node[roundnode] (x1) {$s_1=x$};
	\node[roundnode] (square) [right=of x1] {$f_2=x^2$};
	\node[roundnode] (sum) [right=of square] {$f_3=f_2+s_1$};
	
	% %Lines
	\draw[->] (x1.north) .. controls +(up:7mm) and +(up:7mm).. (sum.north);
	\draw[->] (x1.east) -- (square.west);
	\draw[->] (square.east) -- (sum.west);
  \end{tikzpicture}
  \caption{Directed Acyclic Graph (DAG) of $f_3$}
  \label{tikz:dag}
\end{figure}
\subsection{Multivariate differentiation}
Let's consider the function displayed on figure \ref{tikz:dag}.
\begin{equation}
  f_3(f_1(x),f_2(x)) = s_3 = f_1(x) + f_2(x) = s_1 + s_2 = x + x^2 
\end{equation}
The chain rule gives
\begin{equation}
  f'_3(x) = \dfrac{\partial f_3}{\partial s_1} \dfrac{\partial s_1}{\partial x} + \dfrac{\partial f_3}{\partial s_2} \dfrac{\partial s_2}{\partial x}
\end{equation}
In forward mode, the values and the derivatives are propagated forward together. When a node has multiple inputs, it is necessary to use the chain rule. 

Evaluating $f_3$ at $x=3$ leads to
\begin{equation}
	\begin{cases}
		t_0 &= 1 \\
		t_1 &= f'_1(x) \vert_{x=3} \cdot t_0 = 1\\
		t_2 &= f'_2(x) \vert_{x=3} \cdot t_0 = 6\\
		t_3 &= \dfrac{\partial f_3}{\partial s_1} \vert_{x=3} \cdot t_1 + \dfrac{\partial f_3}{\partial s_2} \vert_{x=3} \cdot t_2 = 7\\
	\end{cases}
\end{equation}
In backward mode, the gradient accumulators needs to be initialized to 0.
\begin{equation}
	\dfrac{\partial s_3}{\partial s_1} = \dfrac{\partial s_3}{\partial s_2} = \dfrac{\partial s_3}{\partial x} = 0
\end{equation}
Next, the intermediate values are computed in one forward pass
\begin{equation}
	\begin{aligned}
		\dfrac{\partial s_3}{\partial s_1} &+= 1 \Rightarrow \dfrac{\partial s_3}{\partial x} += 1 \cdot 1 \vert_{x=3}\\		
		\dfrac{\partial s_3}{\partial s_2} &+= 1 \Rightarrow \dfrac{\partial s_3}{\partial x} += 1 \cdot 2x \vert_{x=3}\\
	\end{aligned}
\end{equation}
Eventually giving
\begin{equation}
	\dfrac{\partial s_3}{\partial x} = 7
\end{equation}
\section{Jacobian computation}
Forward and backward mode allows the indirect computation of the function's Jacobian matrix. Reminding that, for a function $f:\R^n\to\R^m$,
the jacobian matrix will have dimensions $m\times n$, the following intepretations are possible.

\subsection{Forward mode}
Forward mode computes the function's Jacobian matrix by successive Jacobian-vector products, one for each variable. 
\begin{equation}
  J_f(x) \cdot v \qquad \qquad \text{(JVP)}
\end{equation}
For each input variable, the Jacobian-vector product, representing the directional derivative, is computed with a one-encoding vector $v$ of the said variable
\footnote{The one-hot encoding of a variable is a vector of length $n$ filled with zeros, with $1$ set at the encoded variable's position.}. 
$v$ represents the direction of the input variable.
The directional derivative may be seen as answering to
\begin{formal}
  How does a perturbation to the input propagates to the outputs?
\end{formal}
The full jacobian matrix is obtained as the horizontal concatenation of all Jacobian-vector products computed for each input variable.

\subsection{Backward mode}
Backward mode computes the vector-Jacobian products.
\begin{equation}
  v^T J_f(x) \qquad \qquad \text{(VJP)}
\end{equation}
Here, $v^T$ is the output covector of size $m$. It is a one-hot encoding of one of the outputs. The \emph{VJP} answers to 
\begin{formal}
  Which input perturbations matter for this specific output?
\end{formal} 
In backward mode, the full jacobian matrix is obtained by vertical concatenation of all vector-Jacobian products, computed per output.

\subsection{Comparison}
Consider a function $f:\R^n \to \R^m$. Computing the full Jacobian requires $n$ forward passes (JVP) or $m$ backward passes (VJP). 

Therefore,
\begin{itemize}
  \item If $n \ll m$, the forward mode is faster
  \item If $n \gg m$, the backward mode is faster
  \item If $n \approx m$, none prevail
\end{itemize}
\section{Memory usage}
The forward mode only needs to store the current value and the current derivative. The memory usage stays approximatively constant.\\
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{img/fd_mem.png}
	\caption{Forward mode memory usage}
	\label{fig:fd_mem}
\end{figure}
The backward mode, however, needs to store all intermediate values to compute the derivatives in the backward pass. 
As a consequence, the memory usage will increase during the forward pass, and then reduce during the derivatives computation, in the backward pass.\\
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{img/bd_mem.png}
	\caption{Backward mode memory usage}
	\label{fig:bd_mem}
\end{figure}
Concluding with this section, the forward mode is deemed more memory efficient than the backward mode. 
However, this aspect is generally less significant than the number of operations performed (JVP vs VJP).
\section{Second order AD}
Automatic Differentiation is also able to compute higher-order derivatives, such as the Hessian. 

For any function $f:\R^n\to\R^m$, the Hessian, per output, is a matrix of dimensions $n\times n$. Its entries can be computed as
\begin{equation}
  (\nabla^2 f(x))_{ij} = \dfrac{\partial^2 f(x)}{\partial x_i \partial x_j}
\end{equation}
The Hessian can also be seen as the Jacobian of the function's gradient
\begin{equation}
  \nabla^2 f(x) = J_{\nabla f}(x)
\end{equation}
This last definition clearly shows that AD systems can be used to obtain the Hessian, by computing the gradient and the Jacobian of $f$ as already seen.
Since, the Jacobian and the gradient can be both computed independently, different AD modes (forward / backward) can be used to compute each one of them. 

As a result, 4 Hessian's computation strategies can be defined
\footnote{Since the Hessian per output is $n\times n$, the full Hessian is a 3D tensor of dimensions $m\times n\times n$.
In the following, $m=1$ will always be assumed for the sake of simplicity, but the reasoning stays the same.}.

\subsection{Prelude}
\subsubsection{Rewriting the Hessian}
Based on the chain rule, the Hessian's entries $\dfrac{\partial^2 (f_2 \circ f_1)}{\partial x_i \partial x_j}$ can be rewritten 
into a more AD-suitable expression. 
Let $s_0 = x$ and $s_k = f_k(s_{k-1})$ for $k \ge 1$.
Denote by $J_k \triangleq J_{f_k}(s_{k-1})$ the Jacobian of $f_k$ evaluated at $s_{k-1}$.

%Writing $\partial f_k\triangleq\partial f_k(s_{k-1})$ and $\partial^2f_k\triangleq\partial^2 f_k({s_{k-1}})$, one obtains
% \begin{equation}
%   \begin{aligned}
%     \dfrac{\partial^2 (f_2 \circ f_1)}{\partial x_i \partial x_j} &= \dfrac{\partial}{\partial x_j} \left( \dfrac{ \partial (f_2 \circ f_1)}{\partial x_i} \right) \\
%     &= \dfrac{\partial}{\partial x_j} \left( \partial f_2 \dfrac{ \partial f_1}{\partial x_i} \right) \\
%     &= \left( \partial^2 f_2 \dfrac{\partial f_1}{\partial x_j} \right) \dfrac{\partial f_1}{\partial x_i} + \partial f_2 \dfrac{\partial^2 f_1}{\partial x_i \partial x_j} 
%   \end{aligned}
% \end{equation}
\begin{equation}
  \begin{aligned}
    \frac{\partial^2 (f_2 \circ f_1)}{\partial x_i \partial x_j}
    &= \frac{\partial}{\partial x_j}\left(J_2 \frac{\partial f_1}{\partial x_i}\right) \\
    &=\left(\frac{\partial J_2}{\partial s_1}\frac{\partial f_1}{\partial x_j}\right)\frac{\partial f_1}{\partial x_i}+J_2\frac{\partial^2 f_1}{\partial x_i \partial x_j}
  \end{aligned}
\end{equation}

Introducing $\He_{kj} \triangleq \dfrac{\partial\J_k}{\partial x_j} = \dfrac{\partial \J_k}{s_{k-1}}\J_{k-1}$, 
the final expression reads
\begin{equation}
  \dfrac{\partial^2 (f_2 \circ f_1)}{\partial x_i \partial x_j} = \He_{2j} \dfrac{\partial f_1}{\partial x_i} + \J_2 \dfrac{\partial^2 f_1}{\partial x_i \partial x_j}
\end{equation}

\subsubsection{\text{Dual} numbers}
\emph{\text{Dual} numbers} are a concept borrowed from algebra. Initially introduced in 1873 to represent 
the \hyperlink{https://en.wikipedia.org/wiki/\text{Dual}\_number\#History}{angle} between lines in space, it can also be applied to automatic differentiation.

A \text{Dual} number is an expression of the form $a+b\varepsilon$ where $a$ and $b$ are real numbers and $\varepsilon$ is a symbol such that $\varepsilon^2=0$ 
with $\varepsilon\ne0$. This allows defining the product of two \text{Dual} numbers as 
\begin{equation}
  (a+b\varepsilon)(c+d\varepsilon)= ac + (ad+bc)\varepsilon  
\end{equation}
One can readily see an application equivalent to the Taylor's first-order approximation, via the Taylor series expansion.
\begin{equation}
  f(a+b\varepsilon)=\sum^\infty_{n=0}\dfrac{f^{(n)}(a)b^n\varepsilon^n}{n!}
  \overset{\footnote{Since every term involving $\varepsilon^2$ or greater power of $\varepsilon$ vanishes.}}=f(a)+bf'(a)\varepsilon
\end{equation}  

Back to the automatic differentiation concept, $f(a)$ is the function's value evaluated at $a$, while $bf'(a)\varepsilon$ is the Jacobian-vector product 
evaluated at $a$. 
\begin{itemize}
  \item $b$ represents the tangent vector
  \item $f'(a)$ is the derivative (Jacobian) evaluated at $a$ in the scalar-output case
\end{itemize}
\text{Dual} numbers encode \emph{exactly} the derivative of a function.

In the following sections, this process is extended to multivariate functions.
\subsection{Forward on forward}
\begin{formal}
  "Directional derivative of a directional derivative"
\end{formal}
\subsubsection{Mathematically}
The forward on forward strategy computes
\begin{equation}
  D(D(f)[v])[w]= w^THv
\end{equation}
where $v,w$ may be any arbitrary direction vectors.
Usually, when computing the full Hessian, they are taken as the one-hot encodings of the variables the differentiation is made with respect to.

\subsubsection{Algorithmically}
Defining $\text{Dual}(s_1\mid t_1)$, with $s_1 = \text{Dual}\left(f_1(x)\ \middle|\ \dfrac{\partial f_1}{\partial x_j}\right)$ and 
$t_1 = \text{Dual}\left(\dfrac{\partial f_1}{\partial x_i}\ \middle|\ \dfrac{\partial^2 f_1}{\partial x_i \partial x_j}\right)$, the forward-on-forward algorithm reads as follows.
\begin{enumerate}
  \item Compute $s_2 = f_2(s_1) = \text{Dual}\left(f_2(f_1(x))\ \middle|\ \J_2 \dfrac{\partial f_1}{\partial x_j}\right)$
  \item Compute $\J_{f_2}(s_1)$, which gives $\text{Dual}(\J_2\mid\He_{2j})$
  \item Compute 
  \begin{equation} 
    \begin{aligned} 
      t_2 &= \J_{f_2}(s_1) t_1 \\ &= \text{Dual}(\J_2\mid\He_{2j}) 
              \text{Dual}\left(\dfrac{\partial f_1}{\partial x_i}\ \middle|\ \dfrac{\partial^2 f_1}{\partial x_i \partial x_j}\right) \\ 
          &= \text{Dual}\left(\J_2 \dfrac{\partial f_1}{\partial x_i}\ \middle|\ \J_2 \dfrac{\partial^2 f_1}{\partial x_i \partial x_j} + \He_{2j} \dfrac{\partial f_1}{\partial x_i}\right) 
    \end{aligned} 
  \end{equation}
  \item Repeat 
\end{enumerate}

This process can be summarized as the following equations, with $g_k(x) = f_k \circ \dots \circ f_1$.
\begin{equation}
  \begin{cases}
    s_k &= \text{Dual}\left(g_k(x)\ \middle|\ \dfrac{\partial g_k}{\partial x_j}\right)\\
    t_k &= \text{Dual}\left(\dfrac{\partial g_k}{\partial x_i}\ \middle|\ \dfrac{\partial^2 g_k}{\partial x_i \partial x_j}\right)
  \end{cases}
\end{equation}

\warning\ As one may see, the value of $w^THv$ is a scalar.
The full process must be repeated for every pair of input variables, this strategy requires $n^2$ evaluations 
and is therefore impractical except for very small $n$.
\subsection{Forward on reverse}
\begin{formal}
  "Directional derivative of the gradient"
\end{formal}

\subsubsection{Mathematically}
The forward on reverse strategy computes the Hessian-Vector Product (HVP)
\begin{equation}
  D(\nabla f)[v]=Hv
\end{equation}

\subsubsection{Algorithmically}
The forward-on-reverse strategy consists in applying forward-mode AD
to the reverse-mode computation of the gradient.
Operationally, this results in:
\begin{enumerate}
  \item a forward pass propagating both primal values and tangents,
  \item a backward pass in which adjoint variables are augmented with
        tangent components.
\end{enumerate}

Defining $s_1 \triangleq \text{Dual}\left(f_1(x)\ \middle|\ \dfrac{\partial f_1}{\partial x_j}\right)$ 
\begin{enumerate}
  \item Compute $s_2 = f_2(s_1) $
  \item Compute $\J_{f_2}(s_1)$ which gives $\text{Dual}(\J_2, \He_{2j})$
\end{enumerate}
The backward pass follows. 

Defining $r_2 \triangleq \text{Dual}(r_{2,1}\mid r_{2,2})$, the computation is
\begin{equation}
  \begin{aligned}
    r_2 \J_2 &= \text{Dual}(r_{2,1}\mid r_{2,2}) \text{Dual}(\J_2\mid \He_{2j})\\ &= \text{Dual}(r_{2,1} \J_2\mid r_{2,1} \He_{2j} + r_{2,2} \J_2)
  \end{aligned}
\end{equation}

Using this last equation, the recurrence relation is obtained.
\begin{equation}
  r_k = \text{Dual}\left(\dfrac{\partial f}{\partial s_k}\ \middle|\ \dfrac{\partial^2 f}{\partial s_k \partial x_j}\right)
\end{equation}
where the backward recurrence follows the same graph structure as
reverse-mode AD, and each adjoint update is performed using Dual
arithmetic.

Here, the tangent component of each adjoint stores the mixed second derivative.
\subsection{Reverse on forward}

\subsubsection{Mathematically}
The reverse-on-forward strategy computes the Vector-Hessian Product (VHP)
\begin{equation}
  D(D(f)[v])^*[w]=H^Tw=Hw
\end{equation}
The last equation stands due to the symmetric nature of the Hessian matrix.

\subsubsection{Algorithmically}
The reverse-on-forward strategy starts with a forward pass.

Defining $s_1\triangleq \text{Dual}\left(f_1(x)\ \middle|\ \dfrac{\partial f_1}{\partial x_i }\right)$
\footnote{Note the difference with the previous modes ($j \to i$)}, the algorithm follows
\begin{enumerate}
  \item Compute $s_2 = f_2(s_1) = \text{Dual}\left(f_2(s_1)\ \middle|\ \J_2 \dfrac{\partial f_1}{\partial x_i}\right)$
  \item The reverse mode computes the local Jacobian.

  Since $s_1$ and $s_2$ are Dual-valued variables, the Jacobian $\partial s_2 / \partial s_1$ is taken with respect to the primal and tangent components.
  \begin{equation}
    \dfrac{\partial s_2}{\partial s_1} = \begin{bmatrix}
      \J_2 & 0\\
      \He_{2i} & \J_2
    \end{bmatrix} % = \dfrac{\partial ((s_2)_1,(s_2)_2)}{\partial ((s_1)_1,(s_1)_2)}
  \end{equation} 
\end{enumerate}
Next, the backward pass is applied, giving:
\begin{equation}
  \begin{cases}
    r_{1, 1} = r_{2, 1} \J_2 + r_{2, 2} \He_{2i}\\
    r_{1, 2} = r_{2, 2} \J_2
  \end{cases}
\end{equation}
The solution of the recurrence equation is given by
\begin{equation}
  r_k = \text{Dual}\left(\dfrac{\partial f}{\partial s_k}\ \middle|\ \dfrac{\partial^2 f}{\partial s_k \partial x_i}\right)
\end{equation}

\subsection{Reverse on reverse}
\subsubsection{Mathematically}

\begin{equation}
  \nabla^2f(x)=J_{\nabla f}(x)
\end{equation}
Equivalently, for a given direction $w$, it computes the Vector-Hessian Product (VHP)
\begin{equation}
  D(\nabla f)^*[w]=H^Tw=Hw
\end{equation}
The last equation stands due to the symmetric nature of the Hessian matrix. 
Repeating this process for every basis $w=e_i$ yields the full Hessian.

\subsubsection{Order}
The reverse-on-forward strategy applies the reverse-mode to the reverse-mode computation of the gradient.
\begin{enumerate}
  \item A forward pass to obtain function's values
  \item A first reverse pass to compute first-order adjoints
  \item A second reverse pass to compute second-order adjoints
\end{enumerate}

\subsubsection{Forward pass (values computation)}
Let the primal variables 
\begin{equation}
  s_0 = x,\quad s_k = fk(s_{k-1}),\quad k=1,\dots K
\end{equation}
During the forward pass, the $K$ intermediary values are stored, as in standard reverse-mode AD.

\subsubsection{First reverse pass (gradient computation)}
Define the first-order adjoints 
\begin{equation}
  r_k\triangleq\frac{\partial f}{\partial s_k}
\end{equation}
They satisfy the recurrence equations obtained above
\begin{equation}
  \begin{cases}
    r_K=1,\\
    r_{k-1}=r_k\J_k
  \end{cases}
\end{equation}
with $\J_k=\frac{\partial f_k}{\partial s_{k-1}}$.

At the end of the pass, we obtain $\nabla f(x)=r_0$. All adjoints are stored, as they are required for the second reverse pass.

\subsubsection{Second reverse pass (second-order adjoints)}

The adjoint equations are differentiates w.r.t themselves.

Let $\dot{r}_k$ denote the \emph{second-order adjoint} defined by 
\begin{equation}
  \dot r_k\triangleq\frac{\partial}{\partial x_i}\left(\frac{\partial f}{\partial s_k}\right)
\end{equation}
for a fixed input direction $e_i$ (one-hot encoding of $x_i$).

Differentiating the first-order recurrence
\begin{equation}
  r_{k-1}=r_k\J_k
\end{equation}
w.r.t $x_i$ yields
\begin{equation}
  \dot r_{k-1}=\dot r_k\J_k+r_k\frac{\partial \J_k}{\partial x_i}
\end{equation}
Using the chain rule,
\begin{equation}
  \frac{\partial \J_k}{\partial x_i}=\frac{\partial \J_k}{\partial s_{k-1}}\frac{\partial s_{k-1}}{\partial x_i} = \He_{ki}
\end{equation}
where $\He_{ki}$ denotes the Hessian of $f_k$ contracted with the forward sensitivity $\dfrac{\partial s_{k-1}}{\partial x_i}$.

The second-order reverse recurrence becomes 
\begin{equation}
  \dot r_{k-1}=\dot r_k\J_k+r_k\He_{ki}
\end{equation}
with the initial condition $\dot r_K=0$ since $r_K=1$ is constant.

After completing the second reverse pass, the Hesian column corresponding to direction $e_i$ is obtained as 
\begin{equation}
  \nabla^2f(x)e_i=\dot r_0
\end{equation}
Repeating the process for all $i=1,\dots n$ yields the full Hessian matrix.

\subsection{Comparison}
\begin{table}[H]
\centering
\label{tab:second_order_ad_summary}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\textbf{Strategy} 
& \textbf{Object computed} 
& \textbf{Output per run} 
& \textbf{Passes} 
& \textbf{Time cost} 
& \textbf{Memory cost}\\
\hline
FoF 
& $w^T H v$ 
& Scalar 
& $1 \to$ 
& $\mathcal{O}(K)$ 
& Low \\
\hline
FoR 
& $H v$ 
& $\mathbb{R}^n$ 
& $1 \to + 1 \leftarrow$ 
& $\mathcal{O}(K)$ 
& Moderate \\
\hline
RoF 
& $H w$ 
& $\mathbb{R}^n$ 
& $1 \to + 1 \leftarrow$
& $\mathcal{O}(K)$ 
& Moderate \\
\hline
RoR 
& $\nabla^2 f$ (column-wise) 
& $\mathbb{R}^n$ 
& $1 \to + 2 \leftarrow$ 
& $\mathcal{O}(K)$ 
& Very high \\
\hline
\end{tabular}
\caption{Comparison of second-order AD strategies ($\to$: forward, $\leftarrow$: backward)}
\end{table}

\begin{table}[H]
\centering
\label{tab:second_order_ad_features}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Criterion} 
& \textbf{FoF} 
& \textbf{FoR} 
& \textbf{RoF} 
& \textbf{RoR} \\
\hline
Computes full Hessian directly 
& \textcolor{red}{No} & \textcolor{red}{No} & \textcolor{red}{No} & \textcolor{red}{No} \\
\hline
Produces Hessian-vector product 
& \textcolor{red}{No} & \textcolor{green}{Yes} & \textcolor{green}{Yes} & \textcolor{green}{Yes} \\
\hline
Uses dual numbers 
& \textcolor{green}{Yes} & \textcolor{green}{Yes} & \textcolor{green}{Yes} & \textcolor{red}{No} \\
\hline
Requires primal trace storage 
& \textcolor{red}{No} & \textcolor{green}{Yes} & \textcolor{green}{Yes} & \textcolor{green}{Yes} \\
\hline
Requires adjoint storage 
& \textcolor{red}{No} & \textcolor{green}{Yes} & \textcolor{green}{Yes} & \textcolor{green}{Yes} \\
\hline
Requires second derivatives of primitives 
& \textcolor{green}{Yes} & \textcolor{green}{Yes} & \textcolor{green}{Yes} & \textcolor{green}{Yes} \\
\hline
Exploits Hessian symmetry 
& \textcolor{red}{No} & \textcolor{green}{Yes} & \textcolor{green}{Yes} & \textcolor{green}{Yes} \\
\hline
Suitable for large $n$ 
& \textcolor{red}{No} & \textcolor{green}{Yes} & \textcolor{green}{Yes} & \textcolor{red}{No} \\
\hline
Implementation complexity 
& Low & Medium & Medium & Very high \\
\hline
\end{tabular}
\caption{Feature comparison of Hessian computation strategies}
\end{table}

\begin{table}[h]
\centering
\label{tab:full_hessian_cost}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Strategy} 
& \textbf{Number of runs} 
& \textbf{Total cost} \\
\hline
FoF 
& $n^2$ 
& $\mathcal{O}(n^2 K)$ \\
\hline
FoR 
& $n$ 
& $\mathcal{O}(n K)$ \\
\hline
RoF 
& $n$ 
& $\mathcal{O}(n K)$ \\
\hline
RoR 
& $n$ 
& $\mathcal{O}(n K)$ (large constant) \\
\hline
\end{tabular}
\caption{Cost of assembling the full Hessian matrix}
\end{table}

Among the four strategies, forward-on-reverse is generally preferred in practice, as it provides Hessian-vector products 
at a computational cost comparable to that of a gradient evaluation, while avoiding the prohibitive memory requirements 
of reverse-on-reverse differentiation.

\section{Implicit differentiation}
Above sections considered \emph{explicit} functions where each intermediary step was known. 
\emph{Implicit} functions differentiation is considered when the intermediary steps are hidden and only the output is known (e.g. Newton's method, Gradient Descent).

Usual AD is inefficient for this type of functions due to the requirements of unrolling the implicit function behavior.
Instead, one could use the fact that the derivatives depends on the solution and the path.

For instance, one may consider the square root function $x=\sqrt a$ and extract its implicit formulation through a fixed-point equation.
\begin{equation}
  \begin{aligned}
	x &= \sqrt{a} \\ 
	x^2 &= a \\
	2x^2 &= x^2 + a \\
	g(x,a) = x &= \frac{1}{2} \left( x + \frac{a}{x} \right)
  \end{aligned}
\end{equation}
Iterating, using $x_{k+1} = g(x_k,a)$, from an initial guess $x_0$, the solution would eventually converge to $x^* = \sqrt{a}$. 
The implicit equation is $F(x,a) = x - g(x,a)$ and its solution satisfies $F(x^*,a) = 0$.

\begin{thm}[Inverse function theorem]
  Assume
  \begin{itemize}
    \item $f:\mathcal{W} \to \mathcal{W}$ is $C^2$
    \item $\partial f (w_0)$ is invertible.
  \end{itemize}
  Then 
  \begin{itemize}
    \item $f$ is bijective from a neighborhood of $w_0$ to a neighborhood of $f(w_0)$
    \item For $\omega$ in a neighborhood of $f(w_0)$, $f^{-1}$ is $C^2$ and $\partial f^{-1}(\omega)=(\partial f(f^{-1}(w)))^{-1}$
    where the last equation has been obtained through the chain rule.
  \end{itemize}
\end{thm}

\begin{thm}[Implicit function theorem (IFT, univariate case)]
  Assume
  \begin{itemize}
    \item $F:\R \to \R$
    \item $\exists (w_0, \lambda_0)$ such that $F(w_0, \lambda_0) = 0$ and $\partial_1 F(w_0, \lambda_0) \neq 0$
    \item $F(w,\lambda)$ is $C^2$ in a neighborhood $\mathcal{U}$ of $(w_0, \lambda_0)$ 
  \end{itemize}
  Then there exists a neighborhood $\mathcal{V} \subseteq \mathcal{U}$ where exists $w^*(\lambda)$ such that:
  \begin{equation}
	\begin{aligned}
		w^*(\lambda_0) &= w_0 \\
		F(w^*(\lambda), \lambda) &= 0, \quad \forall (w^*(\lambda),\lambda) \in \mathcal{V} \\
		\partial w^*(\lambda) &= - \left(\partial_1 F (w^*(\lambda), \lambda)\right)^{-1}\partial_2 F(w^*(\lambda), \lambda)
	\end{aligned}
  \end{equation}
\end{thm}
\begin{exmp}
  Implicit relation between $x$ and $y$
  \begin{equation*}
    x^2+y^2 = 1
  \end{equation*}
  Two possible explicit functions
  \begin{equation*}
    \begin{aligned}
      y^+(x)&=\sqrt{1-x^2}\\
      y^-(x)&=-\sqrt{1-x^2}
    \end{aligned}
  \end{equation*}
  Let $F(y,x)=x^2+y^2-1$. Given an initial point $(x_0,y_0)$ with $x^2_0+y^2_0=1$.
  \begin{itemize}
    \item if $y_0>0$ then IFT holds and $y^*(x)=y^+(x)$,
    \item if $y_0<0$ then IFT holds and $y^*(x)=y^-(x)$,
    \item if $y_0=0$ then $\partial_1 F(y_0,x_0)=2y_0=0$ so IFT does not hold
  \end{itemize}
  This example shows that even a function such as $F(y,x)=x^2+y^2-1$ with 2 possible explicit functions does not violate the IFT.
\end{exmp}
\begin{thm}[Implicit function theorem (IFT, multivariate case)]
  Assume
  \begin{itemize}
	\item $F:\mathcal{W} \times \Lambda \to \mathcal{W}$
	\item $\exists (w_0, \lambda_0)$ such that $F(w_0, \lambda_0) = 0$ and $\partial_1 F(w_0, \lambda_0) \neq 0$
	\item $F(w,\lambda)$ is $C^2$ in a neighborhood $\mathcal{U}$ of $(w_0, \lambda_0)$ 
  \end{itemize}
  Then there exists a neighborhood $\mathcal{V} \subseteq \mathcal{U}$ where exists $w^*(\lambda)$ such that:
  \begin{equation}
	\begin{aligned}
		w^*(\lambda_0) &= w_0 \\
		F(w^*(\lambda), \lambda) &= 0, \quad \forall (w^*(\lambda),\lambda) \in \mathcal{V} \\
		\partial w^*(\lambda) &= - \left(\partial_1 F(w^*(\lambda), \lambda)\right)^{-1}\partial_2 F(w^*(\lambda), \lambda)
	\end{aligned}
  \end{equation}
\end{thm}
\subsection{Implicit JVP and VJP}
For implicit differentiation, derivatives must be propagated differently.

Consider the implicit function defined by $F(w^*, \lambda) = 0$, from which one may want to compute the JVP and VJP.

\subsubsection{JVP}
Reminding that the forward tangent a step $k$, $t_k$, is computed using the known previous tangent $t_{k-1}$, and the function at the step,
assuming the IFT holds, the following equality may be derived.
\begin{equation}
	t_k = - \left( \partial_1 F(w^*, \lambda) \right)^{-1} \cdot \partial_2 F(w^*, \lambda) \cdot t_{k-1}
\end{equation}
where $t_k = \partial w^*(\lambda)/\partial s_0$ and $t_{k-1} = \partial \lambda / \partial s_0$. 
Each JVP of the implicit function may be obtaine by solving a linear system. 

Setting $A = -\partial_1 F(w^*(\lambda), \lambda)$ and $B = \partial_2 F(w^*(\lambda), \lambda)$, 
the system to solve becomes:
\begin{equation}
	t_k = A^{-1} B t_{k-1}
\end{equation}
Once the forward pass is done, $A$ is fixed and the linear system can be solved efficiently for each JVP by using the LU decomposition of $A$.\\
\begin{equation}
	\begin{aligned}
		LU t_k = B t_{k-1}&\\
		\Downarrow \qquad&\\
		\begin{cases}
			L y &= B t_{k-1} \\
			U t_k &= y
		\end{cases}
	\end{aligned}
\end{equation}

\subsubsection{VJP}

Computing the VJP boils down to the computation of the reverse tangent $r_{k-1}$ knowing the next reverse tangent $r_k$ and the function at this step. 
Assuming the IFT holds, the following equality can be derived.
\begin{equation}
  r_{k-1} = - \left( \partial_2 F(w^*, \lambda) \right)^T \cdot \left( \partial_1 F(w^*, \lambda) \right)^{-T} \cdot r_k
\end{equation}
where $r_{k-1} = \partial s_0 / \partial \lambda$ and $r_k = \partial s_0 / \partial w^*(\lambda)$.
Denoting $A = \partial_1 F(w^*(\lambda), \lambda)$ and $B = \partial_2 F(w^*(\lambda), \lambda)$, 
the VJP can also be obtained as the solution of a linear system.
\begin{equation}
  \begin{cases}
	A^T y = r_k \\
	r_{k-1} = - B^T y
  \end{cases}
\end{equation}
\subsection{AD with optimization problem}
Consider the optimization problem defined by:
\begin{equation}
  \min c^Tx \quad \text{s.t.} \quad Ax = b, \quad x \geq 0
\end{equation}
and its dual:
\begin{equation}
  \max b^Ty \quad \text{s.t.} \quad A^Ty \leq c
\end{equation}
The KKT conditions gives the optimality conditions for this problem:
\begin{equation}
  \begin{cases}
	Ax = b \\
	(A^Ty - c) \perp x \geq 0 
  \end{cases}
\end{equation}
The KKT conditions can be rewritten as an implicit function $F((x,y),(A,b,c))$:
\begin{equation}
  F((x,y),(A,b,c)) = \begin{bmatrix}
	Ax - b \\
	Diag(x)(A^Ty - c)
  \end{bmatrix}
\end{equation}
If the IFT holds for this function, then $\partial_1 F(w^*, \lambda)$ can be computed.
\begin{equation}
  \frac{\partial F}{\partial (x,y)} = \begin{bmatrix}
	A & 0 \\
	Diag(A^Ty - c) & Diag(x)A^T
  \end{bmatrix}
\end{equation}
\section{Sparse AD}
Often in practice, the Jacobian matrix is sparse, meaning that many of its entries are zero. In such cases, sparse AD techniques can be used 
to exploit the sparsity pattern and reduce the computational cost of the Hessian. 
For this section, the sparsity pattern of the Jacobian matrix is assumed to be known.

Consider for instance the following Jacobian matrix:
\begin{equation}
  J = \begin{bmatrix}
	0 & * & 0 & * & 0\\
	0 & 0 & 0 & * & *\\
	0 & * & * & 0 & 0\\
	* & 0 & * & 0 & 0
  \end{bmatrix}
\end{equation}
Computing the Hessian using the standard AD techniques would require 5 JVP or 4 VJP. 
It is possible to reduce the cost by grouping rows or columns together as long as they do not share any non-zero entry. 
For instance, with the Jacobian matrix above, the Hessian can be computed with only 2 JVP or 2 VJP. 

The intuition comes from the fact that when we compute a JVP or a VJP, the zero entries do not contribute to the result.
Hence, columns or rows can be grouped together if they do not share non-zero entries. 

For instance, computing the first two JVPs yields
\begin{equation}
	J\begin{bmatrix} 1 \\ 0 \\ 0 \\ 0 \\ 0 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ 0 \\ * \end{bmatrix} \qquad \text{and} \qquad
	J\begin{bmatrix} 0 \\ 1 \\ 0 \\ 0 \\ 0 \end{bmatrix} = \begin{bmatrix} *\\ 0 \\ * \\ 0 \end{bmatrix}
\end{equation}
Combining these two JVPs gives:
\begin{equation}
	J\begin{bmatrix} 1 \\ 1 \\ 0 \\ 0 \\ 0 \end{bmatrix} = \begin{bmatrix} * \\ 0 \\ * \\ * \end{bmatrix}
\end{equation}
Using the known sparsity pattern, the results can be put back in the right place. This process is called \emph{decompression}. 

Hence computing the Hessian boilds down to combining the following columns and using them as vectors for the JVPs:
\begin{equation}
  \begin{bmatrix}
	0 & \textcolor{red}{*} & 0 & \textcolor{blue}{*} & 0\\
	0 & 0 & 0 & \textcolor{blue}{*} & \textcolor{red}{*}\\
	0 & \textcolor{red}{*} & \textcolor{blue}{*} & 0 & 0\\
	\textcolor{red}{*} & 0 & \textcolor{blue}{*} & 0 & 0
  \end{bmatrix} \qquad \Rightarrow \qquad \textcolor{red}{v_1}=\begin{bmatrix} 1 \\ 1 \\
	0 \\ 0 \\ 1 \end{bmatrix}, \quad \textcolor{blue}{v_2}=\begin{bmatrix} 0 \\ 0 \\ 1 \\ 1 \\ 0 \end{bmatrix}
\end{equation}
Or for the VJP:
\begin{equation}
  \begin{bmatrix}
	0 & \textcolor{red}{*} & 0 & \textcolor{red}{*} & 0\\
	0 & 0 & 0 & \textcolor{blue}{*} & \textcolor{blue}{*}\\
	0 & \textcolor{blue}{*} & \textcolor{blue}{*} & 0 & 0\\
	\textcolor{red}{*} & 0 & \textcolor{red}{*} & 0 & 0
  \end{bmatrix} \qquad \Rightarrow \qquad \textcolor{red}{v_1}=\begin{bmatrix} 1 & 0 & 0 & 1 \end{bmatrix}, \quad \textcolor{blue}{v_2}=\begin{bmatrix} 0 & 1 & 1 & 0 \end{bmatrix}
\end{equation}
\subsection{Sparsity detection}
The problem of grouping the columns or rows of a sparse matrix can be modeled as a graph coloring problem in many ways.
For instance, it could be a graph where each column is a node and where an edge connects two nodes if they share a non-zero entry. 

Once the problem is modeled, any graph coloring algorithm can be used to color the graph such that no two adjacent nodes have the same color. 
As a result, each color represents a group of columns that can be combined together for the JVPs. The same can be done for the rows for the VJPs. 

Here is a visual example.
\begin{equation}
  J = \begin{bmatrix}
	0 & \textcolor{red}{*} & 0 & \textcolor{blue}{*} & 0\\
	0 & 0 & 0 & \textcolor{blue}{*} & \textcolor{red}{*}\\
	0 & \textcolor{red}{*} & \textcolor{blue}{*} & 0 & 0\\
	\textcolor{red}{*} & 0 & \textcolor{blue}{*} & 0 & 0
  \end{bmatrix}
\end{equation}
\begin{center}
  \begin {tikzpicture}[-latex ,auto ,node distance = 2.5 cm and 2.5 cm ,on grid ,
	semithick ,
	state1/.style ={ circle ,top color =white , bottom color = red ,
	draw,red , text=black , minimum width =1 cm}, state2/.style ={ circle ,top color =white , bottom color = blue ,
	draw,blue , text=black , minimum width =1 cm}]
	\node[state1] (E) {$5$};
	\node[state2] (D) [above right=of E] {$4$};
	\node[state2] (C) [above left=of E] {$3$};
	\node[state1] (B) [above =of D] {$2$};
	\node[state1] (A) [above =of C] {$1$};
	\path (A) edge (C);
	\path (C) edge (B);
	\path (B) edge (D);
	\path (D) edge (E);
  \end{tikzpicture}
\end{center}
In more complex sparsity patterns, somes nodes could be linked multiple times, and therefore a particular care must be taken to ensure no multipy linked nodes
share an edge.
\begin{equation}
  J = \begin{bmatrix}
	0 & 0 & * \\
	* & * & 0 \\
	* & * & 0
  \end{bmatrix}
\end{equation}
Node 1 and 2 should be linked by two edges because they share two non-zero entries, hence the colors must be different.

The probem can also be modeled as a bipartite graph, where one set of nodes represents the columns and the other set represents the rows. 
There will be an edge between a column node and a row node if the corresponding entry in the matrix is non-zero. 
Considering the same instance
\begin{equation}
  J = \begin{bmatrix}
	0 & \textcolor{green}{*} & 0 & \textcolor{green}{*} & 0\\
	0 & 0 & 0 & \textcolor{violet}{*} & \textcolor{violet}{*}\\
	0 & \textcolor{violet}{*} & \textcolor{violet}{*} & 0 & 0\\
	\textcolor{green}{*} & 0 & \textcolor{green}{*} & 0 & 0
  \end{bmatrix}  \qquad or \qquad \begin{bmatrix}
	0 & \textcolor{red}{*} & 0 & \textcolor{blue}{*} & 0\\
	0 & 0 & 0 & \textcolor{blue}{*} & \textcolor{red}{*}\\
	0 & \textcolor{red}{*} & \textcolor{blue}{*} & 0 & 0\\
	\textcolor{red}{*} & 0 & \textcolor{blue}{*} & 0 & 0
  \end{bmatrix} 
\end{equation}
\begin{center}
  \begin {tikzpicture}[-latex ,auto ,node distance = 2 cm and 4 cm ,on grid ,
	semithick ,
	state1/.style ={ circle ,top color =white , bottom color = red , draw,red , text=black , minimum width =1 cm}, state2/.style ={ circle ,top color =white , bottom color = blue , draw,blue , text=black , minimum width =1 cm}, state3/.style ={ circle ,top color =white , bottom color = green , draw,green , text=black , minimum width =1 cm}, state4/.style ={ circle ,top color =white , bottom color = violet , draw,violet , text=black , minimum width =1 cm}]
	\node[state3] (A) {$r_1$};
	\node[state4] (B) [below=of A] {$r_2$};
	\node[state4] (C) [below=of B] {$r_3$};
	\node[state3] (D) [below=of C] {$r_4$};
	\node[state1] (E) [right=of A] {$c_1$};
	\node[state1] (F) [below=of E] {$c_2$};
	\node[state2] (G) [below=of F] {$c_3$};
	\node[state2] (H) [below=of G] {$c_4$};
	\node[state1] (I) [below=of H] {$c_5$};
	\path (A) edge (F);
	\path (A) edge (H);
	\path (B) edge (H);
	\path (B) edge (I);
	\path (C) edge (F);
	\path (C) edge (G);
	\path (D) edge (E);
	\path (D) edge (G);
  \end{tikzpicture}
\end{center}
With this modeling, the coloring problem is a distance-2 graph coloring problem. 
Two column nodes being at distance 2 from each other can't share the same color because if they share a non-zero entry ar row $n$, 
they will be connected to the same row node $n$.

The same applies for the rows.

\subsection{Symmetric matrix}
When the Jacobian matrix is symmetric, a more efficient algorithm can be used to compute the HVP. 
For instance, consider the following symmetric Jacobian matrix.
\begin{equation}
  J = \begin{bmatrix}
	a_1 & a_2 & a_3 & 0\\
	a_2 & a_4 & 0 & a_5\\
	a_3 & 0 & a_6 & 0\\
	0 & a_5 & 0 & a_7
  \end{bmatrix}
\end{equation}
With the previous sparse AD techniques, 3 HVPs or 3 VHPs would be required, instead of 4 wit the standard AD. 
Using the symmetry of the matrix, this computations can be reduced to 2 HVPs or 2 VHPs. Indeed, columns can colored as
\begin{equation}\label{eq:sym_norm}
  J = \begin{bmatrix}
	\textcolor{red}{a_1} & \textcolor{blue}{a_2} & \textcolor{blue}{a_3} & 0\\
	\textcolor{red}{a_2} & \textcolor{blue}{a_4} & 0 & \textcolor{red}{a_5}\\
	\textcolor{red}{a_3} & 0 & \textcolor{blue}{a_6} & 0\\
	0 & \textcolor{blue}{a_5} & 0 & \textcolor{red}{a_7}
  \end{bmatrix}
\end{equation}
Using the symmetry property, the entry shared between columns 2 and 3 can be ignored since this value would be also computed by the HVP on the 
first column. Hence, it can be considered as 0 for now, and replaced back later.
 
Computing the two HVPs and retrieving the full Hessian matrix shows
\begin{equation}\label{eq:sym_norm_sol}
  J \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 0 & 1 \\ 1 & 0 \end{bmatrix} = \begin{bmatrix} a_1 & a_2+a_3 \\ a_2 +a_5 & a_4 \\ a_3 & a_6 \\ a_7 & a_5 \end{bmatrix}
\end{equation}
$a_2$ can be obtained by solving a small linear system. 

Using computed values and knowing the sparsity pattern, the full matrix can be reconstructed. 

The same process can be developed for VHPs by grouping rows instead of columns.

\subsection{Star coloring}

Given a graph $G=(V,E)$, and a coloring function $\phi: V \to \{1, \ldots, p\}$, a star coloring is distance-1 coloring with the additional constraint 
that every path of 4 vertices uses at least 3 colors. In other words, no path of length 3 is \emph{bi-chromatic}. This prevents \emph{length-3} dependency chains.
For instance, considering the same symmetric Jacobian matrix as above
\begin{equation}
  J = \begin{bmatrix}
	a_1 & a_2 & a_3 & 0\\
	a_2 & a_4 & 0 & a_5\\
	a_3 & 0 & a_6 & 0\\
	0 & a_5 & 0 & a_7
  \end{bmatrix}
\end{equation}
Coloring its associated graph with the classic distance-1 coloring would yield the same matrix as \eqref{eq:sym_norm}.
Computing the HVPs, one would obtain the matrix \eqref{eq:sym_norm_sol} and solving a linear system would be required to retrieve $a_2$. 

Usings star coloring, one could retrieve explicitly all the element. 

Consider the following coloring
\begin{equation}
  J = \begin{bmatrix}
	\textcolor{red}{a_1} & \textcolor{blue}{a_2} & \textcolor{blue}{a_3} & 0\\
	\textcolor{red}{a_2} & \textcolor{blue}{a_4} & 0 & \textcolor{green}{a_5}\\
	\textcolor{red}{a_3} & 0 & \textcolor{blue}{a_6} & 0\\
	0 & \textcolor{blue}{a_5} & 0 & \textcolor{green}{a_7}
  \end{bmatrix}
\end{equation}
Using the green color ensures the star coloring condition. 
Now, computing the 3 HVPs
\begin{equation}
  J \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} = \begin{bmatrix} a_1 & a_2 + a_3 & 0 \\ a_2 & a_4 & a_5 \\ a_3 & a_6 & 0\\ 0 & a_5 & a_7 \end{bmatrix}
\end{equation}
All the values can be immediately retrieved from the matrix without solving any linear equation system. 

This proves its better efficiency compared to the usual distance-1 coloring (4 HVPs), and the symmetric distance-1 coloring (2 HVPs + linear system).
\subsection{Acyclic coloring}
Given a graph $G=(V,E)$, and a coloring function $\phi: V \to \{1, \ldots, p\}$, an acyclic coloring is a distance-1 coloring with the additional constraint 
that every cycle uses at least 3 colors. This allows to form forests of disjoints trees with the subgraph of each color pair. 

%With that, we can do this little algorithm:\\
% \begin{enumerate}
%   \item Compute HVPs for each color
%   \item Look at a 2-color subgraph (a tree)
%   \item Read all leaf edges
%   \item Substract their contribution from their parent
%   \item Remove the leaves
%   \item Repeat until the tree is empty
% \end{enumerate}

% For example, consider the following Hessian matrix:
% \begin{equation}
%   J = \begin{bmatrix}
%     a_1 & a_2 & 0 & a_3 & 0\\
%     a_2 & a_4 & a_5 & 0 & 0\\ 
%     0 & a_5 & a_6 & 0 & a_7\\
%     a_3 & 0 & 0 & a_8 & a_9\\
%     0 & 0 & a_7 & a_9 & a_{10}
%   \end{bmatrix}
% \end{equation}
\textcolor{red}{TODO better explain HVP computation with acyclic coloring and give an example}
\subsection{Chromatic number ($\xi$)}
For every graph G,
\begin{equation}
  \xi_1(G) \leq \xi_{acyclic} (G) \leq \xi_{star}(G) \leq \xi_2(G)
\end{equation}
$\xi_1$ is the usual distance-1 chromatic number, $\xi_2$ the distance-2 chromatic number.
\chapter{Neural networks}
% \textcolor{red}{TODO link tangent with neural networks}\\
Neural networks are a class of machine learning models inspired by the structure and function of the human brain. They are composed of layers of interconnected nodes (neurons) that process and transmit information.\\
First let's define some variables:
\begin{itemize}
  \item $X$: input data (matrix)
  \item $y$: target data
  \item $W_k$: weights matrix at layer $k$
  \item $b_k$: bias vector at layer $k$
  \item $\sigma$: activation function (ReLU, sigmoid, etc)
  \item $\ell(.)$: loss function 
  \item $H$: number of hidden layers
  \item $S_i$: intermediate state  
\end{itemize} 
To propagate and update the information through the network we use forward pass and backward pass and so automatic differentiation.\\
We can describe the forward pass of a neural network in two equivalent ways:
\begin{equation}
  \begin{aligned}
	&\text{Right to left:} \qquad &&\text{Left to right:}\\
	&S_0 = X \qquad &&S_0 = x\\
	&S_{2k-1} = W_k S_{2k-2} + b_k \qquad &&S_{2k-1} = S_{2k-2}W_k + b_k\\
	&S_{2k} = \sigma(S_{2k-1}) \qquad &&S_{2k} = \sigma(S_{2k-1})\\
	&S_{2H+1} = W_{k+1}S_{2H} \qquad &&S_{2H+1} = S_{2H}W_{k+1}\\
	&S_{2H+2} = \ell(S_{2H+1}, Y) \qquad &&S_{2H+2} = \ell(S_{2H+1}, Y)\\ 
  \end{aligned}
\end{equation}
The principal differences is that the weights acts on the left or on the right of the data, it can be useful depending whether you represents inputs as rowvectors or columnvectors.\\
It can be represented by this computational graph:\\
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{img/nn_fd.png}
	\caption{Neural network forward pass}
	\label{fig:nn_fd}
\end{figure}
And the backward pass can be represented like this:\\
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{img/nn_bd.png}
	\caption{Neural network backward pass}
	\label{fig:nn_bd}
\end{figure}
\section{Autoregressive models}
An Autoregressive model wants to predict the next values in a sequence based on its previous values. Given a sequence of $n_{cxt}$ (context) past vectors $x_{-1}, x_{-2}, \ldots, x_{-n_{cxt}}$, the model aims to predict the next vector $x_0$ and maybe more $x_1$. Example, we want to predict $x_0$ and $x_1$ based on the past:
\begin{equation}
  \begin{aligned}
	p(x_0, x_1 | x_{-1}, \ldots, x_{-n_{cxt}}) &= p(x_0 | x_{-1}, \ldots, x_{-n_{cxt}}) \cdot p(x_1 | x_0, x_{-1}, \ldots, x_{-n_{cxt} + 1}, x_{-n_{cxt}}) \\
	&\approx p(x_0 | x_{-1}, \ldots, x_{-n_{cxt}}) \cdot p(x_1 | x_0, x_{-1}, \ldots, x_{-n_{cxt} + 1})
  \end{aligned} 
\end{equation}
So the models aims to predict the probability distribution of the next vector $x_0$ with $\hat{p}(x_0 | X)$ with $X$ that concatenates all the context vectors. And then we use the cross-entropy to measure how well the predicted distribution $\hat{p}$ matches the true distribution $p$:
\begin{equation}
  \Lo(X) = - \sum_{x_0} p(x_0 | X) \log(\hat{p}(x_0 | X))
\end{equation}
\section{Tokenization}
How can we use this autoregressive model for LLM (Large Language Models) for example ? Consider that we have $n_{cxt}$ characters of context and we want to predict the next characters, this means that we would have a one-hot encoding in $\R^{26}$, which means $n_{voc} = 26$. This means that $n_{cxt}$ should be a very large number, but this is annoying because transormers have a quadratic complexity in $n_{cxt}$.\\ 
Another idea could be to turn each word into a one-hot encoding, but the vocabulary size is often very large (+200k words), with this we could have a relative low $n_{ctx}$, but $n_{voc}$ would be to big.\\
An intermediate solution is to use byte pair encoding algorithm which greedily merges the most frequent pairs of characters into new tokens.\\ 
Example: consider the word "abracadabra", we see that we have two frequent pairs "ab" and "ra", so we can merge them into new tokens $X$ and $Y$ respectively:
\begin{equation}
  \text{abracadabra} \to \text{XYcadXY}
\end{equation}
then we can repeat the process until we reach the desired vocabulary size.\\ The next iteration could give:
\begin{equation}
  \text{XYcadXY} \to \text{ZcadZ}
\end{equation}
This tokenization method allows us to have good trade-off between the vocabulary size $n_{voc}$ and the context size $n_{cxt}$. But if the model isn't trained enough we could have an issue which is that the model doesn't output fully constructed words, for example it could output half of a word because that what makes more sense even though the word is half complete.\\
\section{Embedding}
Consider a vocabulary of size $n_{voc}$, a bigram model and a network with $d$ layers. The model would be:
\begin{equation}
  \hat{p}(x_0 | x_{-1}) = \text{softmax}(W_d \tanh(\dots \tanh(W_1 x_{-1}) \dots ))
\end{equation}
The matrix $W_1$ has $n_{voc}$ columns and $W_d$ has $n_{voc}$ rows, so when $n_{voc}$ is large, the model becomes very big.\\
So an idea would be to use an encoder and a decoder to reduce the sizes, it is called an embedding size $d_{emb} \ll n_{voc}$. The encoder ($C \in \R^{d_{emb} \times n_{voc}}$) maps the one-hot encoding of size $n_{voc}$ to a dense vector of size $d_{emb}$ and the decoder ($D \in \R^{n_{voc} \times d_{emb}}$) maps back the dense vector to a vector of size $n_{voc}$. So the model becomes:
\begin{equation}
  \hat{p}(x_0 | x_{-1}) = \text{softmax}(D W_d \tanh(\dots \tanh(W_1 C x_{-1})\dots ))
\end{equation}
If we choose wisely $d_{emb}$, which means much smaller than $n_{voc}$, then it is faster to compute $W_1(C x_{-1})$ than in the previous model. Moreover we are forcing $W_1C$ to be low-rank which can help to reduce overfitting but reduce the expressivness (capacity to capture a range of possible relation between input and output) of the model. It is useful to share the embedding between different input and output when $n_{ctx} > 1$, we also found that forcing $D = C^T$ appears to work well in practice.\\
\subsection{Shared embedding}
Let's investigate the case where we have $n_{cxt} > 1$. When $n_{cxt} > 1$, the encoder C is shared by all tokens, so we get this model:
\begin{equation}
  \hat{p}(x_0 | x_{-1}, \ldots, x_{-n_{cxt}}) = \text{softmax}(D W_d \tanh(\dots \tanh(W_1 \begin{bmatrix} C x_{-1} \\ \vdots \\ C x_{-n_{cxt}} \end{bmatrix}) \dots ))
\end{equation}
We can note that now $W_1$ has $n_{cxt}d_{emb}$ columns. Assuming $d_{emb} \ll n_{voc}$ and $n_{cxt} \gg 1$, this is much smaller than the $n_{ctx}n_{voc}$ that we had before the embedding. The number of rows of $W_2$ is not affected by the embedding so it remains $n_{voc}$.\\
We could represent this model like this:\\
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{img/shared_embedding.png}
	\caption{Neural network with shared embedding}
	\label{fig:shared_embedding}
\end{figure}
\section{Recurrent neural networks (RNN)}
A RNN is a type of neural network that is designed to process sequential data by taking advantages of the memory of previous inputs. The idea is to have a hidden state $h_t$ that depends on the current input $x_t$ and the previous hidden state $h_{t-1}$. With this idea, the network reuses the same weights at every time step and thus gives meaning to the sequence. The equations of a simple RNN are:
\begin{equation}
  \begin{cases}
	h_{t+1} = \tanh(W h_{t} + U x_{t-1} + b) \\
	\hat{y}_t = \text{softmax}(V h_t + c)
  \end{cases}
\end{equation}
Where $W, U, V$ are weight matrices and $b, c$ are bias vectors.\\
This model presents some limitations, for example it is difficult to learn long-term dependencies because of the vanishing/exploding gradient problem. The fact that it processes a sequence step by step makes it difficult to parallelize the computations. The fact that it need to store the hidden state at each time step can be also memory-intensive for long sequences.\\
\section{Attention head}
First we need to define a numerical dictionary. Consider keys $k_i \in \R^{d_k}$, values $v_i \in \R^{d_v}$. Given a query $q \in \R^{d_k}$, we want to retrieve the value $v_i$ corresponding to the key $k_i$ that is the most similar to the query $q$. To do this, we can use the dot-product. With that definition, we can define the attention head:
\begin{equation}
  \text{Attention}(Q,K,V) = \sum_{i=1}^{n_{ctx}} \alpha_i v_i
\end{equation}
where $\alpha = \text{softmax}(<q,k_1>, \cdots, <q,k_{n_{ctx}}>)$ is the attention weight for key $k_i$. But in practice, we have vectors of queries, keys and values, each contained in matrices $Q, K, V$. So we can rewrite the attention head as:
\begin{equation}
  \text{Attention}(Q,K,V) = V\text{softmax}\left(\dfrac{K^TQ}{\sqrt{d_k}}\right)
\end{equation}
Where the division by $\sqrt{d_k}$ is used to prevent the dot-product from growing too large.\\
For a transformers, we could explain $Q, K, V$ like this, for each token we have a vector $q, k, v$ that represent:
\begin{itemize}
  \item $q$: What am I looking for?
  \item $k$: What do I contain?
  \item $v$: What information do I pass if Im selected?
\end{itemize}
\subsection{Masked attention}
To ensure that the model only attends to previous tokens and not future tokens, we can use a masked attention mechanism. This is done by applying a mask to the attention weights before the softmax operation. The mask is typically a lower triangular matrix that sets the weights corresponding to future tokens to negative infinity, effectively preventing them from contributing to the attention output.\\
So with masked attention, the equation becomes:
\begin{equation}
  \begin{cases}
    M = \begin{bmatrix} 0 & 0 &  \cdots & 0 \\ -\infty & 0 &  \ddots & 0 \\ \vdots & \ddots & \ddots & \vdots \\ -\infty & -\infty &  \cdots & 0 \\ \end{bmatrix}\\
    \text{Attention}(Q,K,V) = V\text{softmax}\left(M+\dfrac{K^TQ}{\sqrt{d_k}}\right)
  \end{cases}
\end{equation}
\subsection{Multi-head attention}
Instead of having a single attention head, we can have multiple attention heads that can understand different aspects of the input data. Each head has its own set of weights and computes its own attention output, all in parallel. After computing all the heads, their outputs are concatenated and linearly transformed to be in the right dimension with $W^O$, this is thus gathering all of the meanings of the attention different heads to a single representation. So the multi-head attention can be defined as:
\begin{equation}
  \begin{cases}
	\text{head}_j = \text{Attention}(W_j^QQ, W_j^KK, W_j^VV) \\
	\text{MultiHead}(Q,K,V) = W^O \begin{bmatrix} \text{head}_1 \\ \text{head}_2 \\ \vdots \\ \text{head}_h \end{bmatrix}
  \end{cases}
\end{equation}
\section{Decoder-only transformer}
A decoder-only transformer is a type of transformer architecture that is used for autoregressive model. It consists of reading the tokens and predicting the next token in the sequence. In this type of transformer, the matrices $Q, K, V$ are the same, and we define them as the input tokens $CX$, $C$ being the embedding matrix. So the decoder-only transformer can be computed with $\text{MultiHead}(CX,CX,CX)$. Let's seek the different mechanisms used in a decoder-only transformer.\\
\subsection{Positional encoding}
The positional encoding is essential in a transformer model because it provides information about the order of the tokens in the input sequence. Since transformers do not have a built-in notion of sequence order like RNNs, positional encodings are added to the input embeddings to give the model a sense of position.\\
We cannot use one-hot encoding for the position because the dimension would be $n_{ctx}$ and not $d_{emb}$ as expected. The classic approach is to use sines and cosines with an angle that depends on the position of the token and the embedding dimension. The positional encoding for a token at position $pos$ and with the embedding dimension index $i$ and embedding dimension $d_{emb}$ could be defined as:
\begin{equation}
  \begin{cases}
    angle = \dfrac{pos}{10000^{2i/d_{emb}}} \\
    PE[pos, 2i] = \sin\left(angle\right) \\
    PE[pos, 2i+1] = \cos\left(angle\right)
  \end{cases}
\end{equation}
And then we add this positional encoding to the input embeddings before doing the attention mechanism:
\begin{equation}
  \text{MultiHead}(CX + PE, CX + PE, CX + PE)
\end{equation}
\subsection{Residual connection}
The principle of residual connection is to add the input of a layer to its output before applying the layer normalization (next subsection). This helps to mitigate the vanishing gradient problem and help the network to learn because its learning is more controlled, because we remind it of the original input.\\
\subsection{Layer normalization}
The norm of the gradient increases exponentially with the number of layers, so to prevent this we can use layer normalization. This helps stabilize the training process and improve convergence, by rescaling the data before the activation function, which means that we control the mean and the variance of the data. There is two way of doing this normalization, the batch normalization and the layer normalization. The batch normalization is difficult and not very efficient to use in transformers, so we use layer normalization. It is done like this:
\begin{equation}
  \text{LayerNorm}(y_i) = \gamma \dfrac{y_i - \mu_i}{\sigma_i} + \beta 
\end{equation}
With $\gamma$ and $\beta$ two learnable parameters, $\mu_i$ the mean of the elements of $y_i$ and $\sigma_i$ the standard deviation of the elements of $y_i$. Then we can add this in the transformer architecture like this:
\begin{equation}
  \text{LayerNorm}(\text{MultiHead}(CX + PE, CX + PE, CX + PE) + (CX + PE))
\end{equation}
\subsection{Feed-forward network (FFN)}
The feed-forward network consists in doing this operation:
\begin{equation}
  \text{FFN}(x) = W_2 \text{ReLU}(W_1 x + b_1) + b_2
\end{equation}
With $W_1 \in \R^{d_{ff} \times d_{emb}}$ and $W_2 \in \R^{d_{emb} \times d_{ff}}$, $d_{ff}$ being the dimension of the feed-forward network, generally $d_{ff} = 4d_{emb}$. This feed-forward network is applied to each token independently and identically. The feed-forward network is used to introduce non-linearity and increase the model's capacity to learn complex patterns in the data. It is used the memorize more complex information in the weights, which helps the model to better understand the complex relationships between tokens in the sequence.
\subsection{Transformers variation}
\textcolor{red}{To investigate ?}
\section{Performances of transformers}
Before analysing the complexity of a transformer, let us reminds the main dimension of the variables:
\begin{itemize}
  \item $n_{ctx}$: context size (number of tokens in the input sequence)
  \item $n_{voc}$: vocabulary size (number of unique tokens)
  \item $d_{emb}$: embedding size (dimension of embeddings)
  \item $d_k, d_v$: key and value dimension (per head)
  \item $d_{ff}$: feed-forward network dimension
  \item $N$: number of layers
\end{itemize}
We can thus analyse the time complexity of a transformer step by step (ignoring the mebeding step):
\begin{enumerate}
  \item Linear projections for $Q, K, V$ (with $W_j^Q, W_j^K, W_j^V$): $\Oo\left(d_k d_{emb} n_{ctx}\right)$
  \item Attention score computation ($K^TQ$): $\Oo\left((d_k+d_v)n_{ctx}^2\right)$
  \item Output projection ($W^O$): $\Oo\left(d_v d_{emb} n_{ctx}\right)$
  \item FFN: $\Oo\left(d_{emb} d_{ff} n_{ctx}\right)$
\end{enumerate}
This brings the total cost for a single layer to:
\begin{equation}
  \Oo\left( (d_k + d_v) n_{ctx}^2 + (d_k + d_v + d_{ff}) d_{emb} n_{ctx} \right)
\end{equation}
To get the total cost for $N$ layers, we just need to multiply by $N$.\\
\begin{equation}
  \Oo\left( N (d_k + d_v) n_{ctx}^2 + N (d_k + d_v + d_{ff}) d_{emb} n_{ctx} \right)
\end{equation}
Knowing that generally $d_k \approx d_v \approx d_{emb} \approx d_{ff}$, we can simplify the complexity to:
\begin{equation}
  \Oo\left( N d_{emb} n_{ctx}^2 + N d_{emb}^2 n_{ctx} \right) = \Oo\left( N d_{emb} n_{ctx} (n_{ctx} + d_{emb}) \right)
\end{equation}
Here we can see the two dominant terms of the complexity:
\begin{itemize}
  \item $N d_{emb} n_{ctx}^2$: comes from the attention score computation, it is the most critical term when $n_{ctx}$ is large.
  \item $N d_{emb}^2 n_{ctx}$: comes from the FFN and the projection, it is the most critical term when $d_{emb}$ and thus for wide models.
\end{itemize}
We can also observe that the dimension of the vocabulary $n_{voc}$ does not appear in the complexity. It is because it is only involved in the embedding complexity.
% \section{Encoder-decoder transformers}
% \textcolor{red}{Idk what to say}
\chapter{Diffusion Models}
The objective of diffusion models is to generate data by learning how to reverse a gradual noising process, turning random noise into structured samples through iterative denoising. 
\section{Tweedie's formula}
First, we need to understand why diffusion models can be trained by denoising and why predicting the noise is equivalent to learning a score. For that, we need to introduce Tweedie's formula. Consider that we observe a noisy version of a random variable $X$. The noisy observation is given by $Y = X + \sigma \varepsilon$, where $\varepsilon \sim \mathcal{N}(0, 1)$. We want, given this noisy observation $Y=y$, to estimate the original variable $X$. The MMSE estimator is given by $\mathbb{E}[X | Y=y]$. Tweedie's formula states that:
\begin{equation}
  \mathbb{E}[X | Y=y] = y + \sigma^2 \nabla_y \log f_Y(y)
\end{equation}
where $f_Y(y)$ is the probability density function of the noisy observation $Y$. And thus $\nabla_y \log f_Y(y)$ is a score function that tells us in which direction the probability mass increases and thus where cleaner data is. We can also estimate the noise $\varepsilon$:
\begin{equation}
  \mathbb{E}[\varepsilon | Y=y] = - \sigma \nabla_y \log f_Y(y)
\end{equation}
And this means that if we can predict the noise, we can also estimate the score function and thus the denoised data. This is the principle used in diffusion models.
\section{Langevin dynamics (sampling)}
Langevin dynamics is a way to sample from a probability distribution ($p(y)$) when you only know its log-density gradient (the score), not the density itself. The idea is to iteratively update a sample by taking small steps in the direction of the score, while also adding some random noise to ensure exploration of the space. The update rule for Langevin dynamics is given by:
\begin{equation}
  y_{k+1} = y_k + \delta_k \nabla_y \log p(y_k) + \sqrt{2 \delta_k} w_k	
\end{equation}
with $w_k \sim \mathcal{N}(0, 1)$ and $\delta_k$ is a small step size. By repeating this process many times, the samples $y_k$ will converge to the target distribution $p(y)$. Adding this noise is important because it helps the samples to explore the space more effectively and avoid getting stuck in local modes of the distribution. The Langevin dynamics is seen in diffusion models at each denoising step.
\section{Score matching}
This is the part "learning the score" of diffusion models. Here the idea is to fix a noise level $\sigma$ and to corrupt the data $X$ with Gaussian noise to get $Y = X + \sigma \varepsilon$, with $\varepsilon \sim \mathcal{N}(0, 1)$. Then we want to minimize the error to learn the noisy data distribution ($p_{X+\sigma \varepsilon}$):
\begin{equation}
	  \mathbb{E}\left[ \| \varepsilon_\theta (X+ \sigma \varepsilon) - \varepsilon \|^2 \right]
\end{equation}
But with this, $\sigma$ needs to be scaled properly, otherwise the model will not learn well. If $\sigma$ is too small, then the support of $X+ \sigma \varepsilon$ may not cover the whole space, and thus $\varepsilon_\theta(y)$ may be inaccurate. If $\sigma$ is too large, then the noise dominates the signal, and the model may struggle to learn meaningful patterns.
\subsection{Variance dependent score}
To address the issue of choosing the right noise level $\sigma$, we can introduce a variance-dependent score function. Instead of using a fixed $\sigma$, we can define a score function that depends on the noise level. The idea is to train a model $\varepsilon_\theta(y, \sigma)$ that takes both the noisy observation $y$ and the noise level $\sigma$ as inputs. The training objective becomes:
\begin{equation}
	\mathbb{E}\left[ \| \varepsilon_\theta (X+ \sigma \varepsilon, \sigma) - \varepsilon \|^2 \right]
\end{equation}
\section{Deterministic Sampling}
On the picture below, we can see how does denoising looks like with DDIM (Denoising Diffusion Implicit Models). The idea is to use a deterministic process to reverse the diffusion process. If we denoise in one step, for low noise levels, it works quite well. For large noise levels, the score provides only a global direction, so multiple denoising steps are required to gradually approach the data manifold.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.9\textwidth]{img/DDIM.png}
	\caption{DDIM sampling example}
\end{figure}
DDIM move from a variance $\sigma_t$ to a variance $\sigma_{t-1}$, like in the figure above (c):
\begin{equation}
	\begin{aligned}
		X_t &= X_0 + \sigma_t \varepsilon\\
		\E[X_{t-1} | X_t = x_t] &= \E[X_0 | X_t = x_t] + \sigma_{t-1} \E[\varepsilon | X_t = x_t] \\
		&= \E[X_t | X_t = x_t] - \sigma_t \E[\varepsilon | X_t = x_t] + \sigma_{t-1} \E[\varepsilon | X_t = x_t] \\
		&= x_t - (\sigma_t - \sigma_{t-1}) \E[\varepsilon | X_t = x_t] \\
		&= x_t - (\sigma_t - \sigma_{t-1}) \epsilon_\theta(x_t)
	\end{aligned}
\end{equation}
\section{Denoising with randomness}
Now we want to add some randomness to the denoising process, so we introduce DDPM (Denoising Diffusion Probabilistic Models). The idea is to add some noise during the denoising process to ensure diversity in the generated samples. Give $0 \leq \mu < 1$, pick $\sigma_{t'}$ such that $\sigma_{t-1} = \sigma_t^\mu \sigma_{t'}^{1-\mu}$ and we have the following sampler:
\begin{equation}
	x_{t-1} = x_t + (\sigma_{t'} - \sigma_t) \epsilon_\theta(x_t, \sigma_t) + \eta w_t 
\end{equation}
with $w_t \sim \mathcal{N}(0, 1)$. If we choose $\mu = 0$ and $\sigma_{t'} = \sigma_{t-1}$, we recover the DDIM sampler. And if we choose $\mu = 1/2$, we have the DDPM sampler. We have by assumption $\sigma_{t-1} < \sigma_t$. Then, because $\sigma_{t-1}$ is the geometric mean of $\sigma_t$ and $\sigma_{t'}$, it must be between so $\sigma_{t'} < \sigma_{t-1} < \sigma_t$. To choose $\eta$, we know we want $\sigma^2_{t-1} = \mathbb{V}(\sigma_{t'}\varepsilon_\theta(x_t,\sigma_t))+\eta W_t$. So we can choose $\eta = \sqrt{\sigma_{t-1}^2 - \sigma_{t'}^2}$.\\
\subsection{Acceleration}
We can accelerate the convergence of the previous sampler using the observation that "The noise prediction at a lower noise level is more accurate". We can thus define:
\begin{equation}
	\bar{\epsilon}_t = \gamma \epsilon_\theta(x_t, \sigma_t) + (1-\gamma) \epsilon_\theta(x_{t+1}, \sigma_{t+1}) \qquad \gamma > 1
\end{equation}
\section{Auto-Encoder}
We want to find an encoder $E$ and a decoder $D$ that minimizes the loss:
\begin{equation}
	\E[\| X - D(E(X)) \|^2_2]
\end{equation}
$E(X)$ typically reduces the dimension of $X$ to force the model to keep only essential features. If $E$ and $D$ are linear, then the auto-encoder is $x \to DEx$ and so only the product $DE$ matters and the problem becomes:
\begin{equation}
	\min_{D,E} \E[\| X - DEX \|^2_F]
\end{equation}
Because we want to compress the data, we have the constraint $\text{rank}(DE) \leq r$. So the auto-encoder is searching for the best rank-$r$ approximation of X. We can use the SVD of $X = U \Sigma V^T$ to find the best rank-$r$ approximation, and in our case it is working the same way as the PCA. And thus to approximate $X$ we can choose $DE = U_r \Sigma_r U_r^T$. With all this, an Auto-Encoder can be thought of as a nonlinear generalization of PCA.
\subsection{Variationnal Auto-Encoder}
We want to learn the distribution of our data represented by the random variable $X$. To improve learning, we can add artificial noise to learn a more interesting distribution.
\begin{equation}
	\E [\| X - D(E_\mu(X) + \varepsilon \odot E_\sigma(X)) \|^2_2]
\end{equation}
\subsection{Evidence Lower Bound (ELBO)}
The evidence lower bound is a useful lower bound on the log-likelihood of some observed data. It is useful because it provides a guarantee on the worst-case for the log-likelihood of some distribution (e.g. $p(X)$) which models a set of data. But first we need to introduce the Kullback-Leibler (KL) divergence. The KL divergence is a measure of how one probability distribution diverges from a second, expected probability distribution. For two distributions $P$ and $Q$, the KL divergence from $Q$ to $P$ is defined as:
\begin{equation}
	D_{KL}(P \| Q) = \sum_{x} P(x) \log\left(\dfrac{P(x)}{Q(x)}\right)
\end{equation}
And with that we can define the ELBO. 
\begin{equation}
	\mathcal{L}(x) = \E[\log(f_{X|Z}(x|Y))] - D_{KL}((Y|X=x) \| Z)
\end{equation}
The KL divergence is always non-negative, and measure the error made by our estimator.
\subsection{Gaussian ELBO}
We can compute the ELBO in the case of Gaussian distributions, like with the variationnal Auto-Encoder. Suppose $X = D_\mu(Z) + \varepsilon_1 \odot D_\sigma(Z)$ and $Y = E_\mu(X) + \varepsilon_2 \odot E_\sigma(X)$, with $\varepsilon_1, \varepsilon_2 \sim \mathcal{N}(0, 1)$. Then we have:
\begin{equation}
	2 D_{KL}((Y|X=x) \| Z) = \| E_\mu(X)\|^2_2 + \| E_\sigma(X) \|^2_2 - \left(\sum_{i=1}^{r} \log((E_\sigma(X))_i^2) +r\right)
\end{equation}
And for the first term of the ELBO:
\begin{equation}
	\begin{aligned}
		&\E\left[\log(f_{X|Z}(x|Y))\right] \\
		&= \E\left[\log(f_{X|Z}(x|E_\mu(X) + \varepsilon_2 \odot E_\sigma(X)))\right] \\
		&= - \dfrac{\log(2\pi)}{2} + \E\left[\| \text{Diag}(D_\sigma(E_\mu(X) + \varepsilon_2 \odot E_\sigma(X)))^{-1} \left(x - D_\mu(D_\sigma(E_\mu(X) + \varepsilon_2 \odot E_\sigma(X)))\right) \|^2_2\right] 
	\end{aligned}	
\end{equation}
\subsection{Monte-Carlo sampling}
The gaussian ELBO can be estimated using Monte-Carlo sampling. Given $L$ samples $(\epsilon_1, \dots, \epsilon_L)$ from $\mathcal{N}(0, 1)$, it gives:
\begin{equation}
	\E \left[ \log(f_{X|Z}(x|Y))\right] \approx \dfrac{1}{L} \sum_{i=1}^{L} \log(f_{X|Z}(x|E_\mu(X) + \epsilon_i \odot E_\sigma(X)))
\end{equation}
With the simple case where $D_\sigma(z) = 1$, we get this approximation ($L_2$ form):
\begin{equation}
	\E \left[ \log(f_{X|Z}(x|Y))\right] \approx - \dfrac{\log(2\pi)}{2} + \dfrac{1}{L} \sum_{i=1}^{L} \| x - D_\mu(E_\mu(x) + \epsilon_i ) \|^2_2
\end{equation}
\subsection{Maximum Likelihood Estimator with variationnal Auto-Encoder}
Reminder, the encoder $E$ maps a data point $x$ to a Gaussian distribution $Y \sim \mathcal{N}(E_\mu(x), E_\sigma(x))$. The decoder $D$ maps a latent variable $z$ to the Gaussian distribution $\mathcal{N}(D_\mu)$. The maximum likelihood estimator, maximizes the following sum over our datapoints $x$ with its ELBO:
\begin{equation}
	\sum_{x} \log(f_X(x)) \geq \sum_{x} -D_{KL}((Y|X=x) \| Z) + \E[\log(f_{X|Z}(x|Y))]
\end{equation}
So it minimizes the loss i.e. the second term. And the KL divergence is acting as a regularizer to prevent overfitting.
\subsection{Denoising Auto-Encoder}
All of the previous subsections has been explained to get to this point: Denoising Auto-Encoder. Reminder the goal of the diffusion model is to denoise data ($Y = X + \sigma \varepsilon$) and find the noise $\varepsilon$, the denoising auto-encoder tries instead to find the original data $X$. We have those following structures:
\begin{itemize}
	\item Auto-Encoder $D(E(X))$
	\item Variationnal Auto-Encoder $D(E(X) + \varepsilon)$
	\item Denoising Auto-Encoder $D(E(X + \sigma \varepsilon))$
\end{itemize}
To train the denoising auto-encoder, we can use the Evidence Lower-Bound with $Y = X + \sigma \varepsilon$ and $Z$ such that $X = D(E(Z))$:
\begin{equation}
	-\log(f_X(x)) \leq D_{KL}((Y|X=x) \| Z) - \E[\log(f_{X|Z}(x|Y))]
\end{equation}
\section{Conditionned Diffusion Models}
Conditioned diffusion generates data by denoising noise, but the denoising is guided by a conditioning signal (text, image, layout, etc.). Where unconditionned diffusion learn $p(x)$, conditionned diffusion learn $p(x|c)$, with $c$ the conditionning signal. It is for example used to train text-to-image generation, where the model generates an image based on a text description. This conditionned diffusion will need to learn the relationship between the text and the image so it will need cross-attention like in a transformers.
\section{Classifier-Free Guidance}
To improve the conditioned diffusion for multi-modal distributions, we can use a classifier but it need also training. So to overcome this, we can use classifier-free guidance. The idea is to train the model to handle both conditioned and unconditioned data. During training, we train both conditionned (with condition $\tau$) and unconditionned diffusion model and combine them with:
\begin{equation}
	\bar{\epsilon}_t = \lambda \epsilon_\theta(x_t, \sigma_t, \tau) + (1-\lambda) \epsilon_\theta(x_t, \sigma_t) \qquad \lambda > 1
\end{equation}
\section{Optical illusions}
To generate optical illusions, we do not need to create a specialized model, we can use a pre-trained diffusion model. We can use this model and at each step, we denoise the image under $N$ transformations ($v_i$), bring all denoising directions back to the original space, average them, and use this average as the update. The optimization problem can be formulated as:
\begin{equation}
	\bar{\epsilon}_t = \dfrac{1}{N} \sum_{i=1}^{N} v_i^{-1} (\epsilon_\theta(v_i(x_t), \sigma_t, \tau))
\end{equation}
The fact that we try to retrieve the same image under different transformations creates conflicting objectives that lead to the generation of optical illusions.



















\chapter{Kernels}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{img/kernels.png}
	\caption{Illustration of kernels}
\end{figure}
A kernel is a function that transforms a dataset into another, typically of higher dimension. This helps separate the nonlinear feature, to use the usual linear tools. For example, the canonical kernel is 
\begin{equation}
  r(x,y) = (x^Ty)^2 = \phi(x)^T \phi(y)
\end{equation} 
where the kernel function then is 
\begin{equation}\label{eq:example}
  \phi(x) = \begin{pmatrix}
	x_1^2 \\ x_2^2 \\ \sqrt{2}x_1x_2
  \end{pmatrix}
\end{equation}
\section{Reminders on scalar product}
\begin{itemize}
  \item [$\to$] Reminder: a Euclidean space is a finite-dimensional vector space endowed with a scalar product. 
\end{itemize}
A scalar product $\langle \cdot, \cdot \rangle_V$ verifies 
\begin{itemize}
  \item Symmetry: $\forall x,y\in V, \: \langle x,y\rangle_V = \langle y,x \rangle_V$;
  \item Definite positive: if $x\in V$ and $x\neq 0$, then $\langle x,x\rangle >0$, and if $x=0$, then $\langle x,x\rangle=0$;
  \item Bilinearity: $\forall x,y,z\in V, \: \forall \alpha,\beta \in \mathbb{F}, \langle(\alpha x+\beta y),z\rangle_V = \alpha \langle x,z\rangle_V + \beta \langle y,z\rangle_V$.
\end{itemize}
\subsection{Equivalence}
Consider a positive definite symmetric matrix $M\succ 0\in \R^{n\times n}$ with the scalar product $\langle x,y\rangle_M = x^TMy$. Any such matrix can be factored as $M=L^TL$, where $L$ is invertible. We can do a change of coordinates to re-express under the cartesian scalar product:
\begin{equation}
  \begin{cases}
	w = Lx \\
	z = Ly
  \end{cases} \Longrightarrow \langle x,y\rangle_M = \langle w,z\rangle_{\R^n}
\end{equation}
Then, all $n$-dimensional Euclidean spaces are equivalent up to a change of coordinates. 
\subsection{Hilbert space}
For functions integrable on an interval $[a,b]$, we define the scalar product as 
\begin{equation}
  \langle f,g\rangle=\int_a^b f(t)g(t)dt 
\end{equation}
\section{Kernel methods for finite sets}
\subsection{Example -- word embedding}
Text embedding has as objective to represent a text with a vector. The general idea is one-hot encoding, i.e. for a data set $X$ of $N$ words, we create one-hot vectors of dimension $N$, using the feature map (or embedding) $\phi:X\to H$. A sentence is simply the sum of those vectors, weighted by the number of occurences. From this, we can create the kernel matrix $K$, where each element $K_{ij}$ is the comparison between the basis vectors $e_i, e_j$. We suppose that the $\phi$ function has a unitary norm.
\begin{equation}
  K_{ij} = \langle \phi(x_i),\phi(x_j)\rangle_K \in [-1,1]
\end{equation}
A value of $1$ means that the words $x_i$ and $x_j$ are identical, but if the value is $-1$, their meaning are opposite. In the case of a $0$, there is no connection between the words. Now, we can calculate the Cholesky decomposition $K=L^TL$ for a new basis, and use the usual scalar product. 
\begin{itemize}
  \item [$\to$] Note: if the matrix $K$ is not invertible, we just hit a degenerate case where the $\phi(x_i)$ are not linearly independent. This is not a problem. 
\end{itemize}
\subsection{Kernel trick}
The kernel trick is to never explicitly define the function $\phi$: we can work with the matrix $K$ only, as any vector can be expressed as a linear combination of $\phi(x_i)$ and the main ingredient of linear methods is generally the scalar product, here defined only using $K$. 
\begin{equation}
  \langle \phi(y),\phi(z)\rangle = \langle \sum_{i=1}^N \alpha_i \phi(x_i), \sum_{i=1}^N \beta_i \phi(x_i)\rangle_K = \sum_{i=1}^N \sum_{j=1}^N \alpha_i \beta_j \langle \phi(x_i),\phi(x_j)\rangle_K = \alpha^T K \beta 
\end{equation}
\subsection{Reconstruction}
Given the matrix $K$, we can reconstruction the embedding ($\phi$ and $\langle \cdot,\cdot\rangle_K)$ up to a rotation. For $X$ the set of words and $N$ the dimension of the space, we have $\phi:X\to \R^N$ and we define the scalar product:
\begin{equation}
  \langle x,y\rangle_M = \langle x,y\rangle_{K^{-1}}
\end{equation}
Then, remembering $\phi(x_i) = K e_i$,
\begin{equation}
  \langle \phi(x_i), \phi(x_j)\rangle_M = (Ke_i)^T K^{-1}(Ke_j) = e_i^T K^TK^{-1}Ke_j = e_i^T K e_j
\end{equation}
Let us prove the "up to a rotation" part. Define 
\begin{equation}
  \begin{cases}
	\phi':X\to \R^N \\ \phi'(x) = Q\phi(x) \\ \langle x,y\rangle_{M'} = \langle x,y\rangle_{QK^{-1}Q^T}
  \end{cases}
\end{equation}
where $Q$ is a rotation matrix, i.e. $Q^TQ = I$. Then,
\begin{equation}
  \begin{aligned}
	\langle \phi'(x),\phi'(y)\rangle_{M'}&= (Q\phi(x))^T Q K^{-1} Q^T (Q\phi(y))\\
	&= \phi(x)^T K^{-1}\phi(y) = \langle \phi(x),\phi(y)\rangle_{K^{-1}}
  \end{aligned}
\end{equation}
The result is true for any rotation matrix $Q$. 
\begin{itemize}
  \item [$\to$] Note: the complexity of computing $K$ is $\mathcal{O}(N^2)$, while the Cholesky decomposition to find the new basis has a complexity of $\mathcal{O}(N^3)$. This decomposition is to be avoided if not really necessary. 
\end{itemize}
\subsection{Advantages}
Let us consider proteins. The alphabet is of size $20$ (number of existing amino-acids), and a protein is made of 4 of these. This means that our space is initially $H = \R^{20^4} = \R^{160.000}$. Consider $N=100$ proteins to classify. \\
The embedding consists in storing $N$ vectors of size $h=160.000$, meaning 1.6M scalars. However, we only need the $K$ matrix of size $N^2=10.000$, and the entries are easy to compute. This means that, using the kernel trick, i.e. working with $K$ directly, we are much more efficient when $N<<h$. 
\section{Kernels methods for continuous sets}
\subsection{Reproducibility Kernel Hilbert Space}
Consider richer sets than finite $X$, e.g. infinite or uncountable sets, with distances defined but not scalar product. Then, let $H$ be a vector space of conitnuous functions $X\to \R$. $H$ is a RKHS if it is a Hilbert space and 
\begin{equation}
  \forall x\in X, \: \exists v\in H \text{ such that } \forall f\in H\:: \: \langle v,f\rangle_H = f(x)
\end{equation}
Mathematically, the kernel function is the equivalent of the kernel matrix, but for infinite spaces:
\begin{equation}
  \phi:X\to \R^N \text{ such that } k(x,y) = \langle \phi(x),\phi(y)\rangle_K = \alpha_x^T K \alpha_y
\end{equation}
For a RKHS $H$, we verify 
\begin{equation}
  \begin{cases}
	\forall x\in X\: : \: k(x,\cdot)\in H\\
	\forall x\in X\: ,\forall f\in H, \: f(x) = \langle f,k(x,\cdot)\rangle_K
  \end{cases}
\end{equation}
The reproducing kernel property is 
\begin{equation}
  k(x,y) = \langle k(x, \cdot), k(y,\cdot)\rangle_H
\end{equation}
\subsection{Properties}
\begin{thm}
  A continuous map $k:X\times X \to \R$ is the kernel of some RKHS $H\subset \mathcal{C}(X,\R)$
  \begin{itemize}
	\item iff $k(x,y) = \langle \phi(x),\phi(y)\rangle_H$ for some feature map $\phi:X\to H$;
	\item iff, for all finite subsets $X_0\subset X$, the kernel matrix is symmetric positive definite.
  \end{itemize}
\end{thm}
\section{Polynomial kernels}
Polynomial kernels are a particular type of continuous kernels, where the feature map is a function 
\begin{equation}
  \phi:R^n \to \R^\ell 
\end{equation}
Let us take our initial example \ref{eq:example}, and generalize it to 
\begin{equation}
  k(x,y) = (x^Ty)^d  \qquad X = \R^n 
\end{equation}
Then, the dimension of the out space $H = \R^\ell$ is $\ell = \begin{pmatrix}
  n+d-1 \\d
\end{pmatrix}$. 
\end{document}