\documentclass[12pt, openany]{report}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath,amsfonts,amssymb}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage[a4paper,left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage[english]{babel}
\usepackage{libertine}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{float}
\usepackage{enumitem}
\usepackage{pythonhighlight}
\usepackage[]{titletoc}
\usepackage{empheq}
\usepackage{titlesec}
\usepackage{mathpazo}
\usepackage{xfrac}
\usepackage{textcomp}
\usepackage{mathtools}
\usepackage{caption}
\usepackage{tabularray}
\usepackage{subcaption}
\usepackage[bottom]{footmisc}
\usepackage{pdfpages}
\usepackage{tabularx}
\usepackage{enumitem}           % for adjusting space between bullet points
\usepackage {tikz}
\usetikzlibrary{positioning}
\usepackage[strict]{changepage} % for adjustwidth environment
\usepackage{framed}             % for formal definitions
\usepackage{fourier}            % for the warning symbol
\usepackage{amsthm}
\usepackage[skins]{tcolorbox}
\titleformat{\chapter}[display]
  {\normalfont\bfseries}{}{0pt}{\Huge}
\usepackage{hyperref}
\newcommand{\hsp}{\hspace{20pt}}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Oo}{\mathcal{O}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\J}{\mathbf{J}}
\newcommand{\He}{\mathbf{H}}
\newcommand{\Lo}{\mathcal{L}}
\newcommand{\softmax}{\text{softmax}}
\theoremstyle{definition}
\newtheorem{exmp}{Example}[chapter]
\newtheorem{thm}{Theorem}[chapter]
\newtheorem{definition}[thm]{Definition}
\newtheorem{lem}[thm]{Lemma}

% environment derived from framed.sty: see leftbar environment definition
\definecolor{formalshade}{rgb}{0.95,0.95,1}
\definecolor{darkblue}{rgb}{0.0, 0.0, 0.55}

\newenvironment{formal}{
  \def\FrameCommand{
    \hspace{1pt}
    {\color{darkblue}\vrule width 2pt}
    {\color{formalshade}\vrule width 4pt}
    \colorbox{formalshade}
  }
  \MakeFramed{\advance\hsize-\width\FrameRestore}
  \noindent\hspace{-4.55pt}% disable indenting first paragraph
  \begin{adjustwidth}{}{7pt}
  \vspace{2pt}\vspace{2pt}
}
{
  \vspace{2pt}\end{adjustwidth}\endMakeFramed
}

\hbadness=100000
\begin{document}
\begin{titlepage}
	\begin{sffamily}
	\begin{center}
		\includegraphics[scale=0.3]{img/page_de_garde.jpg} \\[1cm]
		\HRule \\[0.4cm]
		{ \huge \bfseries LINMA2472 - Algorithm in data science \\[0.4cm] }
	
		\HRule \\[1.5cm]
		\textsc{\LARGE Alexandre Or\'ekhoff \\ Issambre L'Hermite Dumont \\ Simon Desmidt}\\[3cm]
		\vfill
		\vspace{2cm}
		{\large Academic year 2025-2026 - Q1}
		\vspace{0.4cm}
		 
		\includegraphics[width=0.15\textwidth]{img/epl.png}
		
		UCLouvain\\
	
	\end{center}
	\end{sffamily}
\end{titlepage}

\setcounter{tocdepth}{1}
\tableofcontents
\chapter{Automatic Differentiation}
Automatic Differentiation (AD) is an algorithmic technique to compute automatically the derivative (gradient) of a function defined in a computer program. 
Unlike symbolic differentiation (\emph{done by hand}) and numerical  differentiation (\emph{finite difference approximation}), 
automatic differentiation exploits the fact that every function can be decomposed into a sequence of elementary operations (addition, multiplication, sine, exponential, etc.) 
By mean of the chain rule, applied to obtain the all of a function's derivatives, its gradient can be exactly and efficiently computed.

Automatic differentiation is widely used in machine learning because neural networks requires the computation of the loss function's gradient with respect 
to the model's parameters (weights and biases). This is necessary to update them during the training process (e.g. with a gradient descent) 
and it would be a pain in the ass to compute this manually for each node.\\
\section{Chain rule}
Assume $f$ is the composition of $m$ functions. The chain rule gives
\begin{equation}
  f'(x) = f_m'(f_{m-1}(f_{m-2}(\dots f_1(x)\dots)))\cdot f'_{m-1}(f_{m-2}(\dots f_1(x)\dots)) \cdot \dots \cdot f_2'(f_1(x)) \cdot f_1'(x)
\end{equation}
Defining
\begin{equation}
  \begin{cases}
	s_0 &= x \\
	s_k &= f_k(s_{k-1})
  \end{cases}
\end{equation}
gives
\begin{equation}
  f'(x) = f'_m(s_{m-1})\cdot f'_{m-1}(s_{m-2}) \cdot \dots
  \cdot f'_2(s_1) \cdot f'_1(s_0)
\end{equation}
Based on this expression, 2 ways of applying the chain rule can be defined: forward and backward differentiation.
\section{Forward differentiation}
Also called forward mode, this algorithm consists in propagating forward the derivative and the values at the same time. 
Propagating the values forward is called a \textbf{forward pass}. 
This process can be represented by the graph on figure \ref{fig:forward_diff}. The blue part represents the values and the green part shows the derivatives.
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{img/forward_diff.png}
	\caption{Forward differentiation}
	\label{fig:forward_diff}
\end{figure}
The algorithm follows:
\begin{equation}
  \begin{cases}
	t_0 &= 1 \\
	t_k &= f'_k(s_{k-1}) \cdot t_{k-1}\\
  \end{cases}
\end{equation}
This process is repeated for every input variables. Consequently, it is \emph{very efficient} for functions with a \emph{small number of input} variables 
and bad for functions with a large number of inputs. 

The forward differentiation algorithm is the easiest to implement.
\section{Backward differentiation}
Also called backward mode, the algorithm consists in propagating the values forward and the derivatives backward \emph{in one pass}. 
Propagating the derivatives backward is called a \textbf{backward pass}. 
This process can be represented by the graph on figure \ref{fig:backward_diff}. The blue part represents the values and the orange part the derivatives.
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{img/backward_diff.png}
	\caption{Backward differentiation}
	\label{fig:backward_diff}
\end{figure}
The idea is to compute all the intermediate values $s_k$ in a forward pass and then compute the derivatives $r_k$ based on the output in a backward pass. 
The algorithm follows the recurrence relation:
\begin{equation}
  \begin{cases}
	r_m &= 1 \\
	r_k &= r_{k+1} \cdot f'_{k+1}(s_{k})\\
  \end{cases}
\end{equation}
This method is heavier to implement but it is very efficient for functions with a large number of input variables and a small number of output variables. 
Typically, the backward mode is preferred for neural networks, where there is typically \emph{only one} output variable: the loss.

\section{Computational graph and multivariate differentiation}
\subsection{Computational graph}  
To represent the computation of a function, a computational graph can be used. It is a directed acyclic graph where the nodes represent the operations 
and the edges represent the variables. For instance, consider the function with $f_1(x)=x=s_1$ and $f_2(x)=x^2=s_2$:
\begin{equation}
  f_3(s_1,s_2) = s_1 + s_2 = x + x^2
\end{equation}
The computational graph is:\\
\begin{figure}[H]
  \centering
  \begin{tikzpicture}[
	roundnode/.style={circle, draw=green!60, fill=green!5, very thick, minimum size=7mm},]
	%Nodes
	\node[roundnode] (x1) {$s_1=x$};
	\node[roundnode] (square) [right=of x1] {$f_2=x^2$};
	\node[roundnode] (sum) [right=of square] {$f_3=f_2+s_1$};
	
	% %Lines
	\draw[->] (x1.north) .. controls +(up:7mm) and +(up:7mm).. (sum.north);
	\draw[->] (x1.east) -- (square.west);
	\draw[->] (square.east) -- (sum.west);
  \end{tikzpicture}
  \caption{Directed Acyclic Graph (DAG) of $f_3$}
  \label{tikz:dag}
\end{figure}
\subsection{Multivariate differentiation}
Let's consider the function displayed on figure \ref{tikz:dag}.
\begin{equation}
  f_3(f_1(x),f_2(x)) = s_3 = f_1(x) + f_2(x) = s_1 + s_2 = x + x^2 
\end{equation}
The chain rule gives
\begin{equation}
  f'_3(x) = \dfrac{\partial f_3}{\partial s_1} \dfrac{\partial s_1}{\partial x} + \dfrac{\partial f_3}{\partial s_2} \dfrac{\partial s_2}{\partial x}
\end{equation}
In forward mode, the values and the derivatives are propagated forward together. When a node has multiple inputs, it is necessary to use the chain rule. 

Evaluating $f_3$ at $x=3$ leads to
\begin{equation}
	\begin{cases}
		t_0 &= 1 \\
		t_1 &= f'_1(x) \vert_{x=3} \cdot t_0 = 1\\
		t_2 &= f'_2(x) \vert_{x=3} \cdot t_0 = 6\\
		t_3 &= \dfrac{\partial f_3}{\partial s_1} \vert_{x=3} \cdot t_1 + \dfrac{\partial f_3}{\partial s_2} \vert_{x=3} \cdot t_2 = 7\\
	\end{cases}
\end{equation}
In backward mode, the gradient accumulators needs to be initialized to 0.
\begin{equation}
	\dfrac{\partial s_3}{\partial s_1} = \dfrac{\partial s_3}{\partial s_2} = \dfrac{\partial s_3}{\partial x} = 0
\end{equation}
Next, the intermediate values are computed in one forward pass
\begin{equation}
	\begin{aligned}
		\dfrac{\partial s_3}{\partial s_1} &+= 1 \Rightarrow \dfrac{\partial s_3}{\partial x} += 1 \cdot 1 \vert_{x=3}\\		
		\dfrac{\partial s_3}{\partial s_2} &+= 1 \Rightarrow \dfrac{\partial s_3}{\partial x} += 1 \cdot 2x \vert_{x=3}\\
	\end{aligned}
\end{equation}
Eventually giving
\begin{equation}
	\dfrac{\partial s_3}{\partial x} = 7
\end{equation}
\section{Jacobian computation}
Forward and backward mode allows the indirect computation of the function's Jacobian matrix. Reminding that, for a function $f:\R^n\to\R^m$,
the jacobian matrix will have dimensions $m\times n$, the following intepretations are possible.

\subsection{Forward mode}
Forward mode computes the function's Jacobian matrix by successive Jacobian-vector products, one for each variable. 
\begin{equation}
  J_f(x) \cdot v \qquad \qquad \text{(JVP)}
\end{equation}
For each input variable, the Jacobian-vector product, representing the directional derivative, is computed with a one-encoding vector $v$ of the said variable
\footnote{The one-hot encoding of a variable is a vector of length $n$ filled with zeros, with $1$ set at the encoded variable's position.}. 
$v$ represents the direction of the input variable.
The directional derivative may be seen as answering to
\begin{formal}
  How does a perturbation to the input propagates to the outputs?
\end{formal}
The full jacobian matrix is obtained as the horizontal concatenation of all Jacobian-vector products computed for each input variable.

\subsection{Backward mode}
Backward mode computes the vector-Jacobian products.
\begin{equation}
  v^T J_f(x) \qquad \qquad \text{(VJP)}
\end{equation}
Here, $v^T$ is the output covector of size $m$. It is a one-hot encoding of one of the outputs. The \emph{VJP} answers to 
\begin{formal}
  Which input perturbations matter for this specific output?
\end{formal} 
In backward mode, the full jacobian matrix is obtained by vertical concatenation of all vector-Jacobian products, computed per output.

\subsection{Comparison}
Consider a function $f:\R^n \to \R^m$. Computing the full Jacobian requires $n$ forward passes (JVP) or $m$ backward passes (VJP). 

Therefore,
\begin{itemize}[noitemsep]
  \item If $n \ll m$, the forward mode is faster
  \item If $n \gg m$, the backward mode is faster
  \item If $n \approx m$, none prevail
\end{itemize}
\section{Memory usage}
The forward mode only needs to store the current value and the current derivative. The memory usage stays approximatively constant.\\
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{img/fd_mem.png}
	\caption{Forward mode memory usage}
	\label{fig:fd_mem}
\end{figure}
The backward mode, however, needs to store all intermediate values to compute the derivatives in the backward pass. 
As a consequence, the memory usage will increase during the forward pass, and then reduce during the derivatives computation, in the backward pass.\\
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{img/bd_mem.png}
	\caption{Backward mode memory usage}
	\label{fig:bd_mem}
\end{figure}
Concluding with this section, the forward mode is deemed more memory efficient than the backward mode. 
However, this aspect is, in general, less significant than the number of operations performed (JVP vs VJP).
\section{Second order AD}
Automatic Differentiation is also able to compute higher-order derivatives, such as the Hessian. 

For any function $f:\R^n\to\R^m$, the Hessian, per output, is a matrix of dimensions $n\times n$. Its entries can be computed as
\begin{equation}
  (\nabla^2 f(x))_{ij} = \dfrac{\partial^2 f(x)}{\partial x_i \partial x_j}
\end{equation}
The Hessian can also be seen as the Jacobian of the function's gradient
\begin{equation}
  \nabla^2 f(x) = J_{\nabla f}(x)
\end{equation}
This last definition clearly shows that AD systems can be used to obtain the Hessian, by computing the gradient and the Jacobian of $f$ as already seen.
Since, the Jacobian and the gradient can be both computed independently, different AD modes (forward / backward) can be used to compute each one of them. 

As a result, 4 Hessian's computation strategies can be defined
\footnote{Since the Hessian per output is $n\times n$, the full Hessian is a 3D tensor of dimensions $m\times n\times n$.
In the following, $m=1$ will always be assumed for the sake of simplicity, but the reasoning stays the same.}.

\subsection{Prelude}
\subsubsection{Rewriting the Hessian}
Based on the chain rule, the Hessian's entries $\dfrac{\partial^2 (f_2 \circ f_1)}{\partial x_i \partial x_j}$ can be rewritten 
into a more AD-suitable expression. 
Let $s_0 = x$ and $s_k = f_k(s_{k-1})$ for $k \ge 1$.
Denote by $J_k \triangleq J_{f_k}(s_{k-1})$ the Jacobian of $f_k$ evaluated at $s_{k-1}$.

%Writing $\partial f_k\triangleq\partial f_k(s_{k-1})$ and $\partial^2f_k\triangleq\partial^2 f_k({s_{k-1}})$, one obtains
% \begin{equation}
%   \begin{aligned}
%     \dfrac{\partial^2 (f_2 \circ f_1)}{\partial x_i \partial x_j} &= \dfrac{\partial}{\partial x_j} \left( \dfrac{ \partial (f_2 \circ f_1)}{\partial x_i} \right) \\
%     &= \dfrac{\partial}{\partial x_j} \left( \partial f_2 \dfrac{ \partial f_1}{\partial x_i} \right) \\
%     &= \left( \partial^2 f_2 \dfrac{\partial f_1}{\partial x_j} \right) \dfrac{\partial f_1}{\partial x_i} + \partial f_2 \dfrac{\partial^2 f_1}{\partial x_i \partial x_j} 
%   \end{aligned}
% \end{equation}
\begin{equation}
  \begin{aligned}
    \frac{\partial^2 (f_2 \circ f_1)}{\partial x_i \partial x_j}
    &= \frac{\partial}{\partial x_j}\left(J_2 \frac{\partial f_1}{\partial x_i}\right) \\
    &=\left(\frac{\partial J_2}{\partial s_1}\frac{\partial f_1}{\partial x_j}\right)\frac{\partial f_1}{\partial x_i}+J_2\frac{\partial^2 f_1}{\partial x_i \partial x_j}
  \end{aligned}
\end{equation}

Introducing $\He_{kj} \triangleq \dfrac{\partial\J_k}{\partial x_j} = \dfrac{\partial \J_k}{s_{k-1}}\J_{k-1}$, 
the final expression reads
\begin{equation}
  \dfrac{\partial^2 (f_2 \circ f_1)}{\partial x_i \partial x_j} = \He_{2j} \dfrac{\partial f_1}{\partial x_i} + \J_2 \dfrac{\partial^2 f_1}{\partial x_i \partial x_j}
\end{equation}

\subsubsection{\text{Dual} numbers}
\emph{\text{Dual} numbers} are a concept borrowed from algebra. Initially introduced in 1873 to represent 
the \hyperlink{https://en.wikipedia.org/wiki/\text{Dual}\_number\#History}{angle} between lines in space, it can also be applied to automatic differentiation.

A \text{Dual} number is an expression of the form $a+b\varepsilon$ where $a$ and $b$ are real numbers and $\varepsilon$ is a symbol such that $\varepsilon^2=0$ 
with $\varepsilon\ne0$. This allows defining the product of two \text{Dual} numbers as 
\begin{equation}
  (a+b\varepsilon)(c+d\varepsilon)= ac + (ad+bc)\varepsilon  
\end{equation}
One can readily see an application equivalent to the Taylor's first-order approximation, via the Taylor series expansion.
\begin{equation}
  f(a+b\varepsilon)=\sum^\infty_{n=0}\dfrac{f^{(n)}(a)b^n\varepsilon^n}{n!}
  \overset{\footnote{Since every term involving $\varepsilon^2$ or greater power of $\varepsilon$ vanishes.}}=f(a)+bf'(a)\varepsilon
\end{equation}  

Back to the automatic differentiation concept, $f(a)$ is the function's value evaluated at $a$, while $bf'(a)\varepsilon$ is the Jacobian-vector product 
evaluated at $a$. 
\begin{itemize}[noitemsep]
  \item $b$ represents the tangent vector
  \item $f'(a)$ is the derivative (Jacobian) evaluated at $a$ in the scalar-output case
\end{itemize}
\text{Dual} numbers encode \emph{exactly} the derivative of a function.

In the following sections, this process is extended to multivariate functions.
\subsection{Forward on forward}
\begin{formal}
  "Directional derivative of a directional derivative"
\end{formal}
\subsubsection{Mathematically}
The forward on forward strategy computes
\begin{equation}
  D(D(f)[v])[w]= w^THv
\end{equation}
where $v,w$ may be any arbitrary direction vectors.
Usually, when computing the full Hessian, they are taken as the one-hot encodings of the variables the differentiation is made with respect to.

\subsubsection{Algorithmically}
Defining $\text{Dual}(s_1\mid t_1)$, with $s_1 = \text{Dual}\left(f_1(x)\ \middle|\ \dfrac{\partial f_1}{\partial x_j}\right)$ and 
$t_1 = \text{Dual}\left(\dfrac{\partial f_1}{\partial x_i}\ \middle|\ \dfrac{\partial^2 f_1}{\partial x_i \partial x_j}\right)$, the forward-on-forward algorithm reads as follows.
\begin{enumerate}[noitemsep]
  \item Compute $s_2 = f_2(s_1) = \text{Dual}\left(f_2(f_1(x))\ \middle|\ \J_2 \dfrac{\partial f_1}{\partial x_j}\right)$
  \item Compute $\J_{f_2}(s_1)$, which gives $\text{Dual}(\J_2\mid\He_{2j})$
  \item Compute 
  \begin{equation} 
    \begin{aligned} 
      t_2 &= \J_{f_2}(s_1) t_1 \\ &= \text{Dual}(\J_2\mid\He_{2j}) 
              \text{Dual}\left(\dfrac{\partial f_1}{\partial x_i}\ \middle|\ \dfrac{\partial^2 f_1}{\partial x_i \partial x_j}\right) \\ 
          &= \text{Dual}\left(\J_2 \dfrac{\partial f_1}{\partial x_i}\ \middle|\ \J_2 \dfrac{\partial^2 f_1}{\partial x_i \partial x_j} + \He_{2j} \dfrac{\partial f_1}{\partial x_i}\right) 
    \end{aligned} 
  \end{equation}
  \item Repeat 
\end{enumerate}

This process can be summarized as the following equations, with $g_k(x) = f_k \circ \dots \circ f_1$.
\begin{equation}
  \begin{cases}
    s_k &= \text{Dual}\left(g_k(x)\ \middle|\ \dfrac{\partial g_k}{\partial x_j}\right)\\
    t_k &= \text{Dual}\left(\dfrac{\partial g_k}{\partial x_i}\ \middle|\ \dfrac{\partial^2 g_k}{\partial x_i \partial x_j}\right)
  \end{cases}
\end{equation}

\warning\ As one may see, the value of $w^THv$ is a scalar.
The full process must be repeated for every pair of input variables, this strategy requires $n^2$ evaluations 
and is therefore impractical except for very small $n$.
\subsection{Forward on reverse}
\begin{formal}
  "Directional derivative of the gradient"
\end{formal}

\subsubsection{Mathematically}
The forward on reverse strategy computes the Hessian-Vector Product (HVP)
\begin{equation}
  D(\nabla f)[v]=Hv
\end{equation}

\subsubsection{Algorithmically}
The forward-on-reverse strategy consists in applying forward-mode AD
to the reverse-mode computation of the gradient.
Operationally, this results in:
\begin{enumerate}[noitemsep]
  \item a forward pass propagating both primal values and tangents,
  \item a backward pass in which adjoint variables are augmented with
        tangent components.
\end{enumerate}

Defining $s_1 \triangleq \text{Dual}\left(f_1(x)\ \middle|\ \dfrac{\partial f_1}{\partial x_j}\right)$ 
\begin{enumerate}[noitemsep]
  \item Compute $s_2 = f_2(s_1) $
  \item Compute $\J_{f_2}(s_1)$ which gives $\text{Dual}(\J_2, \He_{2j})$
\end{enumerate}
The backward pass follows. 

Defining $r_2 \triangleq \text{Dual}(r_{2,1}\mid r_{2,2})$, the computation is
\begin{equation}
  \begin{aligned}
    r_2 \J_2 &= \text{Dual}(r_{2,1}\mid r_{2,2}) \text{Dual}(\J_2\mid \He_{2j})\\ &= \text{Dual}(r_{2,1} \J_2\mid r_{2,1} \He_{2j} + r_{2,2} \J_2)
  \end{aligned}
\end{equation}

Using this last equation, the recurrence relation is obtained.
\begin{equation}
  r_k = \text{Dual}\left(\dfrac{\partial f}{\partial s_k}\ \middle|\ \dfrac{\partial^2 f}{\partial s_k \partial x_j}\right)
\end{equation}
where the backward recurrence follows the same graph structure as
reverse-mode AD, and each adjoint update is performed using Dual
arithmetic.

Here, the tangent component of each adjoint stores the mixed second derivative.
\subsection{Reverse on forward}

\subsubsection{Mathematically}
The reverse-on-forward strategy computes the Vector-Hessian Product (VHP)
\begin{equation}
  D(D(f)[v])^*[w]=H^Tw=Hw
\end{equation}
The last equation stands due to the symmetric nature of the Hessian matrix.

\subsubsection{Algorithmically}
The reverse-on-forward strategy starts with a forward pass.

Defining $s_1\triangleq \text{Dual}\left(f_1(x)\ \middle|\ \dfrac{\partial f_1}{\partial x_i }\right)$
\footnote{Note the difference with the previous modes ($j \to i$)}, the algorithm follows
\begin{enumerate}[noitemsep]
  \item Compute $s_2 = f_2(s_1) = \text{Dual}\left(f_2(s_1)\ \middle|\ \J_2 \dfrac{\partial f_1}{\partial x_i}\right)$
  \item The reverse mode computes the local Jacobian.

  Since $s_1$ and $s_2$ are Dual-valued variables, the Jacobian $\partial s_2 / \partial s_1$ is taken with respect to the primal and tangent components.
  \begin{equation}
    \dfrac{\partial s_2}{\partial s_1} = \begin{bmatrix}
      \J_2 & 0\\
      \He_{2i} & \J_2
    \end{bmatrix} % = \dfrac{\partial ((s_2)_1,(s_2)_2)}{\partial ((s_1)_1,(s_1)_2)}
  \end{equation} 
\end{enumerate}
Next, the backward pass is applied, giving:
\begin{equation}
  \begin{cases}
    r_{1, 1} = r_{2, 1} \J_2 + r_{2, 2} \He_{2i}\\
    r_{1, 2} = r_{2, 2} \J_2
  \end{cases}
\end{equation}
The solution of the recurrence equation is given by
\begin{equation}
  r_k = \text{Dual}\left(\dfrac{\partial f}{\partial s_k}\ \middle|\ \dfrac{\partial^2 f}{\partial s_k \partial x_i}\right)
\end{equation}

\subsection{Reverse on reverse}
\subsubsection{Mathematically}

\begin{equation}
  \nabla^2f(x)=J_{\nabla f}(x)
\end{equation}
Equivalently, for a given direction $w$, it computes the Vector-Hessian Product (VHP)
\begin{equation}
  D(\nabla f)^*[w]=H^Tw=Hw
\end{equation}
The last equation stands due to the symmetric nature of the Hessian matrix. 
Repeating this process for every basis $w=e_i$ yields the full Hessian.

\subsubsection{Order}
The reverse-on-forward strategy applies the reverse-mode to the reverse-mode computation of the gradient.
\begin{enumerate}[noitemsep]
  \item A forward pass to obtain function's values
  \item A first reverse pass to compute first-order adjoints
  \item A second reverse pass to compute second-order adjoints
\end{enumerate}

\subsubsection{Forward pass (values computation)}
Let the primal variables 
\begin{equation}
  s_0 = x,\quad s_k = fk(s_{k-1}),\quad k=1,\dots K
\end{equation}
During the forward pass, the $K$ intermediary values are stored, as in standard reverse-mode AD.

\subsubsection{First reverse pass (gradient computation)}
Define the first-order adjoints 
\begin{equation}
  r_k\triangleq\frac{\partial f}{\partial s_k}
\end{equation}
They satisfy the recurrence equations obtained above
\begin{equation}
  \begin{cases}
    r_K=1,\\
    r_{k-1}=r_k\J_k
  \end{cases}
\end{equation}
with $\J_k=\frac{\partial f_k}{\partial s_{k-1}}$.

At the end of the pass, $\nabla f(x)=r_0$ is obtained. All adjoints are stored, as they are required for the second reverse pass.

\subsubsection{Second reverse pass (second-order adjoints)}

The adjoint equations are differentiates w.r.t themselves.

Let $\dot{r}_k$ denote the \emph{second-order adjoint} defined by 
\begin{equation}
  \dot r_k\triangleq\frac{\partial}{\partial x_i}\left(\frac{\partial f}{\partial s_k}\right)
\end{equation}
for a fixed input direction $e_i$ (one-hot encoding of $x_i$).

Differentiating the first-order recurrence
\begin{equation}
  r_{k-1}=r_k\J_k
\end{equation}
w.r.t $x_i$ yields
\begin{equation}
  \dot r_{k-1}=\dot r_k\J_k+r_k\frac{\partial \J_k}{\partial x_i}
\end{equation}
Using the chain rule,
\begin{equation}
  \frac{\partial \J_k}{\partial x_i}=\frac{\partial \J_k}{\partial s_{k-1}}\frac{\partial s_{k-1}}{\partial x_i} = \He_{ki}
\end{equation}
where $\He_{ki}$ denotes the Hessian of $f_k$ contracted with the forward sensitivity $\dfrac{\partial s_{k-1}}{\partial x_i}$.

The second-order reverse recurrence becomes 
\begin{equation}
  \dot r_{k-1}=\dot r_k\J_k+r_k\He_{ki}
\end{equation}
with the initial condition $\dot r_K=0$ since $r_K=1$ is constant.

After completing the second reverse pass, the Hesian column corresponding to direction $e_i$ is obtained as 
\begin{equation}
  \nabla^2f(x)e_i=\dot r_0
\end{equation}
Repeating the process for all $i=1,\dots n$ yields the full Hessian matrix.

\subsection{Comparison}
\begin{table}[H]
\centering
\label{tab:second_order_ad_summary}
\begin{tabular}{|l|c|c|c|c|c|}
\hline
\textbf{Strategy} 
& \textbf{Object computed} 
& \textbf{Output per run} 
& \textbf{Passes} 
& \textbf{Time cost} 
& \textbf{Memory cost}\\
\hline
FoF 
& $w^T H v$ 
& Scalar 
& $1 \to$ 
& $\mathcal{O}(K)$ 
& Low \\
\hline
FoR 
& $H v$ 
& $\mathbb{R}^n$ 
& $1 \to + 1 \leftarrow$ 
& $\mathcal{O}(K)$ 
& Moderate \\
\hline
RoF 
& $H w$ 
& $\mathbb{R}^n$ 
& $1 \to + 1 \leftarrow$
& $\mathcal{O}(K)$ 
& Moderate \\
\hline
RoR 
& $\nabla^2 f$ (column-wise) 
& $\mathbb{R}^n$ 
& $1 \to + 2 \leftarrow$ 
& $\mathcal{O}(K)$ 
& Very high \\
\hline
\end{tabular}
\caption{Comparison of second-order AD strategies ($\to$: forward, $\leftarrow$: backward)}
\end{table}

\begin{table}[H]
\centering
\label{tab:second_order_ad_features}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Criterion} 
& \textbf{FoF} 
& \textbf{FoR} 
& \textbf{RoF} 
& \textbf{RoR} \\
\hline
Computes full Hessian directly 
& \textcolor{red}{No} & \textcolor{red}{No} & \textcolor{red}{No} & \textcolor{red}{No} \\
\hline
Produces Hessian-vector product 
& \textcolor{red}{No} & \textcolor{green}{Yes} & \textcolor{green}{Yes} & \textcolor{green}{Yes} \\
\hline
Uses dual numbers 
& \textcolor{green}{Yes} & \textcolor{green}{Yes} & \textcolor{green}{Yes} & \textcolor{red}{No} \\
\hline
Requires primal trace storage 
& \textcolor{red}{No} & \textcolor{green}{Yes} & \textcolor{green}{Yes} & \textcolor{green}{Yes} \\
\hline
Requires adjoint storage 
& \textcolor{red}{No} & \textcolor{green}{Yes} & \textcolor{green}{Yes} & \textcolor{green}{Yes} \\
\hline
Requires second derivatives of primitives 
& \textcolor{green}{Yes} & \textcolor{green}{Yes} & \textcolor{green}{Yes} & \textcolor{green}{Yes} \\
\hline
Exploits Hessian symmetry 
& \textcolor{red}{No} & \textcolor{green}{Yes} & \textcolor{green}{Yes} & \textcolor{green}{Yes} \\
\hline
Suitable for large $n$ 
& \textcolor{red}{No} & \textcolor{green}{Yes} & \textcolor{green}{Yes} & \textcolor{red}{No} \\
\hline
Implementation complexity 
& Low & Medium & Medium & Very high \\
\hline
\end{tabular}
\caption{Feature comparison of Hessian computation strategies}
\end{table}

\begin{table}[h]
\centering
\label{tab:full_hessian_cost}
\begin{tabular}{|l|c|c|}
\hline
\textbf{Strategy} 
& \textbf{Number of runs} 
& \textbf{Total cost} \\
\hline
FoF 
& $n^2$ 
& $\mathcal{O}(n^2 K)$ \\
\hline
FoR 
& $n$ 
& $\mathcal{O}(n K)$ \\
\hline
RoF 
& $n$ 
& $\mathcal{O}(n K)$ \\
\hline
RoR 
& $n$ 
& $\mathcal{O}(n K)$ (large constant) \\
\hline
\end{tabular}
\caption{Cost of assembling the full Hessian matrix}
\end{table}

Among the four strategies, forward-on-reverse is, preferred in practice, as it provides Hessian-vector products 
at a computational cost comparable to that of a gradient evaluation, while avoiding the prohibitive memory requirements 
of reverse-on-reverse differentiation.

\section{Implicit differentiation}
Above sections considered \emph{explicit} functions where each intermediary step was known. 
\emph{Implicit} functions differentiation is considered when the intermediary steps are hidden and only the output is known (e.g. Newton's method, Gradient Descent).

Usual AD is inefficient for this type of functions due to the requirements of unrolling the implicit function behavior.
Instead, one could use the fact that the derivatives depends on the solution and the path.

For instance, one may consider the square root function $x=\sqrt a$ and extract its implicit formulation through a fixed-point equation.
\begin{equation}
  \begin{aligned}
	x &= \sqrt{a} \\ 
	x^2 &= a \\
	2x^2 &= x^2 + a \\
	g(x,a) = x &= \frac{1}{2} \left( x + \frac{a}{x} \right)
  \end{aligned}
\end{equation}
Iterating, using $x_{k+1} = g(x_k,a)$, from an initial guess $x_0$, the solution would eventually converge to $x^* = \sqrt{a}$. 
The implicit equation is $F(x,a) = x - g(x,a)$ and its solution satisfies $F(x^*,a) = 0$.

\begin{thm}[Inverse function theorem]
  Assume
  \begin{itemize}[noitemsep]
    \item $f:\mathcal{W} \to \mathcal{W}$ is $C^2$
    \item $\partial f (w_0)$ is invertible.
  \end{itemize}
  Then 
  \begin{itemize}[noitemsep]
    \item $f$ is bijective from a neighborhood of $w_0$ to a neighborhood of $f(w_0)$
    \item For $\omega$ in a neighborhood of $f(w_0)$, $f^{-1}$ is $C^2$ and $\partial f^{-1}(\omega)=(\partial f(f^{-1}(w)))^{-1}$
    where the last equation has been obtained through the chain rule.
  \end{itemize}
\end{thm}

\begin{thm}[Implicit function theorem (IFT, univariate case)]
  Assume
  \begin{itemize}[noitemsep]
    \item $F:\R \to \R$
    \item $\exists (w_0, \lambda_0)$ such that $F(w_0, \lambda_0) = 0$ and $\partial_1 F(w_0, \lambda_0) \neq 0$
    \item $F(w,\lambda)$ is $C^2$ in a neighborhood $\mathcal{U}$ of $(w_0, \lambda_0)$ 
  \end{itemize}
  Then there exists a neighborhood $\mathcal{V} \subseteq \mathcal{U}$ where exists $w^*(\lambda)$ such that:
  \begin{equation}
	\begin{aligned}
		w^*(\lambda_0) &= w_0 \\
		F(w^*(\lambda), \lambda) &= 0, \quad \forall (w^*(\lambda),\lambda) \in \mathcal{V} \\
		\partial w^*(\lambda) &= - \left(\partial_1 F (w^*(\lambda), \lambda)\right)^{-1}\partial_2 F(w^*(\lambda), \lambda)
	\end{aligned}
  \end{equation}
\end{thm}
\begin{exmp}
  Implicit relation between $x$ and $y$
  \begin{equation*}
    x^2+y^2 = 1
  \end{equation*}
  Two possible explicit functions
  \begin{equation*}
    \begin{aligned}
      y^+(x)&=\sqrt{1-x^2}\\
      y^-(x)&=-\sqrt{1-x^2}
    \end{aligned}
  \end{equation*}
  Let $F(y,x)=x^2+y^2-1$. Given an initial point $(x_0,y_0)$ with $x^2_0+y^2_0=1$.
  \begin{itemize}[noitemsep]
    \item if $y_0>0$ then IFT holds and $y^*(x)=y^+(x)$,
    \item if $y_0<0$ then IFT holds and $y^*(x)=y^-(x)$,
    \item if $y_0=0$ then $\partial_1 F(y_0,x_0)=2y_0=0$ so IFT does not hold
  \end{itemize}
  This example shows that even a function such as $F(y,x)=x^2+y^2-1$ with 2 possible explicit functions does not violate the IFT.
\end{exmp}
\begin{thm}[Implicit function theorem (IFT, multivariate case)]
  Assume
  \begin{itemize}[noitemsep]
	\item $F:\mathcal{W} \times \Lambda \to \mathcal{W}$
	\item $\exists (w_0, \lambda_0)$ such that $F(w_0, \lambda_0) = 0$ and $\partial_1 F(w_0, \lambda_0) \neq 0$
	\item $F(w,\lambda)$ is $C^2$ in a neighborhood $\mathcal{U}$ of $(w_0, \lambda_0)$ 
  \end{itemize}
  Then there exists a neighborhood $\mathcal{V} \subseteq \mathcal{U}$ where exists $w^*(\lambda)$ such that:
  \begin{equation}
	\begin{aligned}
		w^*(\lambda_0) &= w_0 \\
		F(w^*(\lambda), \lambda) &= 0, \quad \forall (w^*(\lambda),\lambda) \in \mathcal{V} \\
		\partial w^*(\lambda) &= - \left(\partial_1 F(w^*(\lambda), \lambda)\right)^{-1}\partial_2 F(w^*(\lambda), \lambda)
	\end{aligned}
  \end{equation}
\end{thm}
\subsection{Implicit JVP and VJP}
For implicit differentiation, derivatives must be propagated differently.

Consider the implicit function defined by $F(w^*, \lambda) = 0$, from which one may want to compute the JVP and VJP.

\subsubsection{JVP}
Reminding that the forward tangent a step $k$, $t_k$, is computed using the known previous tangent $t_{k-1}$, and the function at the step,
assuming the IFT holds, the following equality may be derived.
\begin{equation}
	t_k = - \left( \partial_1 F(w^*, \lambda) \right)^{-1} \cdot \partial_2 F(w^*, \lambda) \cdot t_{k-1}
\end{equation}
where $t_k = \partial w^*(\lambda)/\partial s_0$ and $t_{k-1} = \partial \lambda / \partial s_0$. 
Each JVP of the implicit function may be obtaine by solving a linear system. 

Setting $A = -\partial_1 F(w^*(\lambda), \lambda)$ and $B = \partial_2 F(w^*(\lambda), \lambda)$, 
the system to solve becomes:
\begin{equation}
	t_k = A^{-1} B t_{k-1}
\end{equation}
Once the forward pass is done, $A$ is fixed and the linear system can be solved efficiently for each JVP by using the LU decomposition of $A$.\\
\begin{equation}
	\begin{aligned}
		LU t_k = B t_{k-1}&\\
		\Downarrow \qquad&\\
		\begin{cases}
			L y &= B t_{k-1} \\
			U t_k &= y
		\end{cases}
	\end{aligned}
\end{equation}

\subsubsection{VJP}

Computing the VJP boils down to the computation of the reverse tangent $r_{k-1}$ knowing the next reverse tangent $r_k$ and the function at this step. 
Assuming the IFT holds, the following equality can be derived.
\begin{equation}
  r_{k-1} = - \left( \partial_2 F(w^*, \lambda) \right)^T \cdot \left( \partial_1 F(w^*, \lambda) \right)^{-T} \cdot r_k
\end{equation}
where $r_{k-1} = \partial s_0 / \partial \lambda$ and $r_k = \partial s_0 / \partial w^*(\lambda)$.
Denoting $A = \partial_1 F(w^*(\lambda), \lambda)$ and $B = \partial_2 F(w^*(\lambda), \lambda)$, 
the VJP can also be obtained as the solution of a linear system.
\begin{equation}
  \begin{cases}
	A^T y = r_k \\
	r_{k-1} = - B^T y
  \end{cases}
\end{equation}
\subsection{AD with optimization problem}
Consider the optimization problem defined by:
\begin{equation}
  \min c^Tx \quad \text{s.t.} \quad Ax = b, \quad x \geq 0
\end{equation}
and its dual:
\begin{equation}
  \max b^Ty \quad \text{s.t.} \quad A^Ty \leq c
\end{equation}
The KKT conditions gives the optimality conditions for this problem:
\begin{equation}
  \begin{cases}
	Ax = b \\
	(A^Ty - c) \perp x \geq 0 
  \end{cases}
\end{equation}
The KKT conditions can be rewritten as an implicit function $F((x,y),(A,b,c))$:
\begin{equation}
  F((x,y),(A,b,c)) = \begin{bmatrix}
	Ax - b \\
	Diag(x)(A^Ty - c)
  \end{bmatrix}
\end{equation}
If the IFT holds for this function, then $\partial_1 F(w^*, \lambda)$ can be computed.
\begin{equation}
  \frac{\partial F}{\partial (x,y)} = \begin{bmatrix}
	A & 0 \\
	Diag(A^Ty - c) & Diag(x)A^T
  \end{bmatrix}
\end{equation}
\section{Sparse AD}
Often in practice, the Jacobian matrix is sparse, meaning that many of its entries are zero. In such cases, sparse AD techniques can be used 
to exploit the sparsity pattern and reduce the computational cost of the Hessian. 
For this section, the sparsity pattern of the Jacobian matrix is assumed to be known.

Consider for instance the following Jacobian matrix:
\begin{equation}
  J = \begin{bmatrix}
	0 & * & 0 & * & 0\\
	0 & 0 & 0 & * & *\\
	0 & * & * & 0 & 0\\
	* & 0 & * & 0 & 0
  \end{bmatrix}
\end{equation}
Computing the Hessian using the standard AD techniques would require 5 JVP or 4 VJP. 
It is possible to reduce the cost by grouping rows or columns together as long as they do not share any non-zero entry. 
For instance, with the Jacobian matrix above, the Hessian can be computed with only 2 JVP or 2 VJP. 

The intuition comes from the fact that when computing a JVP or a VJP, the zero entries do not contribute to the result.
Hence, columns or rows can be grouped together if they do not share non-zero entries. 

For instance, computing the first two JVPs yields
\begin{equation}
	J\begin{bmatrix} 1 \\ 0 \\ 0 \\ 0 \\ 0 \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\ 0 \\ * \end{bmatrix} \qquad \text{and} \qquad
	J\begin{bmatrix} 0 \\ 1 \\ 0 \\ 0 \\ 0 \end{bmatrix} = \begin{bmatrix} *\\ 0 \\ * \\ 0 \end{bmatrix}
\end{equation}
Combining these two JVPs gives:
\begin{equation}
	J\begin{bmatrix} 1 \\ 1 \\ 0 \\ 0 \\ 0 \end{bmatrix} = \begin{bmatrix} * \\ 0 \\ * \\ * \end{bmatrix}
\end{equation}
Using the known sparsity pattern, the results can be put back in the right place. This process is called \emph{decompression}. 

Hence computing the Hessian boilds down to combining the following columns and using them as vectors for the JVPs:
\begin{equation}
  \begin{bmatrix}
	0 & \textcolor{red}{*} & 0 & \textcolor{blue}{*} & 0\\
	0 & 0 & 0 & \textcolor{blue}{*} & \textcolor{red}{*}\\
	0 & \textcolor{red}{*} & \textcolor{blue}{*} & 0 & 0\\
	\textcolor{red}{*} & 0 & \textcolor{blue}{*} & 0 & 0
  \end{bmatrix} \qquad \Rightarrow \qquad \textcolor{red}{v_1}=\begin{bmatrix} 1 \\ 1 \\
	0 \\ 0 \\ 1 \end{bmatrix}, \quad \textcolor{blue}{v_2}=\begin{bmatrix} 0 \\ 0 \\ 1 \\ 1 \\ 0 \end{bmatrix}
\end{equation}
Or for the VJP:
\begin{equation}
  \begin{bmatrix}
	0 & \textcolor{red}{*} & 0 & \textcolor{red}{*} & 0\\
	0 & 0 & 0 & \textcolor{blue}{*} & \textcolor{blue}{*}\\
	0 & \textcolor{blue}{*} & \textcolor{blue}{*} & 0 & 0\\
	\textcolor{red}{*} & 0 & \textcolor{red}{*} & 0 & 0
  \end{bmatrix} \qquad \Rightarrow \qquad \textcolor{red}{v_1}=\begin{bmatrix} 1 & 0 & 0 & 1 \end{bmatrix}, \quad \textcolor{blue}{v_2}=\begin{bmatrix} 0 & 1 & 1 & 0 \end{bmatrix}
\end{equation}
\subsection{Sparsity detection}
The problem of grouping the columns or rows of a sparse matrix can be modeled as a graph coloring problem in many ways.
For instance, it could be a graph where each column is a node and where an edge connects two nodes if they share a non-zero entry. 

Once the problem is modeled, any graph coloring algorithm can be used to color the graph such that no two adjacent nodes have the same color. 
As a result, each color represents a group of columns that can be combined together for the JVPs. The same can be done for the rows for the VJPs. 

Here is a visual example.
\begin{equation}
  J = \begin{bmatrix}
	0 & \textcolor{red}{*} & 0 & \textcolor{blue}{*} & 0\\
	0 & 0 & 0 & \textcolor{blue}{*} & \textcolor{red}{*}\\
	0 & \textcolor{red}{*} & \textcolor{blue}{*} & 0 & 0\\
	\textcolor{red}{*} & 0 & \textcolor{blue}{*} & 0 & 0
  \end{bmatrix}
\end{equation}
\begin{center}
  \begin {tikzpicture}[-latex ,auto ,node distance = 2.5 cm and 2.5 cm ,on grid ,
	semithick ,
	state1/.style ={ circle ,top color =white , bottom color = red ,
	draw,red , text=black , minimum width =1 cm}, state2/.style ={ circle ,top color =white , bottom color = blue ,
	draw,blue , text=black , minimum width =1 cm}]
	\node[state1] (E) {$5$};
	\node[state2] (D) [above right=of E] {$4$};
	\node[state2] (C) [above left=of E] {$3$};
	\node[state1] (B) [above =of D] {$2$};
	\node[state1] (A) [above =of C] {$1$};
	\path (A) edge (C);
	\path (C) edge (B);
	\path (B) edge (D);
	\path (D) edge (E);
  \end{tikzpicture}
\end{center}
In more complex sparsity patterns, somes nodes could be linked multiple times, and therefore a particular care must be taken to ensure no multipy linked nodes
share an edge.
\begin{equation}
  J = \begin{bmatrix}
	0 & 0 & * \\
	* & * & 0 \\
	* & * & 0
  \end{bmatrix}
\end{equation}
Node 1 and 2 should be linked by two edges because they share two non-zero entries, hence the colors must be different.

The probem can also be modeled as a bipartite graph, where one set of nodes represents the columns and the other set represents the rows. 
There will be an edge between a column node and a row node if the corresponding entry in the matrix is non-zero. 
Considering the same instance
\begin{equation}
  J = \begin{bmatrix}
	0 & \textcolor{green}{*} & 0 & \textcolor{green}{*} & 0\\
	0 & 0 & 0 & \textcolor{violet}{*} & \textcolor{violet}{*}\\
	0 & \textcolor{violet}{*} & \textcolor{violet}{*} & 0 & 0\\
	\textcolor{green}{*} & 0 & \textcolor{green}{*} & 0 & 0
  \end{bmatrix}  \qquad or \qquad \begin{bmatrix}
	0 & \textcolor{red}{*} & 0 & \textcolor{blue}{*} & 0\\
	0 & 0 & 0 & \textcolor{blue}{*} & \textcolor{red}{*}\\
	0 & \textcolor{red}{*} & \textcolor{blue}{*} & 0 & 0\\
	\textcolor{red}{*} & 0 & \textcolor{blue}{*} & 0 & 0
  \end{bmatrix} 
\end{equation}
\begin{center}
  \begin {tikzpicture}[-latex ,auto ,node distance = 2 cm and 4 cm ,on grid ,
	semithick ,
	state1/.style ={ circle ,top color =white , bottom color = red , draw,red , text=black , minimum width =1 cm}, state2/.style ={ circle ,top color =white , bottom color = blue , draw,blue , 
  text=black , minimum width =1 cm}, state3/.style ={ circle ,top color =white , bottom color = green , draw,green , text=black , minimum width =1 cm}, state4/.style ={ circle ,top color =white , 
  bottom color = violet , draw,violet , text=black , minimum width =1 cm}]
	\node[state3] (A) {$r_1$};
	\node[state4] (B) [below=of A] {$r_2$};
	\node[state4] (C) [below=of B] {$r_3$};
	\node[state3] (D) [below=of C] {$r_4$};
	\node[state1] (E) [right=of A] {$c_1$};
	\node[state1] (F) [below=of E] {$c_2$};
	\node[state2] (G) [below=of F] {$c_3$};
	\node[state2] (H) [below=of G] {$c_4$};
	\node[state1] (I) [below=of H] {$c_5$};
	\path (A) edge (F);
	\path (A) edge (H);
	\path (B) edge (H);
	\path (B) edge (I);
	\path (C) edge (F);
	\path (C) edge (G);
	\path (D) edge (E);
	\path (D) edge (G);
  \end{tikzpicture}
\end{center}
With this modeling, the coloring problem is a distance-2 graph coloring problem. 
Two column nodes being at distance 2 from each other can't share the same color because if they share a non-zero entry ar row $n$, 
they will be connected to the same row node $n$.

The same applies for the rows.

\subsection{Symmetric matrix}
When the Jacobian matrix is symmetric, a more efficient algorithm can be used to compute the HVP. 
For instance, consider the following symmetric Jacobian matrix.
\begin{equation}
  J = \begin{bmatrix}
	a_1 & a_2 & a_3 & 0\\
	a_2 & a_4 & 0 & a_5\\
	a_3 & 0 & a_6 & 0\\
	0 & a_5 & 0 & a_7
  \end{bmatrix}
\end{equation}
With the previous sparse AD techniques, 3 HVPs or 3 VHPs would be required, instead of 4 wit the standard AD. 
Using the symmetry of the matrix, this computations can be reduced to 2 HVPs or 2 VHPs. Indeed, columns can colored as
\begin{equation}\label{eq:sym_norm}
  J = \begin{bmatrix}
	\textcolor{red}{a_1} & \textcolor{blue}{a_2} & \textcolor{blue}{a_3} & 0\\
	\textcolor{red}{a_2} & \textcolor{blue}{a_4} & 0 & \textcolor{red}{a_5}\\
	\textcolor{red}{a_3} & 0 & \textcolor{blue}{a_6} & 0\\
	0 & \textcolor{blue}{a_5} & 0 & \textcolor{red}{a_7}
  \end{bmatrix}
\end{equation}
Using the symmetry property, the entry shared between columns 2 and 3 can be ignored since this value would be also computed by the HVP on the 
first column. Hence, it can be considered as 0 for now, and replaced back later.
 
Computing the two HVPs and retrieving the full Hessian matrix shows
\begin{equation}\label{eq:sym_norm_sol}
  J \begin{bmatrix} 1 & 0 \\ 0 & 1 \\ 0 & 1 \\ 1 & 0 \end{bmatrix} = \begin{bmatrix} a_1 & a_2+a_3 \\ a_2 +a_5 & a_4 \\ a_3 & a_6 \\ a_7 & a_5 \end{bmatrix}
\end{equation}
$a_2$ can be obtained by solving a small linear system. 

Using computed values and knowing the sparsity pattern, the full matrix can be reconstructed. 

The same process can be developed for VHPs by grouping rows instead of columns.

\subsection{Star coloring}

Given a graph $G=(V,E)$, and a coloring function $\phi: V \to \{1, \ldots, p\}$, a star coloring is distance-1 coloring with the additional constraint 
that every path of 4 vertices uses at least 3 colors. In other words, no path of length 3 is \emph{bi-chromatic}. This prevents \emph{length-3} dependency chains.
For instance, considering the same symmetric Jacobian matrix as above
\begin{equation}
  J = \begin{bmatrix}
	a_1 & a_2 & a_3 & 0\\
	a_2 & a_4 & 0 & a_5\\
	a_3 & 0 & a_6 & 0\\
	0 & a_5 & 0 & a_7
  \end{bmatrix}
\end{equation}
Coloring its associated graph with the classic distance-1 coloring would yield the same matrix as \eqref{eq:sym_norm}.
Computing the HVPs, one would obtain the matrix \eqref{eq:sym_norm_sol} and solving a linear system would be required to retrieve $a_2$. 

Usings star coloring, one could retrieve explicitly all the element. 

Consider the following coloring
\begin{equation}
  J = \begin{bmatrix}
	\textcolor{red}{a_1} & \textcolor{blue}{a_2} & \textcolor{blue}{a_3} & 0\\
	\textcolor{red}{a_2} & \textcolor{blue}{a_4} & 0 & \textcolor{green}{a_5}\\
	\textcolor{red}{a_3} & 0 & \textcolor{blue}{a_6} & 0\\
	0 & \textcolor{blue}{a_5} & 0 & \textcolor{green}{a_7}
  \end{bmatrix}
\end{equation}
Using the green color ensures the star coloring condition. 
Now, computing the 3 HVPs
\begin{equation}
  J \begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix} = 
  \begin{bmatrix} a_1 & a_2 + a_3 & 0 \\ a_2 & a_4 & a_5 \\ a_3 & a_6 & 0\\ 0 & a_5 & a_7 \end{bmatrix}
\end{equation}
All the values can be immediately retrieved from the matrix without solving any linear equation system. 

This proves its better efficiency compared to the usual distance-1 coloring (4 HVPs), and the symmetric distance-1 coloring (2 HVPs + linear system).
\subsection{Acyclic coloring}

For any Hessian $H$ and vector $v$, computing 
\begin{equation*}
  y=Hv
\end{equation*}
gives a vector $y$ whose components are defined as 
\begin{equation}
  \label{eq:sum_hvp}
  y_i=\sum_{j\in\mathcal N(i)}H_{ij}v_j
\end{equation}
where $\mathcal N(i)$ is the set of non-zero entry column indexes at row $i$.

From here, assume $\phi: V \to \{1, \ldots, p\}$ is a proper coloring such that no two neighboring nodes have the same color.
\subsubsection{Diagonal entries}
Diagonal values are easy to extract. Assume node $i$ is colored in $c$. Then given the vector $v^{(c)}$ whose $j$-th component is $1$ iff $j$ is colored in $c$,
the $i$-th component of the HVP yieldsÂ²
\begin{equation*}
  (Hv^{(c)})_i = H_{ii}
\end{equation*} 

\subsubsection{Off-diagonal entries}
It is readily seen that for a node $i$, colored $c_1$ and linked to a node $j$, colored $c_2$, $H_{ij}=H_{ji}$ appears in $(Hv^{(c_2)})_i$ and $(Hv^{(c_1)})_j$.
Unfortunately, due to eq.\ref{eq:sum_hvp}, these entries appear summed with others. For instance 
\begin{equation*}
  (Hv^{(c_2)})_i=H_{ij}+H_{ik}+H_{il}
\end{equation*}

\subsubsection{Cycles}
Cycles are a problem in coloring. Assume column nodes $i,j,k,l$ are connected in a cycle.
\begin{center}
  \begin {tikzpicture}
	\node (A) {$i$};
  \node (B) [right=of A]{$j$};
	\node (C) [below=of A]{$k$};
  \node (D) [below=of B]{$l$};
	\draw (A) -- (B);
  \draw (B) -- (D);
  \draw (A) -- (C);
  \draw (C) -- (D);
  \end{tikzpicture}
\end{center}
Following eq.\ref{eq:sum_hvp}, equations for those components would be 
\begin{equation*}
  \left\{\begin{aligned}
    y_i &= H_{ij} + H_{ik}\\
    y_j &= H_{ji} + H_{jl}\\
    y_k &= H_{ki} + H_{kl}\\
    y_l &= H_{lj} + H_{lk}
  \end{aligned}\right.
\end{equation*}
which are cycling equations. Solving this system requires additional HVPs.

\subsubsection{Acyclic coloring}
An \emph{acyclic coloring} ensures that for any two colors $c_1,c_2$, the induced subgraph has no cycles.
It is, therefore, a \emph{tree} or a \emph{forest} where every connected component has at least one leaf.

The algorithm follows
\begin{enumerate}[noitemsep]
  \item Suppose node $i$ (colored $c_1$) is conneced to exactly one node $j$ (colored $c_2$). $i$ is a leaf.
  \item From the HVP from color $c_2$, one obtains
        \begin{equation*}
          (Hv^{(c_2)})_i=H_{ij}
        \end{equation*}
        Indeed, since $i$ is a leaf of $j$, and thus the link $(i-j)$ is the only edge $i$ has, 
        the only contribution to the $i$-th component of the $j$-th HVP (see eq. \ref{eq:sum_hvp}) is $H_{ij}$.
  \item Once $H_{ij}$ is known, it can be substracted from the equation at $j$, the node $i$ can be removed from the tree, and the tree shrinks.
  \item The process is repeated until the full Hessian is constructed. 
\end{enumerate}
This strategy only requires as much HVPs as there are colors required to form an acycling coloring, and often
\begin{equation*}
  \#\text{colors}\ll\#\text{variables}
\end{equation*}

Mentally, this process can be seen as 
\begin{formal}
  \begin{itemize}[noitemsep]
    \item Each HVP gives blurred information
    \item Acyclic coloring makes sure that blue can always be unblurred locally
  \end{itemize}
\end{formal}

\subsection{Chromatic number ($\xi$)}
For every graph G,
\begin{equation}
  \xi_1(G) \leq \xi_{acyclic} (G) \leq \xi_{star}(G) \leq \xi_2(G)
\end{equation}
$\xi_1$ is the usual distance-1 chromatic number, $\xi_2$ the distance-2 chromatic number.
\chapter{Neural networks}
% \textcolor{red}{TODO link tangent with neural networks}\\
Neural networks are a class of machine learning models inspired by the structure and function of the human brain. 
They are composed of layers of interconnected nodes (neurons) that process and transmit information.

First let's define some variables:
\begin{itemize}[noitemsep]
  \item $X$: input data (matrix)
  \item $y$: target data
  \item $W_k$: weights matrix at layer $k$
  \item $b_k$: bias vector at layer $k$
  \item $\sigma$: activation function (ReLU, sigmoid, etc)
  \item $\ell(.)$: loss function 
  \item $H$: number of hidden layers
  \item $S_i$: intermediate state  
\end{itemize} 
A neural network can be seen as a composition of functions, the type of which was discussed in sections above.
A neural network needs to be trained, or in other words its parameters have to be optimized. Optimization requires computing derivatives 
and that's where automatic differentiation comes at play.

The forward pass of a neural network can be described in two equivalent ways.
\begin{equation}
  \begin{aligned}
	&\text{Right to left:} \qquad &&\text{Left to right:}\\
	&S_0 = X \qquad &&S_0 = X\\
	&S_{2k-1} = W_k S_{2k-2} + b_k \qquad &&S_{2k-1} = S_{2k-2}W_k + b_k\\
	&S_{2k} = \sigma(S_{2k-1}) \qquad &&S_{2k} = \sigma(S_{2k-1})\\
	&S_{2H+1} = W_{k+1}S_{2H} \qquad &&S_{2H+1} = S_{2H}W_{k+1}\\
	&S_{2H+2} = \ell(S_{2H+1}, Y) \qquad &&S_{2H+2} = \ell(S_{2H+1}, Y)\\ 
  \end{aligned}
\end{equation}
The main differences resides in the fact that weights are applied on the left side or on the right side of the data.
It can be useful depending on whether the inputs are represented as rowvectors or columnvectors.

Figures \ref{fig:nn_fd} and \ref{fig:nn_bd} shows the respective computational graphs.
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{img/nn_fd.png}
	\caption{Neural network forward pass}
	\label{fig:nn_fd}
\end{figure}
\begin{figure}[H]
	\centering
	\includegraphics[width=\textwidth]{img/nn_bd.png}
	\caption{Neural network backward pass}
	\label{fig:nn_bd}
\end{figure}

Following sections will focus on the building blocks leading to a \emph{transformer}, one type of neural network, involved in Natural Language Processing (NLP) 
and extensively used nowadays in models such as \emph{ChatGPT}, \emph{Claude}, or \emph{LLama}. 

The first step consists in defining the mathematical model. Once it is obtained, one shall find a strategy to encode text into vectors efficiently. 
Eventually, the full transformer architecture may be presented.
\section{Autoregressive models}
Autoregressive models want to predict the next values in a sequence based on previous values. 

Given a sequence of $n_{ctx}$ (context size) past vectors $x_{-1}, x_{-2}, \ldots, x_{-n_{ctx}}$, named \emph{receding horizon}, 
the model aims to predict the next vector $x_0$ and maybe more ($x_1, x_2,\dots$). 

For instance, one may want to predict $x_0$ and $x_1$ based on past data.
\begin{equation}
  \begin{aligned}
	p(x_0, x_1 | x_{-1}, \ldots, x_{-n_{ctx}}) &= p(x_0 | x_{-1}, \ldots, x_{-n_{ctx}}) \cdot p(x_1 | x_0, x_{-1}, \ldots, x_{-n_{ctx} + 1}, \color{red}{x_{-n_{ctx}}}) \\
	&\approx p(x_0 | x_{-1}, \ldots, x_{-n_{ctx}}) \cdot p(x_1 | x_0, x_{-1}, \ldots, x_{-n_{ctx} + 1})
  \end{aligned} 
\end{equation}
\begin{itemize}[noitemsep]
  \item \textbf{Model}: Probability of next vector $\hat p(x_0\mid X)$ where $X$ concatenates $x_{-1},\dots, x_{-n_{ctx}}$
  \item \textbf{Loss}: Cross-entropy to assess the error of $\hat p$ w.r.t. $p$ \begin{equation*}\Lo(X) = - \sum_{x_0} p(x_0 | X) \log(\hat{p}(x_0 | X))\end{equation*}
\end{itemize}
In the particular (and common) case where the probability distribution $p$ puts all the mass on the expected output vector $y$, $p(x_0\mid X)=\delta_y(x_0)$ 
where $\delta_y(x_0)=1$ if $x_0=y$ and $0$ otherwise. 

Accordingly, the cross-entropy can be simplified 
\begin{equation*}
  \begin{aligned}
    \Lo(X) &= - \sum_{x_0} \delta_y\log(\hat{p}(x_0 | X))\\
          &= -\log(\hat{p}(y | X)) 
  \end{aligned}
\end{equation*} 

\section{Tokenization}
\begin{formal}
  First step of word vectorization is \emph{tokenization}.
\end{formal}

Considering one has $n_{ctx}$ characters of context and wants to predict the next characters. The one-hot encoding would lie in $\R^{26}$, thus $n_{voc} = 26$. 
It imposes $n_{ctx}$ to be very large, which is not a good news because transormers have a quadratic complexity in $n_{ctx}$.

Another strategy consists in one-hot encoding each word, but the vocabulary size is often very large (+200k words).
While this strategy could lead to relative low $n_{ctx}$, $n_{voc}$ would be to big.

\subsection{Byte Pair Encoding (BPE)}
An intermediate solution is to use a \emph{Byte Pair Encoding} algorithm which greedily merges the most frequent pairs of tokens into new tokens.

For instance, considering the word \texttt{abracadabra} at the character level, one may observe that it has two frequent pairs \texttt{ab} and \texttt{ra}, 
which can be merged into new tokens \texttt X and \texttt Y respectively.
\begin{equation}
  \texttt{abracadabra} \to \texttt{XYcadXY}
\end{equation}
This process can be repeated until the the desired vocabulary size is reached.

The next iteration could give:
\begin{equation}
  \texttt{XYcadXY} \to \texttt{ZcadZ}
\end{equation}
This tokenization method allows to have a good trade-off between the vocabulary size $n_{voc}$ and the context size $n_{ctx}$. 

However, if the model isn't trained enough, it could return a sequence of tokens making no sense (a sequence of words not related to the previous token) 
if this sequence is the most common token present after the given input context (for instance, returning "is my daughter" after "During the month of April")
in the training corpus.

\subsection{Byte BPE}
\emph{Byte BPE} (Byte Byte Pair Encoding) consists in replacing every character by its numerical value in an ASCII extended table 
(for instance the \hyperlink{https://en.wikipedia.org/wiki/ISO_8859-1}{ISO 8859-1} table). This allows the model to be more flexible on word encodings and accept any language. 

The greatest strength of the Byte BPE encoding consists in separating radicals from prefixes and suffixes by working at character level. 
For instance, the words \textbf{optimiz}e, \textbf{optimiz}ing, \textbf{opimiz}ation, ..., all have the radical \textbf{optimiz} in common 
and the Byte BPE encoder would be able to determine that \textbf{optimiz} is an enough repeating sequence of character to form a single token. 
Instead of storing the 3 words separately, it will store the radical \textbf{optimiz}, and the suffixes \emph{e},\emph{ing}, and \emph{ation} 
which are pretty common in other English words. 

However, if the encoder is not trained enough, it could be affected by the same issue affecting the usual BPE described above.

\section{Embedding}
\begin{formal}
  Second step of word vectorization is \emph{embedding}
\end{formal}

Consider a vocabulary of size $n_{voc}$, a bigram model and a network with $d$ layers. 

The model would be:
\begin{equation}
  \hat{p}(x_0 | x_{-1}) = \softmax(W_d \tanh(\dots \tanh(W_1 x_{-1}) \dots ))
\end{equation}
The matrix $W_1$ has $n_{voc}$ columns and $W_d$ has $n_{voc}$ rows. When $n_{voc}$ is large, the model becomes problematically big.

The main idea resides in using a pair of encoder / decoder to reduce the size. This techniques is called \emph{embedding} and it reduces the vocabulary size to 
the embedding size $d_{emb}$ with $d_{emb} \ll n_{voc}$. The encoder ($C \in \R^{d_{emb} \times n_{voc}}$) maps the one-hot encoding of size $n_{voc}$ to 
a dense vector of size $d_{emb}$ and the decoder ($D \in \R^{n_{voc} \times d_{emb}}$) maps back the dense vector to a vector of size $n_{voc}$. 

The model becomes
\begin{equation}
  \hat{p}(x_0 | x_{-1}) = \softmax(D W_d \tanh(\dots \tanh(W_1 C x_{-1})\dots ))
\end{equation}
Choosing $d_{emb}$ wisely ($d_{emb}\ll n_{voc}$), it becomes faster to compute $W_1(C x_{-1})$ in the new embedding size. 
Moreover, this forces $W_1C$ to be low-rank which can help to reduce overfitting but reduce the \emph{expressiveness} (capacity to capture a range of possible relation between 
input and output) of the model. When $n_{ctx} > 1$, it is possible to compute the embedding matrix once for every vector in the input context. 

In experimentation, it appeared that forcing $D = C^T$ works well in practice.

\subsection{Shared embedding}
When $n_{ctx} > 1$, the encoder C is shared by all tokens, and the model can be written as
\begin{equation}
  \hat{p}(x_0 | x_{-1}, \ldots, x_{-n_{ctx}}) = \softmax(D W_d \tanh(\dots \tanh(W_1 \begin{bmatrix} C x_{-1} \\ \vdots \\ C x_{-n_{ctx}} \end{bmatrix}) \dots ))
\end{equation}
$W_1$ is a matrix of dimensions $n_{voc}\times n_{ctx}d_{emb}$. Assuming $d_{emb} \ll n_{voc}$ and $n_{ctx} \gg 1$, this is much smaller than $n_{ctx}n_{voc}$ 
occurring before the embedding phase. The number of rows of $W_2$ is not affected by the embedding and remains $n_{voc}$.

Figure \ref{fig:shared_embedding} depicts the model with shared embedding.
\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\textwidth]{img/shared_embedding.png}
	\caption{Neural network with shared embedding}
	\label{fig:shared_embedding}
\end{figure}

\section{Recurrent neural networks (RNN)}
A RNN is a type of neural network that is designed to process sequential data by taking advantages of the memory of previous inputs. 

The idea is to have a hidden state $h_t$ that depends on the current input $x_t$ and the previous hidden state $h_{t-1}$. 
With this idea, the network reuses the same weights at every time step and thus gives meaning to the sequence. 

The equations of a simple RNN are:
\begin{equation}
  \begin{cases}
	h_{t+1} = \tanh(W h_{t} + U x_{t-1} + b) \\
	\hat{y}_t = \softmax(V h_t + c)
  \end{cases}
\end{equation}
where $W, U, V$ are weight matrices and $b, c$ are bias vectors.

This model presents some limitations. 
\begin{itemize}[noitemsep]
  \item Long-term dependencies hard to learn due to vanishing/exploding gradient problem.
  \item Hard to parallelize due to step-by-step sequence processing
  \item Memory-intesive for long sequences due to hidden state storage at each time step
\end{itemize}

\section{Attention is all you need}
\subsection{Attention}
\subsubsection{Numerical Dictionnary}
Consider keys $k_i \in \R^{d_k}$, values $v_i \in \R^{d_v}$. 
Given a query $q \in \R^{d_k}$, one may wants to retrieve the value $v_i$ corresponding to the key $k_i$ that is the most similar to the query $q$. 
The adapated tool for this task is the dot-product. 

For each token, the query $q_i$, the key $k_i$, and the vaue $v_i$ may be seen as answers to the following questions.
\begin{itemize}[noitemsep]
  \item $q_i$: What am I looking for?
  \item $k_i$: What do I contain?
  \item $v_i$: What information do I pass if Iâm selected?
\end{itemize}

One key aspect of this numerical dictionnary is its \emph{differentiability}. This property will be useful when training the full model.

\subsubsection{Attention definition}
With a numerical dictionnary defined as above, \emph{attention head} is defined as
\begin{equation}
  \text{Attention}(Q,K,V) = \sum_{i=1}^{n_{ctx}} \alpha_i v_i
\end{equation}
where $\alpha = \softmax(<q,k_1>, \cdots, <q,k_{n_{ctx}}>)$ is the attention weight for key $k_i$. 
In practice, however, queries, keys, and values are vectors. They are respectively contained in matrices $Q, K, V$. 

With the matrix definition, the \emph{attention head} may be rewritten.
\begin{equation}
  \text{Attention}(Q,K,V) = V\softmax\left(\dfrac{K^TQ}{\sqrt{d_k}}\right)
\end{equation}
where the division by $\sqrt{d_k}$ is used to prevent the dot-product from growing too large.

Figure \ref{fig:attention_heatmap} shows the heatmap of the matrix 
\begin{equation*}
  K^TQ=\begin{bmatrix}k_1&\dots&k_{n_{ctx}}\end{bmatrix}\begin{bmatrix}q_1\\\vdots\\q_{n_{ctx}}\end{bmatrix}=\begin{bmatrix}
    \langle k_1,q_1\rangle&\dots&\langle k_1,q_{n_{ctx}}\rangle\\
    \vdots&\ddots&\vdots\\
    \langle k_{n_{ctx}},q_1\rangle&\dots&\langle k_{n_{ctx}},q_{n_{ctx}}\rangle
  \end{bmatrix}
\end{equation*}
This heatmap provides intuition for the attention mechanism. 
It is the key characteristic that allows the transformer to capture the meaning of a sentence by combining the meaning of individual words. 

In particular, it helps the model understand that \emph{blue} and \emph{monster} are not two independant concepts, but rather form a single idea : \emph{a blue monster}. 

The heatmap shown represents the attention weights of a transformer trained for translation. It clearly shows that although \emph{europ\'eenne} and 
\emph{European} do not appear at the same position in the source and target sentences, the transformer is able to determinate they share a closely related meaning.
\begin{figure}
  \centering
  \includegraphics[width=0.5\linewidth]{img/attention_heatmap.png}
  \caption{Heatmap representation of the attention of a transformer trained for translation}
  \label{fig:attention_heatmap}
\end{figure}

\subsection{Masked attention}
To ensure that the model only attends to previous tokens and not future tokens, a masked attention mechanism must bed. 
This is done by applying a mask to the attention weights before the softmax operation. 
The mask is typically a lower triangular matrix that sets the weights corresponding to future tokens to negative infinity, 
effectively preventing them from contributing to the attention output.

With masked attention, the equation becomes
\begin{equation}
  \begin{aligned}
    M &= \begin{bmatrix} 0 & 0 &  \cdots & 0 \\ -\infty & 0 &  \ddots & 0 \\ \vdots & \ddots & \ddots & \vdots \\ -\infty & -\infty &  \cdots & 0 \end{bmatrix}\\
    \text{Attention}(Q,K,V) &= V\softmax\left(M+\dfrac{K^TQ}{\sqrt{d_k}}\right)
  \end{aligned}
\end{equation}
\subsection{Multi-head attention}
Instead of having a single attention head, having multiple attention heads can help the model understand different aspects of the input data. 
Each head has its own set of weights and computes its own attention output. 

After computing all the heads in parallel, their outputs are concatenated and linearly transformed to fit the dimension of $W^O$, the output weights matrix.
This corresponds to a gathering of all of the meanings of the attention different heads to a single representation. 

The multi-head attention mechanism can be defined as
\begin{equation}
  \begin{cases}
	\text{head}_j = \text{Attention}(W_j^QQ, W_j^KK, W_j^VV) \\
	\text{MultiHead}(Q,K,V) = W^O \begin{bmatrix} \text{head}_1 \\ \text{head}_2 \\ \vdots \\ \text{head}_h \end{bmatrix}
  \end{cases}
\end{equation}

\section{Decoder-only transformer}
\emph{Decoder-only} is a transformer architecture used with autoregressive models, predicting the next token based on the past known sequence. 
In this architecture, matrices $Q, K, V$ are the same, and are defined as the input tokens $CX$, where $C$ is the embedding matrix. 
Hence, a decoder-only transformer computes $\text{MultiHead}(CX,CX,CX)$. 

Figure \ref{fig:decoder_arch} depicts the decoder-only transformer architecture that will be discussed in the following sections.
The key component of it, attention, has already been detailed in previous sections.
\begin{figure}
  \centering
  \includegraphics[width=0.6\linewidth]{img/decoder_arch.png}
  \caption{Decoder-only transformer architecture}
  \label{fig:decoder_arch}
\end{figure}
\subsection{Positional encoding}
The positional encoding is essential in a transformer model because it provides information about the order of the tokens in the input sequence. 
Since transformers do not have a built-in notion of sequence order like RNNs, positional encodings are added to the input embeddings 
to give the model a sense of position.

The one-hot encodings can't be used for the position because the dimension would be $n_{ctx}$ and not $d_{emb}$ as expected. 
The classic approach uses sines and cosines with an angle depending on the position of the token and the embedding dimension. 

The positional encoding of a token at position $pos$, with the embedding index $i$, and embedding dimension $d_{emb}$, could be defined as
\begin{equation}
  \begin{cases}
    \text{angle} = \dfrac{\text{pos}}{10000^{2i/d_{emb}}} \\
    \text{PE}[\text{pos}, 2i] = \sin\left(\text{angle}\right) \\
    \text{PE}[\text{pos}, 2i+1] = \cos\left(\text{angle}\right)
  \end{cases}
\end{equation}

The positional encoding is added to the input embedding before applying the attention mechanism
\begin{equation}
  \text{MultiHead}(CX + PE, CX + PE, CX + PE)
\end{equation}

\subsection{Residual connection}
The principle of \emph{residual connection} is to add the input of a layer to its output before applying the layer normalization (see next section).
\footnote{It corresponds to the \emph{Add} part of a \emph{Add\&Norm} layer}

This mitigates the vanishing gradient problem and helps the network's learning by reminding it of the original input.

\subsection{Layer normalization}
The norm of the gradient increases exponentially with the number of layers. Multiple techniques have been developped to remediate to this problem. 

Two normalization techniques are typically considered.
\begin{enumerate}
  \item Batch normalization computes the mean and average per \textbf{feature}, across all \emph{samples}. 
  It aims to normalize the importance of a feature for every sample. For a feature $j$, every sample will have the same mean and variance.  

  Due to its dependence on the number of samples, this techniques performs poorly for RNNs and autoregressive models, which are based on sequences.
  \item Layer normalization computes the mean and average per \textbf{sample}, across all \emph{features}.
  It aims to normalize the importance of all features in a sample such that none is preponderant.

  Since it does not depend on the number of samples, layer normalization is usually preferred for transformers.
\end{enumerate}

A \emph{normalization layer} helps stabilize the training process and improve convergence by rescaling the data before the activation function. 
It helps keeping data's mean and variance under control. 

It can be formulated as
\begin{equation}
  \text{LayerNorm}(y_i) = \gamma \dfrac{y_i - \mu_i}{\sigma_i} + \beta 
\end{equation}
where $\gamma$ and $\beta$ are two learnable parameters, $\mu_i$ the mean of $y_i$'s features, and $\sigma_i$ their standard deviation. 

In the transformer architecture, the \emph{Layer Norm} always follows another layer needing normalization.
\begin{equation}
  \text{LayerNorm}(\text{MultiHead}(CX + PE, CX + PE, CX + PE) + (CX + PE))
\end{equation}
\subsection{Feed-forward network (FFN)}
A \emph{feed-forward layer}, also nown as \emph{dense layer}, simply\footnote{lovely} consists in a matrix product, a vector sum, 
and an activation function (ReLU\footnote{ReLU, for Rectified Linear Unit, is defined as $\text{ReLU}(x)=max(0,x)$}), 
applied to each token independently and identically,
\begin{equation}
  \text{FFN}(x) = W_2 \text{ReLU}(W_1 x + b_1) + b_2
\end{equation}
with
\begin{itemize}
  \item $W_1 \in \R^{d_{ff} \times d_{emb}}$ 
  \item $W_2 \in \R^{d_{emb} \times d_{ff}}$
  \item $d_{ff}$ the dimension of the feed-forward layer. Empirically, it appears that $d_{ff}=4d_{emb}$ works nicely
\end{itemize}
The feed-forward network is used to introduce non-linearity and increase the model's capacity to learn complex patterns in the data. 

It is used the remember more complex information in the weights\footnote{For instance, an FNN layer could be used to remember that Max Verstappen is an 
F1 driver although it is not written explicitly in the input context.}, which helps the model to better understand the complex relationships between 
tokens in the sequence.

\subsection{Transformers variation}

\textcolor{red}{To investigate ?}
\section{Performances of transformers}
Before analyzing the complexity of a transformer, here is a quick reminder of the key dimensions used in a transformer
\begin{itemize}[noitemsep]
  \item $n_{ctx}$: context size (number of tokens in the input sequence)
  \item $n_{voc}$: vocabulary size (number of unique tokens)
  \item $d_{emb}$: embedding size (dimension of embeddings)
  \item $d_k, d_v$: key and value dimension (per head)
  \item $d_{ff}$: feed-forward network dimension
  \item $N$: number of layers
\end{itemize}
The time complexity of a transformer can be analyzed step by step, ignoring the embedding step,
\begin{enumerate}[noitemsep]
  \item Linear projections for $Q, K, V$ (with $W_j^Q, W_j^K, W_j^V$): $\Oo\left(d_k d_{emb} n_{ctx}\right)$
  \item Attention score computation ($K^TQ$): $\Oo\left((d_k+d_v)n_{ctx}^2\right)$
  \item Output projection ($W^O$): $\Oo\left(d_v d_{emb} n_{ctx}\right)$
  \item FFN: $\Oo\left(d_{emb} d_{ff} n_{ctx}\right)$
\end{enumerate}
This brings the total cost for a single layer to:
\begin{equation}
  \Oo\left( (d_k + d_v) n_{ctx}^2 + (d_k + d_v + d_{ff}) d_{emb} n_{ctx} \right)
\end{equation}
To get the total cost for $N$ layers, it just needs to be multiplied by $N$.
\begin{equation}
  \Oo\left( N (d_k + d_v) n_{ctx}^2 + N (d_k + d_v + d_{ff}) d_{emb} n_{ctx} \right)
\end{equation}
Knowing that, in general, $d_k \approx d_v \approx d_{emb} \approx d_{ff}$, the complexity can be simplified.
\begin{equation}
  \Oo\left( N d_{emb} n_{ctx}^2 + N d_{emb}^2 n_{ctx} \right) = \Oo\left( N d_{emb} n_{ctx} (n_{ctx} + d_{emb}) \right)
\end{equation}
Two major terms emerge from the above complexity. 
\begin{itemize}[noitemsep]
  \item $N d_{emb} n_{ctx}^2$: coming from the attention score computation. It is the most critical term when $n_{ctx}$ is large.
  \item $N d_{emb}^2 n_{ctx}$: coming from the FFN and the projection. It is the most critical term when $d_{emb}$ is large and thus important for wide models.
\end{itemize}
It can be noticed that the dimension of the vocabulary $n_{voc}$ does not appear in the complexity. 
This dimension is indeed hidden in the embedding process.
% \section{Encoder-decoder transformers}
% \textcolor{red}{Idk what to say}

\chapter{Diffusion Models}
The objective of diffusion models is to generate data by learning how to reverse a gradual noising process, turning random noise into structured samples 
through iterative denoising. 
\section{Tweedie's formula}
First, we need to understand why diffusion models can be trained by denoising and why predicting the noise is equivalent to learning a score. 
For that, we need to introduce Tweedie's formula. 

Consider that we observe a noisy version of a random variable $X$. 
The noisy observation is given by $Y = X + \sigma \varepsilon$, where $\varepsilon \sim \mathcal{N}(0, 1)$. 
We want, given this noisy observation $Y=y$, to estimate the original variable $X$. The MMSE estimator is given by $\mathbb{E}[X | Y=y]$. 

Tweedie's formula states that:
\begin{equation}
  \mathbb{E}[X | Y=y] = y + \sigma^2 \nabla_y \log f_Y(y)
\end{equation}
where $f_Y(y)$ is the probability density function of the noisy observation $Y$. And thus $\nabla_y \log f_Y(y)$ is a score function 
that tells us in which direction the probability mass increases and thus where cleaner data is. 

We can also estimate the noise $\varepsilon$:
\begin{equation}
  \mathbb{E}[\varepsilon | Y=y] = - \sigma \nabla_y \log f_Y(y)
\end{equation}
And this means that if we can predict the noise, we can also estimate the score function and thus the denoised data. 
This is the principle used in diffusion models.

\section{Langevin dynamics (sampling)}
Langevin dynamics is a way to sample from a probability distribution ($p(y)$) when you only know its log-density gradient (the score), not the density itself. 
The idea is to iteratively update a sample by taking small steps in the direction of the score, while also adding some random noise to 
ensure exploration of the space. 

The update rule for Langevin dynamics is given by:
\begin{equation}
  y_{k+1} = y_k + \delta_k \nabla_y \log p(y_k) + \sqrt{2 \delta_k} w_k	
\end{equation}
with $w_k \sim \mathcal{N}(0, 1)$ and $\delta_k$ is a small step size. 
By repeating this process many times, the samples $y_k$ will converge to the target distribution $p(y)$. 
Adding this noise is important because it helps the samples to explore the space more effectively and avoid getting stuck in local modes of the distribution. 

The Langevin dynamics is seen in diffusion models at each denoising step.

\section{Score matching}
This is the part "learning the score" of diffusion models. Here the idea is to fix a noise level $\sigma$ and to corrupt the data $X$ with Gaussian noise to 
get $Y = X + \sigma \varepsilon$, with $\varepsilon \sim \mathcal{N}(0, 1)$. 

Then we want to minimize the error to learn the noisy data distribution ($p_{X+\sigma \varepsilon}$):
\begin{equation}
	  \mathbb{E}\left[ \| \varepsilon_\theta (X+ \sigma \varepsilon) - \varepsilon \|^2 \right]
\end{equation}
But with this, $\sigma$ needs to be scaled properly, otherwise the model will not learn well. If $\sigma$ is too small, then 
the support of $X+ \sigma \varepsilon$ may not cover the whole space, and thus $\varepsilon_\theta(y)$ may be inaccurate. 
If $\sigma$ is too large, then the noise dominates the signal, and the model may struggle to learn meaningful patterns.

The loss function used is the Fisher divergence
\begin{equation*}
  \E[\|\varepsilon_\theta(y)+\sigma\nabla_y\log f_Y(y)\|^2]=\int_yf_Y(y)\|\varepsilon_\theta(y)+\sigma\nabla_y\log f_Y(y)\|^2\mathrm dy
\end{equation*}

\subsection{Variance dependent score}
To address the issue of choosing the right noise level $\sigma$, we can introduce a variance-dependent score function. 
Instead of using a fixed $\sigma$, we can define a score function that depends on the noise level. 
The idea is to train a model $\varepsilon_\theta(y, \sigma)$ that takes both the noisy observation $y$ and the noise level $\sigma$ as inputs. 

The training objective becomes
\begin{equation}
	\mathbb{E}\left[ \| \varepsilon_\theta (X+ \sigma \varepsilon, \sigma) - \varepsilon \|^2 \right]
\end{equation}
\section{Deterministic Sampling}
On the picture below, we can see how does denoising looks like with DDIM (Denoising Diffusion Implicit Models). 
The idea is to use a deterministic process to reverse the diffusion process. If we denoise in one step, for low noise levels, it works quite well. 

For large noise levels, the score provides only a global direction, so multiple denoising steps are required to gradually approach the data manifold.
\begin{figure}
	\centering
	\includegraphics[width=0.9\textwidth]{img/DDIM.png}
	\caption{DDIM sampling example}
  \label{fig:ddim}
\end{figure}
DDIM move from a variance $\sigma_t$ to a variance $\sigma_{t-1}$, like in figure \ref{fig:ddim}. 
\begin{equation}
	\begin{aligned}
		X_t &= X_0 + \sigma_t \varepsilon\\
		\E[X_{t-1} | X_t = x_t] &= \E[X_0 | X_t = x_t] + \sigma_{t-1} \E[\varepsilon | X_t = x_t] \\
		&= \E[X_t | X_t = x_t] - \sigma_t \E[\varepsilon | X_t = x_t] + \sigma_{t-1} \E[\varepsilon | X_t = x_t] \\
		&= x_t - (\sigma_t - \sigma_{t-1}) \E[\varepsilon | X_t = x_t] \\
		&= x_t - (\sigma_t - \sigma_{t-1}) \epsilon_\theta(x_t)
	\end{aligned}
\end{equation}
\section{Denoising with randomness}
Now we want to add some randomness to the denoising process, so we introduce DDPM (Denoising Diffusion Probabilistic Models). 
The idea is to add some noise during the denoising process to ensure diversity in the generated samples. 

Give $0 \leq \mu < 1$, pick $\sigma_{t'}$ such that $\sigma_{t-1} = \sigma_t^\mu \sigma_{t'}^{1-\mu}$ and we have the following sampler:
\begin{equation}
	x_{t-1} = x_t + (\sigma_{t'} - \sigma_t) \epsilon_\theta(x_t, \sigma_t) + \eta w_t 
\end{equation}
with $w_t \sim \mathcal{N}(0, 1)$. If we choose $\mu = 0$ and $\sigma_{t'} = \sigma_{t-1}$, we recover the DDIM sampler. 
And if we choose $\mu = 1/2$, we have the DDPM sampler. We have by assumption $\sigma_{t-1} < \sigma_t$. 
Then, because $\sigma_{t-1}$ is the geometric mean of $\sigma_t$ and $\sigma_{t'}$, it must be between so $\sigma_{t'} < \sigma_{t-1} < \sigma_t$. 
To choose $\eta$, we know we want $\sigma^2_{t-1} = \mathbb{V}(\sigma_{t'}\varepsilon_\theta(x_t,\sigma_t))+\eta W_t$. 
So we can choose $\eta = \sqrt{\sigma_{t-1}^2 - \sigma_{t'}^2}$.

\subsection{Acceleration}
We can accelerate the convergence of the previous sampler using the observation that "The noise prediction at a lower noise level is more accurate". 
We can thus define:
\begin{equation}
	\bar{\epsilon}_t = \gamma \epsilon_\theta(x_t, \sigma_t) + (1-\gamma) \epsilon_\theta(x_{t+1}, \sigma_{t+1}) \qquad \gamma > 1
\end{equation}
\section{Auto-Encoder}
We want to find an encoder $E$ and a decoder $D$ that minimizes the loss:
\begin{equation}
	\E[\| X - D(E(X)) \|^2_2]
\end{equation}
$E(X)$ typically reduces the dimension of $X$ to force the model to keep only essential features. 

If $E$ and $D$ are linear, then the auto-encoder is $x \to DEx$ and so only the product $DE$ matters and the problem becomes:
\begin{equation}
	\min_{D,E} \E[\| X - DEX \|^2_F]
\end{equation}
Because we want to compress the data, we have the constraint $\text{rank}(DE) \leq r$. So the auto-encoder is searching for the best rank-$r$ approximation of X. 
We can use the SVD of $X = U \Sigma V^T$ to find the best rank-$r$ approximation, and in our case it is working the same way as the PCA. 
And thus to approximate $X$ we can choose $DE = U_r \Sigma_r U_r^T$. With all this, an Auto-Encoder can be thought of as a nonlinear generalization of PCA.

\subsection{Variationnal Auto-Encoder}
We want to learn the distribution of our data represented by the random variable $X$. 
To improve learning, we can add artificial noise to learn a more interesting distribution.
\begin{equation}
	\E [\| X - D(E_\mu(X) + \varepsilon \odot E_\sigma(X)) \|^2_2]
\end{equation}
\subsection{Evidence Lower Bound (ELBO)}

The whole goal of diffusion models is to learn a distribution $p_\theta(x)$. 

Ideally, one would like to maximize the log-likelihood
\begin{equation}
  \max_\theta\E_{p_{data}(x)}[\log p_\theta(x)]
\end{equation}
However, as explained above, diffusion models are formulated as \emph{latent-variable models}, where the latent variables correspond to progressively noised versions of the data.

Using a latent trajectory $Z=x_{1:T}$, representing noisy versions of $x_0$ where the latent variables $z\sim Z = (x_1,\dots,x_T)$ correspond to progressively corrupted versions of the data, one can write
\begin{equation}
  p_\theta(x) = \int p_\theta(x,z)\ \mathrm dz
  \label{eq:diffusion_latent_expression}
\end{equation}
However, this integral is usually intractable.

The goal of the \emph{ELBO} on $\log p_\theta(x)$ is to provide a tractable expression of a lower bound that can be used to optimize the model by maximizing it.

Introducing the distribution $q(z\mid x)$\footnote{$q(z\mid x)$ represents the \emph{variational posterior}. It corresponds to an approximation of the posterior distribution 
with useful properties such as easier sampling, parametrization (often using a neural net), and the most important one, a tractable density. It is called \emph{variational} because the 
definition of \emph{ELBO} is often generalized and in some cases, $q(z\mid x)$'s parameters may be learned, hence $q(z\mid x)$ can variate. In diffusion models, however, the learning problem 
consists solely in learning the reverse process $p_\theta(z\mid x)$, while $q(z\mid x)$ is fixed}, eq.\ref{eq:diffusion_latent_expression} can be rewritten 
\begin{equation}
  \begin{aligned}
    \log p_\theta(x)  &= \log\int p_\theta(x,z)\ \mathrm dz\\
                      &= \log \int q(z\mid x)\frac{p_\theta(x,z)}{q(z\mid x)}\ \mathrm dz
    \label{eq:log_latent}
  \end{aligned}
\end{equation}

Using the definition of expectation 
\begin{equation}
  \E_{p(z)}[f(z)] = \int p(z)f(z)\ \mathrm dz
\end{equation}
expression \ref{eq:log_latent} becomes 
\begin{equation}
  \log p_\theta(x) = \log\E_{q(z\mid x)}\left\lbrack\frac{p_\theta(x,z)}{q(z\mid x)}\right\rbrack
\end{equation}
and since the $\log$ function is concave, it respects $\log \E[f(x)]\ge \E[\log f(x)]$, and the \emph{Evidence Lower Bound} (ELBO) on $p_\theta(x)$ may be derived
\begin{equation}
  \log p_\theta(x) \ge E_{q(z\mid x)}\left\lbrack\log\frac{p_\theta(x,z)}{q(z\mid x)}\right\rbrack = E_{q(z\mid x)}[\log p_\theta(x,z) - \log q(z\mid x)] \triangleq \mathcal L_\text{ELBO}(x)
\end{equation}

In diffusion model convention, the ELBO is usually decomposed over time steps
\begin{equation}
  \mathcal L_\text{ELBO}(x)=\E_q\left\lbrack\log p_\theta(x_0\mid x_1)+\sum^T_{t=2}\log p_\theta(x_{t-1}\mid x_t)\right\rbrack + \log p(x_T)
\end{equation}

\subsection{KL Divergence}
\emph{The KL divergence} is a meaningful tool to understand how maximizing the \emph{ELBO} leads to learn the right distribution.

It is defined as 
\begin{equation}
  D_{KL}(p\| q)=\E_{X\sim p}\left\lbrack\log\frac{p(x)}{q(x)}\right\rbrack=\E_{X\sim p}[\log p(x)-\log q(x)]
  \label{eq:kld_def}
\end{equation}
which somehow looks like the \emph{ELBO} derived in the previous section. It can be thought of as a measure of the discrepancy between any two distributions.

Recalling that the joint and posterior distributions are related 
\begin{equation}
    p_\theta(z\mid x) = \frac{p_\theta(x,z)}{p_\theta(x)}
\end{equation}
and using the definition \ref{eq:kld_def}. The following \emph{KL Divergence} stands 
\begin{equation}
  \begin{aligned}
    D_{KL}(q(z\mid x)\| p_\theta(z\mid x))  &= \E_{q(z\mid x)}\left\lbrack\log\frac{q(z\mid x)}{p_\theta(z\mid x)}\right\rbrack\\
                                            &= \E_{q(z\mid x)}\left\lbrack\log\frac{q(z\mid x)p_\theta(x)}{p_\theta(x,z)}\right\rbrack\\
                                            &= \E_{q(z\mid x)}[\log q(z\mid x) + \log p_\theta(x) - \log p_\theta(x,z)]\\
                                            &= \underbrace{\E_{q(z\mid x)}[\log q(z\mid x) - \log p_\theta(x,z)]}_{-\mathcal L_\text{ELBO}(x)} + \log p_\theta(x)
  \end{aligned}
\end{equation}
where the last equality stands because $\log p_\theta(x)$ is constant in $z$.

Hence, the final expression is 
\begin{equation}
  D_{KL}(q(z\mid x)\| p_\theta(z\mid x)) = \log p_\theta(x) - \mathcal L_\text{ELBO}(x)
  \label{eq:variational_inference}
\end{equation}

Although $p_\theta(x)$ is not a tractable expression, and neither is the \emph{KL Divergence}, it provides an insight into how maximizing the lower bound leads to approximating the underlying 
distribution $p_\theta(x)$ efficiently. 

As $\mathcal L_{p_\theta(x)}(x)$ is a lower bound on $p_\theta(x)$, $D_{KL}$ is always non-negative, which is a required condition for a measure. $D_{KL}$ measures the error 
introduced by approximating the true posterior distribution during inference.

Eventually, eq.\ref{eq:variational_inference} explains where the name \emph{variational inference} comes from. The \emph{variational} aspect is due to the optimized nature of 
$q(z\mid x)$'s parameters, while the \emph{inference} keyword is because this expression transforms the learning process into an approximation of the \textbf{Bayesian inference}. 
The goal is to \textbf{infer} a variational posterior distribution $q(z\mid x)$ that minimizes the distance to the true posterior $p_\theta(z\mid x)$.

This identity can be seen as a \emph{certificate of optimality}.

\subsection{Gaussian ELBO}
We can compute the ELBO in the case of Gaussian distributions, like with the variationnal Auto-Encoder. 
Suppose $X = D_\mu(Z) + \varepsilon_1 \odot D_\sigma(Z)$ and $Y = E_\mu(X) + \varepsilon_2 \odot E_\sigma(X)$, 
with $\varepsilon_1, \varepsilon_2 \sim \mathcal{N}(0, 1)$. 

Then we have:
\begin{equation}
	2 D_{KL}((Y|X=x) \| Z) = \| E_\mu(X)\|^2_2 + \| E_\sigma(X) \|^2_2 - \left(\sum_{i=1}^{r} \log((E_\sigma(X))_i^2) +r\right)
\end{equation}
And for the first term of the ELBO:
\begin{equation}
	\begin{aligned}
		&\E\left[\log(f_{X|Z}(x|Y))\right] \\
		&= \E\left[\log(f_{X|Z}(x|E_\mu(X) + \varepsilon_2 \odot E_\sigma(X)))\right] \\
		&= - \dfrac{\log(2\pi)}{2} + \E\left[\| \text{Diag}(D_\sigma(E_\mu(X) + \varepsilon_2 \odot E_\sigma(X)))^{-1} \left(x - D_\mu(D_\sigma(E_\mu(X) 
    + \varepsilon_2 \odot E_\sigma(X)))\right) \|^2_2\right] 
	\end{aligned}	
\end{equation}
\subsection{Monte-Carlo sampling}
The gaussian ELBO can be estimated using Monte-Carlo sampling. Given $L$ samples $(\epsilon_1, \dots, \epsilon_L)$ from $\mathcal{N}(0, 1)$, it gives:
\begin{equation}
	\E \left[ \log(f_{X|Z}(x|Y))\right] \approx \dfrac{1}{L} \sum_{i=1}^{L} \log(f_{X|Z}(x|E_\mu(X) + \epsilon_i \odot E_\sigma(X)))
\end{equation}
With the simple case where $D_\sigma(z) = 1$, we get this approximation ($L_2$ form):
\begin{equation}
	\E \left[ \log(f_{X|Z}(x|Y))\right] \approx - \dfrac{\log(2\pi)}{2} + \dfrac{1}{L} \sum_{i=1}^{L} \| x - D_\mu(E_\mu(x) + \epsilon_i ) \|^2_2
\end{equation}
\subsection{Maximum Likelihood Estimator with variationnal Auto-Encoder}
Reminder, the encoder $E$ maps a data point $x$ to a Gaussian distribution $Y \sim \mathcal{N}(E_\mu(x), E_\sigma(x))$. 
The decoder $D$ maps a latent variable $z$ to the Gaussian distribution $\mathcal{N}(D_\mu)$. 

The maximum likelihood estimator, maximizes the following sum over our datapoints $x$ with its ELBO:
\begin{equation}
	\sum_{x} \log(f_X(x)) \geq \sum_{x} -D_{KL}((Y|X=x) \| Z) + \E[\log(f_{X|Z}(x|Y))]
\end{equation}
So it minimizes the loss i.e. the second term. And the KL divergence is acting as a regularizer to prevent overfitting.
\subsection{Denoising Auto-Encoder}
All of the previous subsections has been explained to get to this point: Denoising Auto-Encoder. 
Reminder the goal of the diffusion model is to denoise data ($Y = X + \sigma \varepsilon$) and find the noise $\varepsilon$, 
the denoising auto-encoder tries instead to find the original data $X$. 

We have those following structures:
\begin{itemize}[noitemsep]
	\item Auto-Encoder $D(E(X))$
	\item Variationnal Auto-Encoder $D(E(X) + \varepsilon)$
	\item Denoising Auto-Encoder $D(E(X + \sigma \varepsilon))$
\end{itemize}
To train the denoising auto-encoder, we can use the Evidence Lower-Bound with $Y = X + \sigma \varepsilon$ and $Z$ such that $X = D(E(Z))$:
\begin{equation}
	-\log(f_X(x)) \leq D_{KL}((Y|X=x) \| Z) - \E[\log(f_{X|Z}(x|Y))]
\end{equation}
\section{Conditionned Diffusion Models}
Conditioned diffusion generates data by denoising noise, but the denoising is guided by a conditioning signal (text, image, layout, etc.). 
Where unconditionned diffusion learn $p(x)$, conditionned diffusion learn $p(x|c)$, with $c$ the conditionning signal. 
It is for example used to train text-to-image generation, where the model generates an image based on a text description. 
This conditionned diffusion will need to learn the relationship between the text and the image so it will need cross-attention like in a transformers.
\section{Classifier-Free Guidance}
To improve the conditioned diffusion for multi-modal distributions, we can use a classifier but it need also training. 
So to overcome this, we can use classifier-free guidance. The idea is to train the model to handle both conditioned and unconditioned data. 
During training, we train both conditionned (with condition $\tau$) and unconditionned diffusion model and combine them with:
\begin{equation}
	\bar{\epsilon}_t = \lambda \epsilon_\theta(x_t, \sigma_t, \tau) + (1-\lambda) \epsilon_\theta(x_t, \sigma_t) \qquad \lambda > 1
\end{equation}
\section{Optical illusions}
To generate optical illusions, we do not need to create a specialized model, we can use a pre-trained diffusion model. 
We can use this model and at each step, we denoise the image under $N$ transformations ($v_i$), bring all denoising directions back to the original space, 
average them, and use this average as the update. 

The optimization problem can be formulated as:
\begin{equation}
	\bar{\epsilon}_t = \dfrac{1}{N} \sum_{i=1}^{N} v_i^{-1} (\epsilon_\theta(v_i(x_t), \sigma_t, \tau))
\end{equation}
The fact that we try to retrieve the same image under different transformations creates conflicting objectives that lead to the generation of optical illusions.



















\chapter{Kernels}
\begin{figure}[H]
	\centering
	\includegraphics[width=0.5\textwidth]{img/kernels.png}
	\caption{Illustration of kernels}
\end{figure}
A kernel is a function that transforms a dataset into another, typically of higher dimension. This helps separate the nonlinear feature, to use the usual linear tools. For example, the canonical kernel is 
\begin{equation}
  r(x,y) = (x^Ty)^2 = \phi(x)^T \phi(y)
\end{equation} 
where the kernel function then is 
\begin{equation}\label{eq:example}
  \phi(x) = \begin{pmatrix}
	x_1^2 \\ x_2^2 \\ \sqrt{2}x_1x_2
  \end{pmatrix}
\end{equation}
\section{Reminders on scalar product}
\begin{itemize}[noitemsep]
  \item [$\to$] Reminder: a Euclidean space is a finite-dimensional vector space endowed with a scalar product. 
\end{itemize}
A scalar product $\langle \cdot, \cdot \rangle_V$ verifies 
\begin{itemize}[noitemsep]
  \item Symmetry: $\forall x,y\in V, \: \langle x,y\rangle_V = \langle y,x \rangle_V$;
  \item Definite positive: if $x\in V$ and $x\neq 0$, then $\langle x,x\rangle >0$, and if $x=0$, then $\langle x,x\rangle=0$;
  \item Bilinearity: $\forall x,y,z\in V, \: \forall \alpha,\beta \in \mathbb{F}, \langle(\alpha x+\beta y),z\rangle_V = \alpha \langle x,z\rangle_V + \beta \langle y,z\rangle_V$.
\end{itemize}
\subsection{Equivalence}
Consider a positive definite symmetric matrix $M\succ 0\in \R^{n\times n}$ with the scalar product $\langle x,y\rangle_M = x^TMy$. Any such matrix can be factored as $M=L^TL$, where $L$ is invertible. We can do a change of coordinates to re-express under the cartesian scalar product:
\begin{equation}
  \begin{cases}
	w = Lx \\
	z = Ly
  \end{cases} \Longrightarrow \langle x,y\rangle_M = \langle w,z\rangle_{\R^n}
\end{equation}
Then, all $n$-dimensional Euclidean spaces are equivalent up to a change of coordinates. 
\subsection{Hilbert space}
For functions integrable on an interval $[a,b]$, we define the scalar product as 
\begin{equation}
  \langle f,g\rangle=\int_a^b f(t)g(t)dt 
\end{equation}
\section{Kernel methods for finite sets}
\subsection{Example -- word embedding}
Text embedding has as objective to represent a text with a vector. The general idea is one-hot encoding, i.e. for a data set $X$ of $N$ words, we create one-hot vectors of dimension $N$, using the feature map (or embedding) $\phi:X\to H$. A sentence is simply the sum of those vectors, weighted by the number of occurences. From this, we can create the kernel matrix $K$, where each element $K_{ij}$ is the comparison between the basis vectors $e_i, e_j$. We suppose that the $\phi$ function has a unitary norm.
\begin{equation}
  K_{ij} = \langle \phi(x_i),\phi(x_j)\rangle_K \in [-1,1]
\end{equation}
A value of $1$ means that the words $x_i$ and $x_j$ are identical, but if the value is $-1$, their meaning are opposite. In the case of a $0$, there is no connection between the words. Now, we can calculate the Cholesky decomposition $K=L^TL$ for a new basis, and use the usual scalar product. 
\begin{itemize}[noitemsep]
  \item [$\to$] Note: if the matrix $K$ is not invertible, we just hit a degenerate case where the $\phi(x_i)$ are not linearly independent. This is not a problem. 
\end{itemize}
\subsection{Kernel trick}
The kernel trick is to never explicitly define the function $\phi$: we can work with the matrix $K$ only, as any vector can be expressed as a linear combination of $\phi(x_i)$ and the main ingredient of linear methods is generally the scalar product, here defined only using $K$. 
\begin{equation}
  \langle \phi(y),\phi(z)\rangle = \langle \sum_{i=1}^N \alpha_i \phi(x_i), \sum_{i=1}^N \beta_i \phi(x_i)\rangle_K = \sum_{i=1}^N \sum_{j=1}^N \alpha_i \beta_j \langle \phi(x_i),\phi(x_j)\rangle_K = \alpha^T K \beta 
\end{equation}
\subsection{Reconstruction}
Given the matrix $K$, we can reconstruction the embedding ($\phi$ and $\langle \cdot,\cdot\rangle_K)$ up to a rotation. For $X$ the set of words and $N$ the dimension of the space, we have $\phi:X\to \R^N$ and we define the scalar product:
\begin{equation}
  \langle x,y\rangle_M = \langle x,y\rangle_{K^{-1}}
\end{equation}
Then, remembering $\phi(x_i) = K e_i$,
\begin{equation}
  \langle \phi(x_i), \phi(x_j)\rangle_M = (Ke_i)^T K^{-1}(Ke_j) = e_i^T K^TK^{-1}Ke_j = e_i^T K e_j
\end{equation}
Let us prove the "up to a rotation" part. Define 
\begin{equation}
  \begin{cases}
	\phi':X\to \R^N \\ \phi'(x) = Q\phi(x) \\ \langle x,y\rangle_{M'} = \langle x,y\rangle_{QK^{-1}Q^T}
  \end{cases}
\end{equation}
where $Q$ is a rotation matrix, i.e. $Q^TQ = I$. Then,
\begin{equation}
  \begin{aligned}
	\langle \phi'(x),\phi'(y)\rangle_{M'}&= (Q\phi(x))^T Q K^{-1} Q^T (Q\phi(y))\\
	&= \phi(x)^T K^{-1}\phi(y) = \langle \phi(x),\phi(y)\rangle_{K^{-1}}
  \end{aligned}
\end{equation}
The result is true for any rotation matrix $Q$. 
\begin{itemize}[noitemsep]
  \item [$\to$] Note: the complexity of computing $K$ is $\mathcal{O}(N^2)$, while the Cholesky decomposition to find the new basis has a complexity of $\mathcal{O}(N^3)$. This decomposition is to be avoided if not really necessary. 
\end{itemize}
\subsection{Advantages}
Let us consider proteins. The alphabet is of size $20$ (number of existing amino-acids), and a protein is made of 4 of these. This means that our space is initially $H = \R^{20^4} = \R^{160.000}$. Consider $N=100$ proteins to classify. \\
The embedding consists in storing $N$ vectors of size $h=160.000$, meaning 1.6M scalars. However, we only need the $K$ matrix of size $N^2=10.000$, and the entries are easy to compute. This means that, using the kernel trick, i.e. working with $K$ directly, we are much more efficient when $N<<h$. 
\section{Kernels methods for continuous sets}
\subsection{Reproducibility Kernel Hilbert Space}
Consider richer sets than finite $X$, e.g. infinite or uncountable sets, with distances defined but not scalar product. Then, let $H$ be a vector space of conitnuous functions $X\to \R$. $H$ is a RKHS if it is a Hilbert space and 
\begin{equation}
  \forall x\in X, \: \exists v\in H \text{ such that } \forall f\in H\:: \: \langle v,f\rangle_H = f(x)
\end{equation}
Mathematically, the kernel function is the equivalent of the kernel matrix, but for infinite spaces:
\begin{equation}
  \phi:X\to \R^N \text{ such that } k(x,y) = \langle \phi(x),\phi(y)\rangle_K = \alpha_x^T K \alpha_y
\end{equation}
For a RKHS $H$, we verify 
\begin{equation}
  \begin{cases}
	\forall x\in X\: : \: k(x,\cdot)\in H\\
	\forall x\in X\: ,\forall f\in H, \: f(x) = \langle f,k(x,\cdot)\rangle_K
  \end{cases}
\end{equation}
The reproducing kernel property is 
\begin{equation}
  k(x,y) = \langle k(x, \cdot), k(y,\cdot)\rangle_H
\end{equation}
\subsection{Properties}
\begin{thm}
  A continuous map $k:X\times X \to \R$ is the kernel of some RKHS $H\subset \mathcal{C}(X,\R)$
  \begin{itemize}[noitemsep]
	\item iff $k(x,y) = \langle \phi(x),\phi(y)\rangle_H$ for some feature map $\phi:X\to H$;
	\item iff, for all finite subsets $X_0\subset X$, the kernel matrix is symmetric positive definite.
  \end{itemize}
\end{thm}
\section{Polynomial kernels}
Polynomial kernels are a particular type of continuous kernels, where the feature map is a function 
\begin{equation}
  \phi:R^n \to \R^\ell 
\end{equation}
Let us take our initial example \ref{eq:example}, and generalize it to 
\begin{equation}
  k(x,y) = (x^Ty)^d  \qquad X = \R^n 
\end{equation}
Then, the dimension of the out space $H = \R^\ell$ is $\ell = \begin{pmatrix}
  n+d-1 \\d
\end{pmatrix}$. 
\end{document}